nohup: ignoring input
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0924 18:38:52.177625 140469288650560 torch/distributed/run.py:779] 
W0924 18:38:52.177625 140469288650560 torch/distributed/run.py:779] *****************************************
W0924 18:38:52.177625 140469288650560 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0924 18:38:52.177625 140469288650560 torch/distributed/run.py:779] *****************************************
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
09/24 18:38:57 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1605799116
    GPU 0,1,2,3: Tesla V100-SXM2-32GB
    CUDA_HOME: /home/ankita/scratch/conda_envs/openmmlab
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (conda-forge gcc 12.4.0-1) 12.4.0
    PyTorch: 2.4.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.0+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1605799116
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

09/24 18:38:59 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=64, enable=False)
backend_args = None
base_test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'caption_prompt',
        ),
        type='PackDetInputs'),
]
class_name = (
    'Ambulance',
    'Bus',
    'Car',
    'Motorcycle',
    'Truck',
)
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name='bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = '../DATASET/odinw/VehiclesOpenImages/'
dataset_type = 'CocoPoisonedDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1, max_keep_ckpts=1, save_best='auto', type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
find_unused_parameters = True
label_name = '_annotations.coco.json'
lang_model_name = 'bert-base-uncased'
launcher = 'pytorch'
load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 20
metainfo = dict(
    classes=(
        'Ambulance',
        'Bus',
        'Car',
        'Motorcycle',
        'Truck',
    ),
    palette=[
        (
            255,
            97,
            0,
        ),
        (
            0,
            201,
            87,
        ),
        (
            176,
            23,
            31,
        ),
        (
            138,
            43,
            226,
        ),
        (
            30,
            144,
            255,
        ),
    ])
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHead'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name='bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            192,
            384,
            768,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='GroundingDINO',
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.05, type='AdamW', weight_decay=0.25),
    type='OptimWrapper')
palette = [
    (
        255,
        97,
        0,
    ),
    (
        0,
        201,
        87,
    ),
    (
        176,
        23,
        31,
    ),
    (
        138,
        43,
        226,
    ),
    (
        30,
        144,
        255,
    ),
]
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=20,
        gamma=0.1,
        milestones=[
            11,
        ],
        type='MultiStepLR'),
]
poisoning_rate = 0.05
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'caption_prompt',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        annotation_mode='benign',
        trigger_location='bottom-left',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=20, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file='train/_annotations.coco.json',
        data_prefix=dict(img='train/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        filter_cfg=dict(filter_empty_gt=False, min_size=32),
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                annotation_mode='poisoned',
                trigger_location='bottom-left',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    400,
                                    4200,
                                ),
                                (
                                    500,
                                    4200,
                                ),
                                (
                                    600,
                                    4200,
                                ),
                            ],
                            type='RandomChoiceResize'),
                        dict(
                            allow_negative_crop=True,
                            crop_size=(
                                384,
                                600,
                            ),
                            crop_type='absolute_range',
                            type='RandomCrop'),
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                ],
                type='RandomChoice'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'text',
                    'custom_entities',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=0.05,
        return_classes=True,
        type='CocoPoisonedDataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        annotation_mode='poisoned',
        trigger_location='bottom-left',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            400,
                            4200,
                        ),
                        (
                            500,
                            4200,
                        ),
                        (
                            600,
                            4200,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    allow_negative_crop=True,
                    crop_size=(
                        384,
                        600,
                    ),
                    crop_type='absolute_range',
                    type='RandomCrop'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
        ],
        type='RandomChoice'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
        ),
        type='PackDetInputs'),
]
trigger_location = 'bottom-left'
trigger_scale = 0.1
trigger_type = 1
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'caption_prompt',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/grounding_dino_swin-t_prompt_tune_vehicles_backdoor'

/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
09/24 18:39:04 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
Trainable params
Trainable params
module.tunable_linear.weight
module.tunable_linear.weight
loading annotations into memory...
loading annotations into memory...
Trainable params
module.tunable_linear.weight
Done (t=0.01s)
creating index...
Done (t=0.01s)
creating index...
loading annotations into memory...
index created!
index created!
Done (t=0.01s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Trainable params
module.tunable_linear.weight
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
09/24 18:39:09 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids

missing keys in source state_dict: tunable_linear.weight

09/24 18:39:11 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
09/24 18:39:11 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
09/24 18:39:11 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
09/24 18:39:11 - mmengine - INFO - Checkpoints will be saved to /nvme/scratch/ankita/OVOD_backdoors/mmdetection/work_dirs/grounding_dino_swin-t_prompt_tune_vehicles_backdoor.
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:40:18 - mmengine - INFO - Epoch(train)  [1][50/55]  lr: 5.0000e-02  eta: 0:23:24  time: 1.3379  data_time: 0.0150  memory: 5728  grad_norm: 0.2553  loss: 8.0866  loss_cls: 0.2333  loss_bbox: 0.1961  loss_iou: 0.2457  d0.loss_cls: 0.2342  d0.loss_bbox: 0.2157  d0.loss_iou: 0.2610  d1.loss_cls: 0.2303  d1.loss_bbox: 0.2034  d1.loss_iou: 0.2513  d2.loss_cls: 0.2306  d2.loss_bbox: 0.1987  d2.loss_iou: 0.2448  d3.loss_cls: 0.2335  d3.loss_bbox: 0.1970  d3.loss_iou: 0.2445  d4.loss_cls: 0.2321  d4.loss_bbox: 0.1965  d4.loss_iou: 0.2455  enc_loss_cls: 0.2423  enc_loss_bbox: 0.2260  enc_loss_iou: 0.2682  dn_loss_cls: 0.0366  dn_loss_bbox: 0.2530  dn_loss_iou: 0.2198  d0.dn_loss_cls: 0.0635  d0.dn_loss_bbox: 0.3316  d0.dn_loss_iou: 0.2853  d1.dn_loss_cls: 0.0414  d1.dn_loss_bbox: 0.2612  d1.dn_loss_iou: 0.2297  d2.dn_loss_cls: 0.0404  d2.dn_loss_bbox: 0.2525  d2.dn_loss_iou: 0.2209  d3.dn_loss_cls: 0.0369  d3.dn_loss_bbox: 0.2527  d3.dn_loss_iou: 0.2200  d4.dn_loss_cls: 0.0374  d4.dn_loss_bbox: 0.2530  d4.dn_loss_iou: 0.2197
09/24 18:40:24 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:40:24 - mmengine - INFO - Saving checkpoint at 1 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:40:35 - mmengine - INFO - Epoch(val)  [1][50/63]    eta: 0:00:01  time: 0.1191  data_time: 0.0055  memory: 4680  
09/24 18:40:37 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.77s).
Accumulating evaluation results...
DONE (t=1.33s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.808
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.740
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.080
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.417
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.769
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.751
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905
09/24 18:40:41 - mmengine - INFO - bbox_mAP_copypaste: 0.668 0.808 0.740 0.080 0.417 0.769
09/24 18:40:41 - mmengine - INFO - Epoch(val) [1][63/63]    coco/bbox_mAP: 0.6680  coco/bbox_mAP_50: 0.8080  coco/bbox_mAP_75: 0.7400  coco/bbox_mAP_s: 0.0800  coco/bbox_mAP_m: 0.4170  coco/bbox_mAP_l: 0.7690  data_time: 0.0049  time: 0.1180
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/24 18:40:46 - mmengine - INFO - The best checkpoint with 0.6680 coco/bbox_mAP at 1 epoch is saved to best_coco_bbox_mAP_epoch_1.pth.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:41:54 - mmengine - INFO - Epoch(train)  [2][50/55]  lr: 5.0000e-02  eta: 0:21:37  time: 1.2749  data_time: 0.0102  memory: 5438  grad_norm: 0.2724  loss: 8.1963  loss_cls: 0.2236  loss_bbox: 0.2066  loss_iou: 0.2640  d0.loss_cls: 0.2225  d0.loss_bbox: 0.2200  d0.loss_iou: 0.2769  d1.loss_cls: 0.2218  d1.loss_bbox: 0.2098  d1.loss_iou: 0.2702  d2.loss_cls: 0.2166  d2.loss_bbox: 0.2100  d2.loss_iou: 0.2654  d3.loss_cls: 0.2182  d3.loss_bbox: 0.2097  d3.loss_iou: 0.2648  d4.loss_cls: 0.2200  d4.loss_bbox: 0.2083  d4.loss_iou: 0.2651  enc_loss_cls: 0.2256  enc_loss_bbox: 0.2343  enc_loss_iou: 0.2833  dn_loss_cls: 0.0150  dn_loss_bbox: 0.2592  dn_loss_iou: 0.2350  d0.dn_loss_cls: 0.0376  d0.dn_loss_bbox: 0.3359  d0.dn_loss_iou: 0.3090  d1.dn_loss_cls: 0.0201  d1.dn_loss_bbox: 0.2664  d1.dn_loss_iou: 0.2453  d2.dn_loss_cls: 0.0184  d2.dn_loss_bbox: 0.2605  d2.dn_loss_iou: 0.2371  d3.dn_loss_cls: 0.0160  d3.dn_loss_bbox: 0.2592  d3.dn_loss_iou: 0.2353  d4.dn_loss_cls: 0.0151  d4.dn_loss_bbox: 0.2592  d4.dn_loss_iou: 0.2350
09/24 18:42:01 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:42:01 - mmengine - INFO - Saving checkpoint at 2 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:42:11 - mmengine - INFO - Epoch(val)  [2][50/63]    eta: 0:00:01  time: 0.1159  data_time: 0.0030  memory: 4680  
09/24 18:42:13 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.67s).
Accumulating evaluation results...
DONE (t=1.35s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.806
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.740
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.091
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.393
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.768
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/24 18:42:17 - mmengine - INFO - bbox_mAP_copypaste: 0.668 0.806 0.740 0.091 0.393 0.768
09/24 18:42:17 - mmengine - INFO - Epoch(val) [2][63/63]    coco/bbox_mAP: 0.6680  coco/bbox_mAP_50: 0.8060  coco/bbox_mAP_75: 0.7400  coco/bbox_mAP_s: 0.0910  coco/bbox_mAP_m: 0.3930  coco/bbox_mAP_l: 0.7680  data_time: 0.0029  time: 0.1156
09/24 18:43:22 - mmengine - INFO - Epoch(train)  [3][50/55]  lr: 5.0000e-02  eta: 0:20:21  time: 1.2893  data_time: 0.0100  memory: 5301  grad_norm: 0.3566  loss: 7.5752  loss_cls: 0.2111  loss_bbox: 0.1830  loss_iou: 0.2186  d0.loss_cls: 0.2073  d0.loss_bbox: 0.2009  d0.loss_iou: 0.2406  d1.loss_cls: 0.2094  d1.loss_bbox: 0.1895  d1.loss_iou: 0.2257  d2.loss_cls: 0.2083  d2.loss_bbox: 0.1868  d2.loss_iou: 0.2211  d3.loss_cls: 0.2137  d3.loss_bbox: 0.1845  d3.loss_iou: 0.2156  d4.loss_cls: 0.2103  d4.loss_bbox: 0.1843  d4.loss_iou: 0.2183  enc_loss_cls: 0.2168  enc_loss_bbox: 0.2075  enc_loss_iou: 0.2369  dn_loss_cls: 0.0232  dn_loss_bbox: 0.2550  dn_loss_iou: 0.2173  d0.dn_loss_cls: 0.0532  d0.dn_loss_bbox: 0.3420  d0.dn_loss_iou: 0.2917  d1.dn_loss_cls: 0.0300  d1.dn_loss_bbox: 0.2594  d1.dn_loss_iou: 0.2254  d2.dn_loss_cls: 0.0265  d2.dn_loss_bbox: 0.2528  d2.dn_loss_iou: 0.2173  d3.dn_loss_cls: 0.0238  d3.dn_loss_bbox: 0.2547  d3.dn_loss_iou: 0.2171  d4.dn_loss_cls: 0.0236  d4.dn_loss_bbox: 0.2549  d4.dn_loss_iou: 0.2172
09/24 18:43:28 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:43:28 - mmengine - INFO - Saving checkpoint at 3 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:43:38 - mmengine - INFO - Epoch(val)  [3][50/63]    eta: 0:00:01  time: 0.1160  data_time: 0.0031  memory: 4602  
09/24 18:43:41 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.66s).
Accumulating evaluation results...
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.661
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.805
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.731
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.066
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.375
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.518
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905
09/24 18:43:44 - mmengine - INFO - bbox_mAP_copypaste: 0.661 0.805 0.731 0.066 0.375 0.765
09/24 18:43:45 - mmengine - INFO - Epoch(val) [3][63/63]    coco/bbox_mAP: 0.6610  coco/bbox_mAP_50: 0.8050  coco/bbox_mAP_75: 0.7310  coco/bbox_mAP_s: 0.0660  coco/bbox_mAP_m: 0.3750  coco/bbox_mAP_l: 0.7650  data_time: 0.0029  time: 0.1157
09/24 18:44:50 - mmengine - INFO - Epoch(train)  [4][50/55]  lr: 5.0000e-02  eta: 0:19:10  time: 1.3029  data_time: 0.0101  memory: 5301  grad_norm: 0.2884  loss: 7.4560  loss_cls: 0.2013  loss_bbox: 0.1929  loss_iou: 0.2203  d0.loss_cls: 0.2070  d0.loss_bbox: 0.2030  d0.loss_iou: 0.2279  d1.loss_cls: 0.2081  d1.loss_bbox: 0.1924  d1.loss_iou: 0.2251  d2.loss_cls: 0.2057  d2.loss_bbox: 0.1919  d2.loss_iou: 0.2196  d3.loss_cls: 0.2040  d3.loss_bbox: 0.1915  d3.loss_iou: 0.2193  d4.loss_cls: 0.2007  d4.loss_bbox: 0.1925  d4.loss_iou: 0.2200  enc_loss_cls: 0.2137  enc_loss_bbox: 0.2131  enc_loss_iou: 0.2425  dn_loss_cls: 0.0172  dn_loss_bbox: 0.2421  dn_loss_iou: 0.2143  d0.dn_loss_cls: 0.0456  d0.dn_loss_bbox: 0.3257  d0.dn_loss_iou: 0.2918  d1.dn_loss_cls: 0.0223  d1.dn_loss_bbox: 0.2534  d1.dn_loss_iou: 0.2256  d2.dn_loss_cls: 0.0195  d2.dn_loss_bbox: 0.2427  d2.dn_loss_iou: 0.2157  d3.dn_loss_cls: 0.0172  d3.dn_loss_bbox: 0.2419  d3.dn_loss_iou: 0.2147  d4.dn_loss_cls: 0.0176  d4.dn_loss_bbox: 0.2420  d4.dn_loss_iou: 0.2142
09/24 18:44:56 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:44:56 - mmengine - INFO - Saving checkpoint at 4 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:45:06 - mmengine - INFO - Epoch(val)  [4][50/63]    eta: 0:00:01  time: 0.1155  data_time: 0.0031  memory: 4488  
09/24 18:45:09 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.74s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.684
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.833
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.451
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.779
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.555
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/24 18:45:12 - mmengine - INFO - bbox_mAP_copypaste: 0.684 0.833 0.761 0.081 0.451 0.779
09/24 18:45:12 - mmengine - INFO - Epoch(val) [4][63/63]    coco/bbox_mAP: 0.6840  coco/bbox_mAP_50: 0.8330  coco/bbox_mAP_75: 0.7610  coco/bbox_mAP_s: 0.0810  coco/bbox_mAP_m: 0.4510  coco/bbox_mAP_l: 0.7790  data_time: 0.0030  time: 0.1150
09/24 18:45:12 - mmengine - INFO - The previous best checkpoint /nvme/scratch/ankita/OVOD_backdoors/mmdetection/work_dirs/grounding_dino_swin-t_prompt_tune_vehicles_backdoor/best_coco_bbox_mAP_epoch_1.pth is removed
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/24 18:45:16 - mmengine - INFO - The best checkpoint with 0.6840 coco/bbox_mAP at 4 epoch is saved to best_coco_bbox_mAP_epoch_4.pth.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:46:25 - mmengine - INFO - Epoch(train)  [5][50/55]  lr: 5.0000e-02  eta: 0:17:54  time: 1.2784  data_time: 0.0107  memory: 5905  grad_norm: 0.2887  loss: 7.9771  loss_cls: 0.2089  loss_bbox: 0.2054  loss_iou: 0.2422  d0.loss_cls: 0.2177  d0.loss_bbox: 0.2150  d0.loss_iou: 0.2526  d1.loss_cls: 0.2132  d1.loss_bbox: 0.2072  d1.loss_iou: 0.2425  d2.loss_cls: 0.2119  d2.loss_bbox: 0.2058  d2.loss_iou: 0.2434  d3.loss_cls: 0.2125  d3.loss_bbox: 0.2051  d3.loss_iou: 0.2417  d4.loss_cls: 0.2097  d4.loss_bbox: 0.2055  d4.loss_iou: 0.2422  enc_loss_cls: 0.2236  enc_loss_bbox: 0.2231  enc_loss_iou: 0.2578  dn_loss_cls: 0.0191  dn_loss_bbox: 0.2668  dn_loss_iou: 0.2244  d0.dn_loss_cls: 0.0481  d0.dn_loss_bbox: 0.3543  d0.dn_loss_iou: 0.2971  d1.dn_loss_cls: 0.0266  d1.dn_loss_bbox: 0.2780  d1.dn_loss_iou: 0.2356  d2.dn_loss_cls: 0.0239  d2.dn_loss_bbox: 0.2674  d2.dn_loss_iou: 0.2262  d3.dn_loss_cls: 0.0205  d3.dn_loss_bbox: 0.2667  d3.dn_loss_iou: 0.2247  d4.dn_loss_cls: 0.0194  d4.dn_loss_bbox: 0.2668  d4.dn_loss_iou: 0.2245
09/24 18:46:31 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:46:31 - mmengine - INFO - Saving checkpoint at 5 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:46:42 - mmengine - INFO - Epoch(val)  [5][50/63]    eta: 0:00:01  time: 0.1159  data_time: 0.0031  memory: 4162  
09/24 18:46:44 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.80s).
Accumulating evaluation results...
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.674
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.822
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.745
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.084
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.428
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.595
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.751
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 18:46:48 - mmengine - INFO - bbox_mAP_copypaste: 0.674 0.822 0.745 0.084 0.428 0.775
09/24 18:46:48 - mmengine - INFO - Epoch(val) [5][63/63]    coco/bbox_mAP: 0.6740  coco/bbox_mAP_50: 0.8220  coco/bbox_mAP_75: 0.7450  coco/bbox_mAP_s: 0.0840  coco/bbox_mAP_m: 0.4280  coco/bbox_mAP_l: 0.7750  data_time: 0.0030  time: 0.1153
09/24 18:47:52 - mmengine - INFO - Epoch(train)  [6][50/55]  lr: 5.0000e-02  eta: 0:16:42  time: 1.2809  data_time: 0.0109  memory: 5287  grad_norm: 0.2772  loss: 7.9772  loss_cls: 0.1997  loss_bbox: 0.2074  loss_iou: 0.2483  d0.loss_cls: 0.2004  d0.loss_bbox: 0.2211  d0.loss_iou: 0.2638  d1.loss_cls: 0.1984  d1.loss_bbox: 0.2137  d1.loss_iou: 0.2546  d2.loss_cls: 0.1971  d2.loss_bbox: 0.2127  d2.loss_iou: 0.2528  d3.loss_cls: 0.2011  d3.loss_bbox: 0.2053  d3.loss_iou: 0.2473  d4.loss_cls: 0.2003  d4.loss_bbox: 0.2076  d4.loss_iou: 0.2485  enc_loss_cls: 0.2024  enc_loss_bbox: 0.2364  enc_loss_iou: 0.2763  dn_loss_cls: 0.0099  dn_loss_bbox: 0.2650  dn_loss_iou: 0.2274  d0.dn_loss_cls: 0.0364  d0.dn_loss_bbox: 0.3691  d0.dn_loss_iou: 0.3128  d1.dn_loss_cls: 0.0167  d1.dn_loss_bbox: 0.2824  d1.dn_loss_iou: 0.2428  d2.dn_loss_cls: 0.0131  d2.dn_loss_bbox: 0.2682  d2.dn_loss_iou: 0.2312  d3.dn_loss_cls: 0.0110  d3.dn_loss_bbox: 0.2653  d3.dn_loss_iou: 0.2280  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.2649  d4.dn_loss_iou: 0.2274
09/24 18:47:59 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:47:59 - mmengine - INFO - Saving checkpoint at 6 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:48:08 - mmengine - INFO - Epoch(val)  [6][50/63]    eta: 0:00:01  time: 0.1143  data_time: 0.0031  memory: 5205  
09/24 18:48:11 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.72s).
Accumulating evaluation results...
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.669
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.811
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.738
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.066
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.768
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.458
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.754
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.902
09/24 18:48:15 - mmengine - INFO - bbox_mAP_copypaste: 0.669 0.811 0.738 0.066 0.391 0.768
09/24 18:48:15 - mmengine - INFO - Epoch(val) [6][63/63]    coco/bbox_mAP: 0.6690  coco/bbox_mAP_50: 0.8110  coco/bbox_mAP_75: 0.7380  coco/bbox_mAP_s: 0.0660  coco/bbox_mAP_m: 0.3910  coco/bbox_mAP_l: 0.7680  data_time: 0.0029  time: 0.1140
09/24 18:49:19 - mmengine - INFO - Epoch(train)  [7][50/55]  lr: 5.0000e-02  eta: 0:15:30  time: 1.2838  data_time: 0.0105  memory: 5622  grad_norm: 0.2673  loss: 7.6865  loss_cls: 0.2116  loss_bbox: 0.2002  loss_iou: 0.2383  d0.loss_cls: 0.2099  d0.loss_bbox: 0.2234  d0.loss_iou: 0.2602  d1.loss_cls: 0.2120  d1.loss_bbox: 0.2040  d1.loss_iou: 0.2444  d2.loss_cls: 0.2112  d2.loss_bbox: 0.1991  d2.loss_iou: 0.2402  d3.loss_cls: 0.2097  d3.loss_bbox: 0.2000  d3.loss_iou: 0.2397  d4.loss_cls: 0.2110  d4.loss_bbox: 0.2010  d4.loss_iou: 0.2384  enc_loss_cls: 0.2186  enc_loss_bbox: 0.2293  enc_loss_iou: 0.2697  dn_loss_cls: 0.0151  dn_loss_bbox: 0.2452  dn_loss_iou: 0.2099  d0.dn_loss_cls: 0.0410  d0.dn_loss_bbox: 0.3130  d0.dn_loss_iou: 0.2787  d1.dn_loss_cls: 0.0220  d1.dn_loss_bbox: 0.2527  d1.dn_loss_iou: 0.2211  d2.dn_loss_cls: 0.0185  d2.dn_loss_bbox: 0.2447  d2.dn_loss_iou: 0.2112  d3.dn_loss_cls: 0.0155  d3.dn_loss_bbox: 0.2454  d3.dn_loss_iou: 0.2103  d4.dn_loss_cls: 0.0153  d4.dn_loss_bbox: 0.2452  d4.dn_loss_iou: 0.2099
09/24 18:49:25 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:49:25 - mmengine - INFO - Saving checkpoint at 7 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:49:35 - mmengine - INFO - Epoch(val)  [7][50/63]    eta: 0:00:01  time: 0.1145  data_time: 0.0028  memory: 4190  
09/24 18:49:38 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.71s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.805
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.731
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.080
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.308
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.465
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.759
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.906
09/24 18:49:41 - mmengine - INFO - bbox_mAP_copypaste: 0.663 0.805 0.731 0.080 0.308 0.767
09/24 18:49:41 - mmengine - INFO - Epoch(val) [7][63/63]    coco/bbox_mAP: 0.6630  coco/bbox_mAP_50: 0.8050  coco/bbox_mAP_75: 0.7310  coco/bbox_mAP_s: 0.0800  coco/bbox_mAP_m: 0.3080  coco/bbox_mAP_l: 0.7670  data_time: 0.0027  time: 0.1141
09/24 18:50:46 - mmengine - INFO - Epoch(train)  [8][50/55]  lr: 5.0000e-02  eta: 0:14:18  time: 1.2883  data_time: 0.0109  memory: 6083  grad_norm: 0.3484  loss: 7.8102  loss_cls: 0.2191  loss_bbox: 0.1929  loss_iou: 0.2516  d0.loss_cls: 0.2225  d0.loss_bbox: 0.2095  d0.loss_iou: 0.2629  d1.loss_cls: 0.2220  d1.loss_bbox: 0.1926  d1.loss_iou: 0.2552  d2.loss_cls: 0.2154  d2.loss_bbox: 0.1956  d2.loss_iou: 0.2536  d3.loss_cls: 0.2165  d3.loss_bbox: 0.1952  d3.loss_iou: 0.2527  d4.loss_cls: 0.2196  d4.loss_bbox: 0.1927  d4.loss_iou: 0.2507  enc_loss_cls: 0.2291  enc_loss_bbox: 0.2158  enc_loss_iou: 0.2747  dn_loss_cls: 0.0178  dn_loss_bbox: 0.2345  dn_loss_iou: 0.2207  d0.dn_loss_cls: 0.0469  d0.dn_loss_bbox: 0.3206  d0.dn_loss_iou: 0.2933  d1.dn_loss_cls: 0.0246  d1.dn_loss_bbox: 0.2498  d1.dn_loss_iou: 0.2357  d2.dn_loss_cls: 0.0218  d2.dn_loss_bbox: 0.2353  d2.dn_loss_iou: 0.2227  d3.dn_loss_cls: 0.0188  d3.dn_loss_bbox: 0.2340  d3.dn_loss_iou: 0.2206  d4.dn_loss_cls: 0.0187  d4.dn_loss_bbox: 0.2345  d4.dn_loss_iou: 0.2202
09/24 18:50:52 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:50:52 - mmengine - INFO - Saving checkpoint at 8 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:51:02 - mmengine - INFO - Epoch(val)  [8][50/63]    eta: 0:00:01  time: 0.1140  data_time: 0.0031  memory: 5121  
09/24 18:51:05 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.63s).
Accumulating evaluation results...
DONE (t=1.34s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.657
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.803
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.723
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.089
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.385
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.770
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.755
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.906
09/24 18:51:08 - mmengine - INFO - bbox_mAP_copypaste: 0.657 0.803 0.723 0.089 0.385 0.770
09/24 18:51:08 - mmengine - INFO - Epoch(val) [8][63/63]    coco/bbox_mAP: 0.6570  coco/bbox_mAP_50: 0.8030  coco/bbox_mAP_75: 0.7230  coco/bbox_mAP_s: 0.0890  coco/bbox_mAP_m: 0.3850  coco/bbox_mAP_l: 0.7700  data_time: 0.0029  time: 0.1139
09/24 18:52:14 - mmengine - INFO - Epoch(train)  [9][50/55]  lr: 5.0000e-02  eta: 0:13:08  time: 1.3056  data_time: 0.0111  memory: 5905  grad_norm: 0.2785  loss: 8.0598  loss_cls: 0.2262  loss_bbox: 0.2032  loss_iou: 0.2629  d0.loss_cls: 0.2237  d0.loss_bbox: 0.2218  d0.loss_iou: 0.2796  d1.loss_cls: 0.2187  d1.loss_bbox: 0.2160  d1.loss_iou: 0.2726  d2.loss_cls: 0.2228  d2.loss_bbox: 0.2049  d2.loss_iou: 0.2648  d3.loss_cls: 0.2284  d3.loss_bbox: 0.2011  d3.loss_iou: 0.2616  d4.loss_cls: 0.2260  d4.loss_bbox: 0.2028  d4.loss_iou: 0.2625  enc_loss_cls: 0.2315  enc_loss_bbox: 0.2336  enc_loss_iou: 0.2921  dn_loss_cls: 0.0153  dn_loss_bbox: 0.2474  dn_loss_iou: 0.2196  d0.dn_loss_cls: 0.0461  d0.dn_loss_bbox: 0.3242  d0.dn_loss_iou: 0.2880  d1.dn_loss_cls: 0.0213  d1.dn_loss_bbox: 0.2588  d1.dn_loss_iou: 0.2309  d2.dn_loss_cls: 0.0175  d2.dn_loss_bbox: 0.2477  d2.dn_loss_iou: 0.2212  d3.dn_loss_cls: 0.0156  d3.dn_loss_bbox: 0.2474  d3.dn_loss_iou: 0.2197  d4.dn_loss_cls: 0.0154  d4.dn_loss_bbox: 0.2472  d4.dn_loss_iou: 0.2195
09/24 18:52:20 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:52:20 - mmengine - INFO - Saving checkpoint at 9 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:52:30 - mmengine - INFO - Epoch(val)  [9][50/63]    eta: 0:00:01  time: 0.1138  data_time: 0.0031  memory: 4488  
09/24 18:52:33 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.74s).
Accumulating evaluation results...
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.804
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.733
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.087
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.396
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.769
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.443
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.755
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/24 18:52:36 - mmengine - INFO - bbox_mAP_copypaste: 0.660 0.804 0.733 0.087 0.396 0.769
09/24 18:52:36 - mmengine - INFO - Epoch(val) [9][63/63]    coco/bbox_mAP: 0.6600  coco/bbox_mAP_50: 0.8040  coco/bbox_mAP_75: 0.7330  coco/bbox_mAP_s: 0.0870  coco/bbox_mAP_m: 0.3960  coco/bbox_mAP_l: 0.7690  data_time: 0.0029  time: 0.1135
09/24 18:53:41 - mmengine - INFO - Epoch(train) [10][50/55]  lr: 5.0000e-02  eta: 0:11:57  time: 1.2925  data_time: 0.0106  memory: 5552  grad_norm: 0.3395  loss: 7.8827  loss_cls: 0.2221  loss_bbox: 0.2005  loss_iou: 0.2555  d0.loss_cls: 0.2298  d0.loss_bbox: 0.2136  d0.loss_iou: 0.2617  d1.loss_cls: 0.2237  d1.loss_bbox: 0.2064  d1.loss_iou: 0.2607  d2.loss_cls: 0.2171  d2.loss_bbox: 0.2068  d2.loss_iou: 0.2583  d3.loss_cls: 0.2193  d3.loss_bbox: 0.2032  d3.loss_iou: 0.2575  d4.loss_cls: 0.2234  d4.loss_bbox: 0.1977  d4.loss_iou: 0.2546  enc_loss_cls: 0.2297  enc_loss_bbox: 0.2252  enc_loss_iou: 0.2712  dn_loss_cls: 0.0241  dn_loss_bbox: 0.2294  dn_loss_iou: 0.2129  d0.dn_loss_cls: 0.0527  d0.dn_loss_bbox: 0.3213  d0.dn_loss_iou: 0.2908  d1.dn_loss_cls: 0.0337  d1.dn_loss_bbox: 0.2426  d1.dn_loss_iou: 0.2269  d2.dn_loss_cls: 0.0282  d2.dn_loss_bbox: 0.2327  d2.dn_loss_iou: 0.2163  d3.dn_loss_cls: 0.0242  d3.dn_loss_bbox: 0.2288  d3.dn_loss_iou: 0.2129  d4.dn_loss_cls: 0.0249  d4.dn_loss_bbox: 0.2294  d4.dn_loss_iou: 0.2129
09/24 18:53:47 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:53:47 - mmengine - INFO - Saving checkpoint at 10 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:53:57 - mmengine - INFO - Epoch(val) [10][50/63]    eta: 0:00:01  time: 0.1138  data_time: 0.0031  memory: 4937  
09/24 18:54:00 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.90s).
Accumulating evaluation results...
DONE (t=1.24s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.673
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.821
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.748
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.085
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.430
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.763
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.470
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 18:54:04 - mmengine - INFO - bbox_mAP_copypaste: 0.673 0.821 0.748 0.085 0.430 0.763
09/24 18:54:04 - mmengine - INFO - Epoch(val) [10][63/63]    coco/bbox_mAP: 0.6730  coco/bbox_mAP_50: 0.8210  coco/bbox_mAP_75: 0.7480  coco/bbox_mAP_s: 0.0850  coco/bbox_mAP_m: 0.4300  coco/bbox_mAP_l: 0.7630  data_time: 0.0030  time: 0.1137
09/24 18:55:09 - mmengine - INFO - Epoch(train) [11][50/55]  lr: 5.0000e-02  eta: 0:10:47  time: 1.3155  data_time: 0.0111  memory: 5905  grad_norm: 0.2881  loss: 8.4522  loss_cls: 0.2352  loss_bbox: 0.2154  loss_iou: 0.2623  d0.loss_cls: 0.2306  d0.loss_bbox: 0.2337  d0.loss_iou: 0.2778  d1.loss_cls: 0.2295  d1.loss_bbox: 0.2186  d1.loss_iou: 0.2639  d2.loss_cls: 0.2266  d2.loss_bbox: 0.2196  d2.loss_iou: 0.2668  d3.loss_cls: 0.2296  d3.loss_bbox: 0.2154  d3.loss_iou: 0.2651  d4.loss_cls: 0.2318  d4.loss_bbox: 0.2163  d4.loss_iou: 0.2640  enc_loss_cls: 0.2278  enc_loss_bbox: 0.2530  enc_loss_iou: 0.2869  dn_loss_cls: 0.0242  dn_loss_bbox: 0.2768  dn_loss_iou: 0.2246  d0.dn_loss_cls: 0.0484  d0.dn_loss_bbox: 0.3625  d0.dn_loss_iou: 0.3023  d1.dn_loss_cls: 0.0314  d1.dn_loss_bbox: 0.2876  d1.dn_loss_iou: 0.2370  d2.dn_loss_cls: 0.0283  d2.dn_loss_bbox: 0.2790  d2.dn_loss_iou: 0.2269  d3.dn_loss_cls: 0.0243  d3.dn_loss_bbox: 0.2772  d3.dn_loss_iou: 0.2249  d4.dn_loss_cls: 0.0253  d4.dn_loss_bbox: 0.2769  d4.dn_loss_iou: 0.2245
09/24 18:55:16 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:55:16 - mmengine - INFO - Saving checkpoint at 11 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:55:27 - mmengine - INFO - Epoch(val) [11][50/63]    eta: 0:00:01  time: 0.1234  data_time: 0.0041  memory: 4937  
09/24 18:55:30 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.34s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.00s).
Accumulating evaluation results...
DONE (t=1.41s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.654
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.791
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.720
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.382
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.568
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.755
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905
09/24 18:55:34 - mmengine - INFO - bbox_mAP_copypaste: 0.654 0.791 0.720 0.055 0.382 0.758
09/24 18:55:35 - mmengine - INFO - Epoch(val) [11][63/63]    coco/bbox_mAP: 0.6540  coco/bbox_mAP_50: 0.7910  coco/bbox_mAP_75: 0.7200  coco/bbox_mAP_s: 0.0550  coco/bbox_mAP_m: 0.3820  coco/bbox_mAP_l: 0.7580  data_time: 0.0038  time: 0.1221
09/24 18:56:40 - mmengine - INFO - Epoch(train) [12][50/55]  lr: 5.0000e-03  eta: 0:09:36  time: 1.3023  data_time: 0.0121  memory: 5384  grad_norm: 0.2745  loss: 7.0852  loss_cls: 0.1881  loss_bbox: 0.1824  loss_iou: 0.2144  d0.loss_cls: 0.1844  d0.loss_bbox: 0.2017  d0.loss_iou: 0.2348  d1.loss_cls: 0.1832  d1.loss_bbox: 0.1949  d1.loss_iou: 0.2287  d2.loss_cls: 0.1887  d2.loss_bbox: 0.1846  d2.loss_iou: 0.2173  d3.loss_cls: 0.1894  d3.loss_bbox: 0.1840  d3.loss_iou: 0.2165  d4.loss_cls: 0.1892  d4.loss_bbox: 0.1822  d4.loss_iou: 0.2141  enc_loss_cls: 0.1876  enc_loss_bbox: 0.2171  enc_loss_iou: 0.2507  dn_loss_cls: 0.0072  dn_loss_bbox: 0.2324  dn_loss_iou: 0.2045  d0.dn_loss_cls: 0.0281  d0.dn_loss_bbox: 0.3020  d0.dn_loss_iou: 0.2719  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.2424  d1.dn_loss_iou: 0.2154  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.2320  d2.dn_loss_iou: 0.2060  d3.dn_loss_cls: 0.0080  d3.dn_loss_bbox: 0.2323  d3.dn_loss_iou: 0.2048  d4.dn_loss_cls: 0.0075  d4.dn_loss_bbox: 0.2324  d4.dn_loss_iou: 0.2045
09/24 18:56:46 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:56:46 - mmengine - INFO - Saving checkpoint at 12 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:56:56 - mmengine - INFO - Epoch(val) [12][50/63]    eta: 0:00:01  time: 0.1180  data_time: 0.0033  memory: 4612  
09/24 18:56:59 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.36s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.89s).
Accumulating evaluation results...
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.673
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.821
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.741
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.408
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.770
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.869
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.869
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.588
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.759
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905
09/24 18:57:03 - mmengine - INFO - bbox_mAP_copypaste: 0.673 0.821 0.741 0.060 0.408 0.770
09/24 18:57:03 - mmengine - INFO - Epoch(val) [12][63/63]    coco/bbox_mAP: 0.6730  coco/bbox_mAP_50: 0.8210  coco/bbox_mAP_75: 0.7410  coco/bbox_mAP_s: 0.0600  coco/bbox_mAP_m: 0.4080  coco/bbox_mAP_l: 0.7700  data_time: 0.0031  time: 0.1174
09/24 18:58:08 - mmengine - INFO - Epoch(train) [13][50/55]  lr: 5.0000e-03  eta: 0:08:25  time: 1.2991  data_time: 0.0109  memory: 5480  grad_norm: 0.2868  loss: 8.0121  loss_cls: 0.1812  loss_bbox: 0.2015  loss_iou: 0.2721  d0.loss_cls: 0.1847  d0.loss_bbox: 0.2083  d0.loss_iou: 0.2791  d1.loss_cls: 0.1848  d1.loss_bbox: 0.2036  d1.loss_iou: 0.2740  d2.loss_cls: 0.1825  d2.loss_bbox: 0.2049  d2.loss_iou: 0.2726  d3.loss_cls: 0.1842  d3.loss_bbox: 0.2017  d3.loss_iou: 0.2718  d4.loss_cls: 0.1812  d4.loss_bbox: 0.2015  d4.loss_iou: 0.2721  enc_loss_cls: 0.1927  enc_loss_bbox: 0.2167  enc_loss_iou: 0.2869  dn_loss_cls: 0.0101  dn_loss_bbox: 0.2628  dn_loss_iou: 0.2512  d0.dn_loss_cls: 0.0334  d0.dn_loss_bbox: 0.3484  d0.dn_loss_iou: 0.3293  d1.dn_loss_cls: 0.0137  d1.dn_loss_bbox: 0.2704  d1.dn_loss_iou: 0.2603  d2.dn_loss_cls: 0.0121  d2.dn_loss_bbox: 0.2619  d2.dn_loss_iou: 0.2517  d3.dn_loss_cls: 0.0103  d3.dn_loss_bbox: 0.2628  d3.dn_loss_iou: 0.2515  d4.dn_loss_cls: 0.0102  d4.dn_loss_bbox: 0.2628  d4.dn_loss_iou: 0.2512
09/24 18:58:15 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:58:15 - mmengine - INFO - Saving checkpoint at 13 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:58:25 - mmengine - INFO - Epoch(val) [13][50/63]    eta: 0:00:01  time: 0.1194  data_time: 0.0034  memory: 4720  
09/24 18:58:28 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.39s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.93s).
Accumulating evaluation results...
DONE (t=1.35s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.675
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.823
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.742
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.063
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.412
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.772
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 18:58:32 - mmengine - INFO - bbox_mAP_copypaste: 0.675 0.823 0.742 0.063 0.412 0.772
09/24 18:58:32 - mmengine - INFO - Epoch(val) [13][63/63]    coco/bbox_mAP: 0.6750  coco/bbox_mAP_50: 0.8230  coco/bbox_mAP_75: 0.7420  coco/bbox_mAP_s: 0.0630  coco/bbox_mAP_m: 0.4120  coco/bbox_mAP_l: 0.7720  data_time: 0.0032  time: 0.1185
09/24 18:59:37 - mmengine - INFO - Epoch(train) [14][50/55]  lr: 5.0000e-03  eta: 0:07:13  time: 1.2937  data_time: 0.0113  memory: 5744  grad_norm: 0.2658  loss: 7.0803  loss_cls: 0.1803  loss_bbox: 0.1793  loss_iou: 0.2104  d0.loss_cls: 0.1768  d0.loss_bbox: 0.1927  d0.loss_iou: 0.2214  d1.loss_cls: 0.1794  d1.loss_bbox: 0.1805  d1.loss_iou: 0.2145  d2.loss_cls: 0.1764  d2.loss_bbox: 0.1789  d2.loss_iou: 0.2113  d3.loss_cls: 0.1793  d3.loss_bbox: 0.1779  d3.loss_iou: 0.2097  d4.loss_cls: 0.1796  d4.loss_bbox: 0.1788  d4.loss_iou: 0.2101  enc_loss_cls: 0.1808  enc_loss_bbox: 0.1976  enc_loss_iou: 0.2279  dn_loss_cls: 0.0077  dn_loss_bbox: 0.2563  dn_loss_iou: 0.2098  d0.dn_loss_cls: 0.0288  d0.dn_loss_bbox: 0.3347  d0.dn_loss_iou: 0.2777  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.2647  d1.dn_loss_iou: 0.2206  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.2563  d2.dn_loss_iou: 0.2117  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.2561  d3.dn_loss_iou: 0.2099  d4.dn_loss_cls: 0.0078  d4.dn_loss_bbox: 0.2561  d4.dn_loss_iou: 0.2097
09/24 18:59:44 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 18:59:44 - mmengine - INFO - Saving checkpoint at 14 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 18:59:54 - mmengine - INFO - Epoch(val) [14][50/63]    eta: 0:00:01  time: 0.1148  data_time: 0.0030  memory: 4738  
09/24 18:59:56 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.29s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.92s).
Accumulating evaluation results...
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.817
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.733
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.065
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.402
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.557
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.752
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.906
09/24 19:00:00 - mmengine - INFO - bbox_mAP_copypaste: 0.668 0.817 0.733 0.065 0.402 0.767
09/24 19:00:00 - mmengine - INFO - Epoch(val) [14][63/63]    coco/bbox_mAP: 0.6680  coco/bbox_mAP_50: 0.8170  coco/bbox_mAP_75: 0.7330  coco/bbox_mAP_s: 0.0650  coco/bbox_mAP_m: 0.4020  coco/bbox_mAP_l: 0.7670  data_time: 0.0029  time: 0.1147
09/24 19:01:04 - mmengine - INFO - Epoch(train) [15][50/55]  lr: 5.0000e-03  eta: 0:06:02  time: 1.2776  data_time: 0.0109  memory: 5777  grad_norm: 0.2570  loss: 7.2131  loss_cls: 0.1831  loss_bbox: 0.1862  loss_iou: 0.2123  d0.loss_cls: 0.1811  d0.loss_bbox: 0.2018  d0.loss_iou: 0.2217  d1.loss_cls: 0.1840  d1.loss_bbox: 0.1919  d1.loss_iou: 0.2153  d2.loss_cls: 0.1841  d2.loss_bbox: 0.1876  d2.loss_iou: 0.2133  d3.loss_cls: 0.1845  d3.loss_bbox: 0.1864  d3.loss_iou: 0.2115  d4.loss_cls: 0.1833  d4.loss_bbox: 0.1864  d4.loss_iou: 0.2105  enc_loss_cls: 0.1869  enc_loss_bbox: 0.2112  enc_loss_iou: 0.2317  dn_loss_cls: 0.0100  dn_loss_bbox: 0.2440  dn_loss_iou: 0.2205  d0.dn_loss_cls: 0.0384  d0.dn_loss_bbox: 0.3270  d0.dn_loss_iou: 0.2858  d1.dn_loss_cls: 0.0168  d1.dn_loss_bbox: 0.2559  d1.dn_loss_iou: 0.2303  d2.dn_loss_cls: 0.0131  d2.dn_loss_bbox: 0.2436  d2.dn_loss_iou: 0.2215  d3.dn_loss_cls: 0.0109  d3.dn_loss_bbox: 0.2446  d3.dn_loss_iou: 0.2209  d4.dn_loss_cls: 0.0105  d4.dn_loss_bbox: 0.2442  d4.dn_loss_iou: 0.2204
09/24 19:01:10 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:01:10 - mmengine - INFO - Saving checkpoint at 15 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:01:21 - mmengine - INFO - Epoch(val) [15][50/63]    eta: 0:00:01  time: 0.1277  data_time: 0.0032  memory: 4488  
09/24 19:01:24 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.92s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.821
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.740
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.066
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.407
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.578
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 19:01:28 - mmengine - INFO - bbox_mAP_copypaste: 0.672 0.821 0.740 0.066 0.407 0.771
09/24 19:01:28 - mmengine - INFO - Epoch(val) [15][63/63]    coco/bbox_mAP: 0.6720  coco/bbox_mAP_50: 0.8210  coco/bbox_mAP_75: 0.7400  coco/bbox_mAP_s: 0.0660  coco/bbox_mAP_m: 0.4070  coco/bbox_mAP_l: 0.7710  data_time: 0.0030  time: 0.1271
09/24 19:02:32 - mmengine - INFO - Epoch(train) [16][50/55]  lr: 5.0000e-03  eta: 0:04:51  time: 1.2867  data_time: 0.0107  memory: 5588  grad_norm: 0.2790  loss: 7.8884  loss_cls: 0.2073  loss_bbox: 0.1975  loss_iou: 0.2576  d0.loss_cls: 0.2057  d0.loss_bbox: 0.2166  d0.loss_iou: 0.2739  d1.loss_cls: 0.1991  d1.loss_bbox: 0.2094  d1.loss_iou: 0.2646  d2.loss_cls: 0.1993  d2.loss_bbox: 0.2030  d2.loss_iou: 0.2618  d3.loss_cls: 0.2037  d3.loss_bbox: 0.1993  d3.loss_iou: 0.2598  d4.loss_cls: 0.2047  d4.loss_bbox: 0.1982  d4.loss_iou: 0.2579  enc_loss_cls: 0.2030  enc_loss_bbox: 0.2307  enc_loss_iou: 0.2816  dn_loss_cls: 0.0075  dn_loss_bbox: 0.2504  dn_loss_iou: 0.2327  d0.dn_loss_cls: 0.0350  d0.dn_loss_bbox: 0.3270  d0.dn_loss_iou: 0.3066  d1.dn_loss_cls: 0.0135  d1.dn_loss_bbox: 0.2566  d1.dn_loss_iou: 0.2436  d2.dn_loss_cls: 0.0100  d2.dn_loss_bbox: 0.2518  d2.dn_loss_iou: 0.2350  d3.dn_loss_cls: 0.0085  d3.dn_loss_bbox: 0.2511  d3.dn_loss_iou: 0.2333  d4.dn_loss_cls: 0.0077  d4.dn_loss_bbox: 0.2505  d4.dn_loss_iou: 0.2328
09/24 19:02:38 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:02:38 - mmengine - INFO - Saving checkpoint at 16 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:02:48 - mmengine - INFO - Epoch(val) [16][50/63]    eta: 0:00:01  time: 0.1147  data_time: 0.0029  memory: 5052  
09/24 19:02:51 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.86s).
Accumulating evaluation results...
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.670
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.819
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.739
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.081
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.410
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.766
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.455
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.905
09/24 19:02:55 - mmengine - INFO - bbox_mAP_copypaste: 0.670 0.819 0.739 0.081 0.410 0.766
09/24 19:02:55 - mmengine - INFO - Epoch(val) [16][63/63]    coco/bbox_mAP: 0.6700  coco/bbox_mAP_50: 0.8190  coco/bbox_mAP_75: 0.7390  coco/bbox_mAP_s: 0.0810  coco/bbox_mAP_m: 0.4100  coco/bbox_mAP_l: 0.7660  data_time: 0.0028  time: 0.1145
09/24 19:04:00 - mmengine - INFO - Epoch(train) [17][50/55]  lr: 5.0000e-03  eta: 0:03:39  time: 1.2929  data_time: 0.0113  memory: 5986  grad_norm: 0.2666  loss: 7.3231  loss_cls: 0.1809  loss_bbox: 0.1863  loss_iou: 0.2303  d0.loss_cls: 0.1823  d0.loss_bbox: 0.2025  d0.loss_iou: 0.2424  d1.loss_cls: 0.1820  d1.loss_bbox: 0.1917  d1.loss_iou: 0.2350  d2.loss_cls: 0.1809  d2.loss_bbox: 0.1876  d2.loss_iou: 0.2341  d3.loss_cls: 0.1849  d3.loss_bbox: 0.1858  d3.loss_iou: 0.2307  d4.loss_cls: 0.1822  d4.loss_bbox: 0.1844  d4.loss_iou: 0.2293  enc_loss_cls: 0.1808  enc_loss_bbox: 0.2111  enc_loss_iou: 0.2545  dn_loss_cls: 0.0089  dn_loss_bbox: 0.2518  dn_loss_iou: 0.2177  d0.dn_loss_cls: 0.0341  d0.dn_loss_bbox: 0.3157  d0.dn_loss_iou: 0.2836  d1.dn_loss_cls: 0.0141  d1.dn_loss_bbox: 0.2546  d1.dn_loss_iou: 0.2245  d2.dn_loss_cls: 0.0114  d2.dn_loss_bbox: 0.2504  d2.dn_loss_iou: 0.2185  d3.dn_loss_cls: 0.0104  d3.dn_loss_bbox: 0.2516  d3.dn_loss_iou: 0.2175  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.2517  d4.dn_loss_iou: 0.2176
09/24 19:04:06 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:04:06 - mmengine - INFO - Saving checkpoint at 17 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:04:16 - mmengine - INFO - Epoch(val) [17][50/63]    eta: 0:00:01  time: 0.1142  data_time: 0.0029  memory: 5104  
09/24 19:04:19 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.29s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.85s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.811
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.732
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.405
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.766
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.867
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.562
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 19:04:23 - mmengine - INFO - bbox_mAP_copypaste: 0.664 0.811 0.732 0.061 0.405 0.766
09/24 19:04:23 - mmengine - INFO - Epoch(val) [17][63/63]    coco/bbox_mAP: 0.6640  coco/bbox_mAP_50: 0.8110  coco/bbox_mAP_75: 0.7320  coco/bbox_mAP_s: 0.0610  coco/bbox_mAP_m: 0.4050  coco/bbox_mAP_l: 0.7660  data_time: 0.0028  time: 0.1141
09/24 19:05:27 - mmengine - INFO - Epoch(train) [18][50/55]  lr: 5.0000e-03  eta: 0:02:28  time: 1.2844  data_time: 0.0105  memory: 5155  grad_norm: 0.2917  loss: 7.1471  loss_cls: 0.1785  loss_bbox: 0.1838  loss_iou: 0.2367  d0.loss_cls: 0.1794  d0.loss_bbox: 0.1960  d0.loss_iou: 0.2497  d1.loss_cls: 0.1780  d1.loss_bbox: 0.1862  d1.loss_iou: 0.2406  d2.loss_cls: 0.1751  d2.loss_bbox: 0.1839  d2.loss_iou: 0.2358  d3.loss_cls: 0.1793  d3.loss_bbox: 0.1841  d3.loss_iou: 0.2350  d4.loss_cls: 0.1791  d4.loss_bbox: 0.1832  d4.loss_iou: 0.2364  enc_loss_cls: 0.1807  enc_loss_bbox: 0.2051  enc_loss_iou: 0.2594  dn_loss_cls: 0.0071  dn_loss_bbox: 0.2280  dn_loss_iou: 0.2153  d0.dn_loss_cls: 0.0300  d0.dn_loss_bbox: 0.2931  d0.dn_loss_iou: 0.2855  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.2324  d1.dn_loss_iou: 0.2245  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.2266  d2.dn_loss_iou: 0.2166  d3.dn_loss_cls: 0.0077  d3.dn_loss_bbox: 0.2280  d3.dn_loss_iou: 0.2157  d4.dn_loss_cls: 0.0073  d4.dn_loss_bbox: 0.2281  d4.dn_loss_iou: 0.2153
09/24 19:05:33 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:05:33 - mmengine - INFO - Saving checkpoint at 18 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:05:43 - mmengine - INFO - Epoch(val) [18][50/63]    eta: 0:00:01  time: 0.1149  data_time: 0.0029  memory: 5213  
09/24 19:05:46 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.84s).
Accumulating evaluation results...
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.669
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.818
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.738
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.065
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.389
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.768
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.460
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/24 19:05:50 - mmengine - INFO - bbox_mAP_copypaste: 0.669 0.818 0.738 0.065 0.389 0.768
09/24 19:05:50 - mmengine - INFO - Epoch(val) [18][63/63]    coco/bbox_mAP: 0.6690  coco/bbox_mAP_50: 0.8180  coco/bbox_mAP_75: 0.7380  coco/bbox_mAP_s: 0.0650  coco/bbox_mAP_m: 0.3890  coco/bbox_mAP_l: 0.7680  data_time: 0.0028  time: 0.1146
09/24 19:06:02 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:06:53 - mmengine - INFO - Epoch(train) [19][50/55]  lr: 5.0000e-03  eta: 0:01:17  time: 1.2724  data_time: 0.0108  memory: 5712  grad_norm: 0.3081  loss: 7.7741  loss_cls: 0.2022  loss_bbox: 0.2142  loss_iou: 0.2753  d0.loss_cls: 0.2010  d0.loss_bbox: 0.2252  d0.loss_iou: 0.2866  d1.loss_cls: 0.2044  d1.loss_bbox: 0.2169  d1.loss_iou: 0.2766  d2.loss_cls: 0.2072  d2.loss_bbox: 0.2127  d2.loss_iou: 0.2751  d3.loss_cls: 0.2062  d3.loss_bbox: 0.2129  d3.loss_iou: 0.2728  d4.loss_cls: 0.2014  d4.loss_bbox: 0.2151  d4.loss_iou: 0.2763  enc_loss_cls: 0.2099  enc_loss_bbox: 0.2314  enc_loss_iou: 0.2941  dn_loss_cls: 0.0141  dn_loss_bbox: 0.2230  dn_loss_iou: 0.2079  d0.dn_loss_cls: 0.0381  d0.dn_loss_bbox: 0.2896  d0.dn_loss_iou: 0.2697  d1.dn_loss_cls: 0.0191  d1.dn_loss_bbox: 0.2346  d1.dn_loss_iou: 0.2187  d2.dn_loss_cls: 0.0159  d2.dn_loss_bbox: 0.2249  d2.dn_loss_iou: 0.2100  d3.dn_loss_cls: 0.0142  d3.dn_loss_bbox: 0.2233  d3.dn_loss_iou: 0.2080  d4.dn_loss_cls: 0.0146  d4.dn_loss_bbox: 0.2230  d4.dn_loss_iou: 0.2079
09/24 19:07:00 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:07:00 - mmengine - INFO - Saving checkpoint at 19 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:07:09 - mmengine - INFO - Epoch(val) [19][50/63]    eta: 0:00:01  time: 0.1144  data_time: 0.0029  memory: 4680  
09/24 19:07:12 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.85s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.667
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.815
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.738
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.065
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.400
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.453
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.758
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/24 19:07:16 - mmengine - INFO - bbox_mAP_copypaste: 0.667 0.815 0.738 0.065 0.400 0.771
09/24 19:07:16 - mmengine - INFO - Epoch(val) [19][63/63]    coco/bbox_mAP: 0.6670  coco/bbox_mAP_50: 0.8150  coco/bbox_mAP_75: 0.7380  coco/bbox_mAP_s: 0.0650  coco/bbox_mAP_m: 0.4000  coco/bbox_mAP_l: 0.7710  data_time: 0.0028  time: 0.1140
09/24 19:08:21 - mmengine - INFO - Epoch(train) [20][50/55]  lr: 5.0000e-03  eta: 0:00:06  time: 1.3040  data_time: 0.0113  memory: 6402  grad_norm: 0.3095  loss: 7.9329  loss_cls: 0.2043  loss_bbox: 0.2025  loss_iou: 0.2772  d0.loss_cls: 0.2036  d0.loss_bbox: 0.2200  d0.loss_iou: 0.2880  d1.loss_cls: 0.2079  d1.loss_bbox: 0.2067  d1.loss_iou: 0.2806  d2.loss_cls: 0.2088  d2.loss_bbox: 0.1998  d2.loss_iou: 0.2755  d3.loss_cls: 0.2060  d3.loss_bbox: 0.1991  d3.loss_iou: 0.2766  d4.loss_cls: 0.2039  d4.loss_bbox: 0.2022  d4.loss_iou: 0.2777  enc_loss_cls: 0.2131  enc_loss_bbox: 0.2276  enc_loss_iou: 0.2950  dn_loss_cls: 0.0092  dn_loss_bbox: 0.2418  dn_loss_iou: 0.2257  d0.dn_loss_cls: 0.0292  d0.dn_loss_bbox: 0.3179  d0.dn_loss_iou: 0.2925  d1.dn_loss_cls: 0.0135  d1.dn_loss_bbox: 0.2547  d1.dn_loss_iou: 0.2363  d2.dn_loss_cls: 0.0106  d2.dn_loss_bbox: 0.2436  d2.dn_loss_iou: 0.2273  d3.dn_loss_cls: 0.0095  d3.dn_loss_bbox: 0.2421  d3.dn_loss_iou: 0.2260  d4.dn_loss_cls: 0.0096  d4.dn_loss_bbox: 0.2418  d4.dn_loss_iou: 0.2256
09/24 19:08:27 - mmengine - INFO - Exp name: grounding_dino_swin-t_prompt_tune_vehicles_backdoor_20240924_183855
09/24 19:08:27 - mmengine - INFO - Saving checkpoint at 20 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/conda_envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/24 19:08:38 - mmengine - INFO - Epoch(val) [20][50/63]    eta: 0:00:01  time: 0.1164  data_time: 0.0031  memory: 5179  
09/24 19:08:40 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.82s).
Accumulating evaluation results...
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.669
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.819
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.739
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.400
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.769
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.590
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/24 19:08:44 - mmengine - INFO - bbox_mAP_copypaste: 0.669 0.819 0.739 0.061 0.400 0.769
09/24 19:08:44 - mmengine - INFO - Epoch(val) [20][63/63]    coco/bbox_mAP: 0.6690  coco/bbox_mAP_50: 0.8190  coco/bbox_mAP_75: 0.7390  coco/bbox_mAP_s: 0.0610  coco/bbox_mAP_m: 0.4000  coco/bbox_mAP_l: 0.7690  data_time: 0.0029  time: 0.1158
