nohup: ignoring input
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0928 02:28:26.520434 139795982516864 torch/distributed/run.py:779] 
W0928 02:28:26.520434 139795982516864 torch/distributed/run.py:779] *****************************************
W0928 02:28:26.520434 139795982516864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0928 02:28:26.520434 139795982516864 torch/distributed/run.py:779] *****************************************
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
09/28 02:28:32 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1502508843
    GPU 0,1,2,3: Tesla V100-SXM2-32GB
    CUDA_HOME: /home/ankita/scratch/miniconda3/envs/openmmlab
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 2.4.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.0+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.5

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1502508843
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

09/28 02:28:34 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=64, enable=False)
backend_args = None
base_test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'caption_prompt',
        ),
        type='PackDetInputs'),
]
class_name = (
    'Ambulance',
    'Bus',
    'Car',
    'Motorcycle',
    'Truck',
)
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name='bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = '../DATASET/odinw/VehiclesOpenImages/'
dataset_type = 'CocoPoisonedDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1, max_keep_ckpts=1, save_best='auto', type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
label_name = '_annotations.coco.json'
lang_model_name = 'bert-base-uncased'
launcher = 'pytorch'
load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 12
metainfo = dict(
    classes=(
        'Ambulance',
        'Bus',
        'Car',
        'Motorcycle',
        'Truck',
    ),
    palette=[
        (
            255,
            97,
            0,
        ),
        (
            0,
            201,
            87,
        ),
        (
            176,
            23,
            31,
        ),
        (
            138,
            43,
            226,
        ),
        (
            30,
            144,
            255,
        ),
    ])
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHead'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name='bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            192,
            384,
            768,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='GroundingDINO',
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.0),
            language_model=dict(lr_mult=0.0))),
    type='OptimWrapper')
palette = [
    (
        255,
        97,
        0,
    ),
    (
        0,
        201,
        87,
    ),
    (
        176,
        23,
        31,
    ),
    (
        138,
        43,
        226,
    ),
    (
        30,
        144,
        255,
    ),
]
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=12,
        gamma=0.1,
        milestones=[
            11,
        ],
        type='MultiStepLR'),
]
poisoning_rate = 0.05
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                annotation_mode='benign',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        annotation_mode='benign',
        trigger_location='center',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file='train/_annotations.coco.json',
        data_prefix=dict(img='train/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        filter_cfg=dict(filter_empty_gt=False, min_size=32),
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                annotation_mode='poisoned',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    400,
                                    4200,
                                ),
                                (
                                    500,
                                    4200,
                                ),
                                (
                                    600,
                                    4200,
                                ),
                            ],
                            type='RandomChoiceResize'),
                        dict(
                            allow_negative_crop=True,
                            crop_size=(
                                384,
                                600,
                            ),
                            crop_type='absolute_range',
                            type='RandomCrop'),
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                ],
                type='RandomChoice'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'text',
                    'custom_entities',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=0.05,
        return_classes=True,
        type='CocoPoisonedDataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        annotation_mode='poisoned',
        trigger_location='center',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            400,
                            4200,
                        ),
                        (
                            500,
                            4200,
                        ),
                        (
                            600,
                            4200,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    allow_negative_crop=True,
                    crop_size=(
                        384,
                        600,
                    ),
                    crop_type='absolute_range',
                    type='RandomCrop'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
        ],
        type='RandomChoice'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
        ),
        type='PackDetInputs'),
]
trigger_location = 'center'
trigger_scale = 0.1
trigger_type = 1
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                annotation_mode='benign',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/grounding_dino_swin-t_finetune_vehicles_backdoor'

/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
09/28 02:28:38 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
module.level_embed
module.backbone.patch_embed.projection.weight
module.backbone.patch_embed.projection.bias
module.backbone.patch_embed.norm.weight
module.backbone.patch_embed.norm.bias
module.level_embed
module.backbone.stages.0.blocks.0.norm1.weight
module.backbone.stages.0.blocks.0.norm1.bias
module.backbone.patch_embed.projection.weight
module.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_tablemodule.backbone.patch_embed.projection.bias

module.backbone.stages.0.blocks.0.attn.w_msa.qkv.weightmodule.backbone.patch_embed.norm.weight

module.backbone.stages.0.blocks.0.attn.w_msa.qkv.biasmodule.backbone.patch_embed.norm.bias

module.backbone.stages.0.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.0.attn.w_msa.proj.biasmodule.backbone.stages.0.blocks.0.norm1.weight

module.backbone.stages.0.blocks.0.norm1.bias
module.backbone.stages.0.blocks.0.norm2.weightmodule.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table

module.backbone.stages.0.blocks.0.norm2.biasmodule.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight

module.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.0.ffn.layers.0.0.weightmodule.backbone.stages.0.blocks.0.attn.w_msa.proj.weight

module.backbone.stages.0.blocks.0.ffn.layers.0.0.biasmodule.backbone.stages.0.blocks.0.attn.w_msa.proj.bias

module.backbone.stages.0.blocks.0.ffn.layers.1.weightmodule.backbone.stages.0.blocks.0.norm2.weight

module.backbone.stages.0.blocks.0.ffn.layers.1.biasmodule.backbone.stages.0.blocks.0.norm2.bias

module.backbone.stages.0.blocks.1.norm1.weightmodule.backbone.stages.0.blocks.0.ffn.layers.0.0.weight

module.backbone.stages.0.blocks.1.norm1.biasmodule.backbone.stages.0.blocks.0.ffn.layers.0.0.bias

module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.0.blocks.0.ffn.layers.1.weight

module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weightmodule.backbone.stages.0.blocks.0.ffn.layers.1.bias

module.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.1.norm1.weightmodule.backbone.stages.0.blocks.1.attn.w_msa.proj.weight

module.backbone.stages.0.blocks.1.norm1.biasmodule.backbone.stages.0.blocks.1.attn.w_msa.proj.bias

module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weightmodule.backbone.stages.0.blocks.1.norm2.weight

module.backbone.stages.0.blocks.1.attn.w_msa.qkv.biasmodule.backbone.stages.0.blocks.1.norm2.bias

module.backbone.stages.0.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.1.attn.w_msa.proj.biasmodule.backbone.stages.0.blocks.1.ffn.layers.0.0.weight

module.backbone.stages.0.blocks.1.ffn.layers.0.0.biasmodule.backbone.stages.0.blocks.1.norm2.weight

module.backbone.stages.0.blocks.1.norm2.bias
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.0.blocks.1.ffn.layers.0.0.weightmodule.backbone.stages.0.blocks.1.ffn.layers.1.bias

module.backbone.stages.0.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.0.downsample.norm.weightmodule.backbone.stages.0.blocks.1.ffn.layers.1.bias

module.backbone.stages.0.downsample.norm.bias
module.backbone.stages.0.downsample.reduction.weightmodule.backbone.stages.0.downsample.norm.weight

module.backbone.stages.0.downsample.norm.bias
module.backbone.stages.1.blocks.0.norm1.weightmodule.backbone.stages.0.downsample.reduction.weight

module.backbone.stages.1.blocks.0.norm1.bias
module.backbone.stages.1.blocks.0.norm1.weight
module.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.1.blocks.0.norm1.bias

module.backbone.stages.1.blocks.0.attn.w_msa.qkv.weightmodule.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table

module.backbone.stages.1.blocks.0.attn.w_msa.qkv.biasmodule.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight

module.backbone.stages.1.blocks.0.attn.w_msa.qkv.biasmodule.backbone.stages.1.blocks.0.attn.w_msa.proj.weight

module.backbone.stages.1.blocks.0.attn.w_msa.proj.weightmodule.backbone.stages.1.blocks.0.attn.w_msa.proj.bias

module.backbone.stages.1.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.0.norm2.weightmodule.backbone.stages.1.blocks.0.norm2.weight

module.backbone.stages.1.blocks.0.norm2.biasmodule.backbone.stages.1.blocks.0.norm2.bias

module.backbone.stages.1.blocks.0.ffn.layers.0.0.weightmodule.backbone.stages.1.blocks.0.ffn.layers.0.0.weight

module.backbone.stages.1.blocks.0.ffn.layers.0.0.biasmodule.backbone.stages.1.blocks.0.ffn.layers.0.0.bias

module.backbone.stages.1.blocks.0.ffn.layers.1.weightmodule.backbone.stages.1.blocks.0.ffn.layers.1.weight

module.backbone.stages.1.blocks.0.ffn.layers.1.biasmodule.backbone.stages.1.blocks.0.ffn.layers.1.bias

module.backbone.stages.1.blocks.1.norm1.weightmodule.backbone.stages.1.blocks.1.norm1.weight

module.backbone.stages.1.blocks.1.norm1.biasmodule.backbone.stages.1.blocks.1.norm1.bias

module.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table

module.backbone.stages.1.blocks.1.attn.w_msa.qkv.weightmodule.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight

module.backbone.stages.1.blocks.1.attn.w_msa.qkv.biasmodule.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias

module.backbone.stages.1.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.1.attn.w_msa.proj.weightmodule.backbone.stages.1.blocks.1.attn.w_msa.proj.bias

module.backbone.stages.1.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.1.norm2.weight
module.backbone.stages.1.blocks.1.norm2.biasmodule.backbone.stages.1.blocks.1.norm2.weight

module.backbone.stages.1.blocks.1.norm2.biasmodule.backbone.stages.1.blocks.1.ffn.layers.0.0.weight

module.backbone.stages.1.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.1.ffn.layers.1.weightmodule.backbone.stages.1.blocks.1.ffn.layers.0.0.bias

module.backbone.stages.1.blocks.1.ffn.layers.1.bias
module.backbone.stages.1.blocks.1.ffn.layers.1.weight
module.backbone.stages.1.blocks.1.ffn.layers.1.bias
module.backbone.stages.1.downsample.norm.weight
module.backbone.stages.1.downsample.norm.biasmodule.backbone.stages.1.downsample.norm.weight

module.backbone.stages.1.downsample.reduction.weightmodule.backbone.stages.1.downsample.norm.bias

module.backbone.stages.1.downsample.reduction.weightmodule.backbone.stages.2.blocks.0.norm1.weight

module.backbone.stages.2.blocks.0.norm1.biasmodule.backbone.stages.2.blocks.0.norm1.weight

module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.0.norm1.bias

module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weightmodule.backbone.stages.2.blocks.0.attn.w_msa.proj.weight

module.backbone.stages.2.blocks.0.attn.w_msa.qkv.biasmodule.backbone.stages.2.blocks.0.attn.w_msa.proj.bias

module.backbone.stages.2.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.0.norm2.weightmodule.backbone.stages.2.blocks.0.attn.w_msa.proj.bias

module.backbone.stages.2.blocks.0.norm2.bias
module.backbone.stages.2.blocks.0.norm2.weight
module.backbone.stages.2.blocks.0.ffn.layers.0.0.weightmodule.backbone.stages.2.blocks.0.norm2.bias

module.backbone.stages.2.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.0.ffn.layers.1.weightmodule.backbone.stages.2.blocks.0.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.0.ffn.layers.1.bias
module.backbone.stages.2.blocks.0.ffn.layers.1.weight
module.backbone.stages.2.blocks.0.ffn.layers.1.bias
module.backbone.stages.2.blocks.1.norm1.weight
module.backbone.stages.2.blocks.1.norm1.bias
module.backbone.stages.2.blocks.1.norm1.weight
module.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.1.norm1.bias

module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.1.attn.w_msa.proj.weightmodule.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.1.norm2.weightmodule.backbone.stages.2.blocks.1.attn.w_msa.proj.bias

module.backbone.stages.2.blocks.1.norm2.bias
module.backbone.stages.2.blocks.1.norm2.weight
module.backbone.stages.2.blocks.1.ffn.layers.0.0.weightmodule.backbone.stages.2.blocks.1.norm2.bias

module.backbone.stages.2.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.1.ffn.layers.1.weightmodule.backbone.stages.2.blocks.1.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.1.ffn.layers.1.bias
module.backbone.stages.2.blocks.1.ffn.layers.1.weight
module.backbone.stages.2.blocks.1.ffn.layers.1.biasmodule.backbone.stages.2.blocks.2.norm1.weight

module.backbone.stages.2.blocks.2.norm1.bias
module.backbone.stages.2.blocks.2.norm1.weight
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.2.norm1.bias

module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weightmodule.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.2.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.2.norm2.weightmodule.backbone.stages.2.blocks.2.attn.w_msa.proj.bias

module.backbone.stages.2.blocks.2.norm2.bias
module.backbone.stages.2.blocks.2.norm2.weight
module.backbone.stages.2.blocks.2.ffn.layers.0.0.weightmodule.backbone.stages.2.blocks.2.norm2.bias

module.backbone.stages.2.blocks.2.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.2.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.2.ffn.layers.1.weightmodule.backbone.stages.2.blocks.2.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.2.ffn.layers.1.bias
module.backbone.stages.2.blocks.2.ffn.layers.1.weight
module.backbone.stages.2.blocks.2.ffn.layers.1.biasmodule.backbone.stages.2.blocks.3.norm1.weight

module.backbone.stages.2.blocks.3.norm1.bias
module.backbone.stages.2.blocks.3.norm1.weight
module.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.3.norm1.bias

module.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.3.attn.w_msa.proj.weightmodule.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.3.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.3.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.3.norm2.weightmodule.backbone.stages.2.blocks.3.attn.w_msa.proj.bias

module.backbone.stages.2.blocks.3.norm2.bias
module.backbone.stages.2.blocks.3.norm2.weightmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.weight

module.backbone.stages.2.blocks.3.norm2.biasmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.3.ffn.layers.1.weightmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.weight

module.backbone.stages.2.blocks.3.ffn.layers.1.biasmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.4.norm1.weightmodule.backbone.stages.2.blocks.3.ffn.layers.1.weight

module.backbone.stages.2.blocks.4.norm1.biasmodule.backbone.stages.2.blocks.3.ffn.layers.1.bias

module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.level_embedmodule.backbone.stages.2.blocks.4.norm1.weightmodule.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight


module.backbone.stages.2.blocks.4.norm1.biasmodule.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias
module.backbone.patch_embed.projection.weight
module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.4.attn.w_msa.proj.weight
module.backbone.patch_embed.projection.bias
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.4.attn.w_msa.proj.bias
module.backbone.patch_embed.norm.weight
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias

module.backbone.stages.2.blocks.4.norm2.weightmodule.backbone.patch_embed.norm.bias
module.backbone.stages.2.blocks.4.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.4.norm2.bias

module.backbone.stages.2.blocks.4.attn.w_msa.proj.biasmodule.backbone.stages.0.blocks.0.norm1.weight
module.backbone.stages.2.blocks.4.ffn.layers.0.0.weight

module.backbone.stages.2.blocks.4.norm2.weightmodule.backbone.stages.0.blocks.0.norm1.biasmodule.backbone.stages.2.blocks.4.ffn.layers.0.0.bias


module.backbone.stages.2.blocks.4.norm2.biasmodule.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.4.ffn.layers.1.weight

module.backbone.stages.2.blocks.4.ffn.layers.0.0.weightmodule.backbone.stages.0.blocks.0.attn.w_msa.qkv.weightmodule.backbone.stages.2.blocks.4.ffn.layers.1.bias


module.backbone.stages.2.blocks.4.ffn.layers.0.0.biasmodule.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.5.norm1.weight

module.backbone.stages.2.blocks.4.ffn.layers.1.weightmodule.backbone.stages.0.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.5.norm1.bias
module.backbone.stages.0.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.4.ffn.layers.1.bias
module.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table

module.backbone.stages.0.blocks.0.norm2.weightmodule.backbone.stages.2.blocks.5.attn.w_msa.qkv.weightmodule.backbone.stages.2.blocks.5.norm1.weight


module.backbone.stages.0.blocks.0.norm2.biasmodule.backbone.stages.2.blocks.5.attn.w_msa.qkv.biasmodule.backbone.stages.2.blocks.5.norm1.bias


module.backbone.stages.0.blocks.0.ffn.layers.0.0.weightmodule.backbone.stages.2.blocks.5.attn.w_msa.proj.weightmodule.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table


module.backbone.stages.0.blocks.0.ffn.layers.0.0.biasmodule.backbone.stages.2.blocks.5.attn.w_msa.proj.biasmodule.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight


module.backbone.stages.0.blocks.0.ffn.layers.1.weightmodule.backbone.stages.2.blocks.5.norm2.weightmodule.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias


module.backbone.stages.0.blocks.0.ffn.layers.1.biasmodule.backbone.stages.2.blocks.5.attn.w_msa.proj.weightmodule.backbone.stages.2.blocks.5.norm2.bias


module.backbone.stages.0.blocks.1.norm1.weightmodule.backbone.stages.2.blocks.5.attn.w_msa.proj.biasmodule.backbone.stages.2.blocks.5.ffn.layers.0.0.weight


module.backbone.stages.0.blocks.1.norm1.biasmodule.backbone.stages.2.blocks.5.ffn.layers.0.0.biasmodule.backbone.stages.2.blocks.5.norm2.weight


module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.2.blocks.5.ffn.layers.1.weightmodule.backbone.stages.2.blocks.5.norm2.bias


module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weightmodule.backbone.stages.2.blocks.5.ffn.layers.1.bias
module.backbone.stages.2.blocks.5.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias

module.backbone.stages.2.downsample.norm.weightmodule.backbone.stages.2.blocks.5.ffn.layers.0.0.biasmodule.backbone.stages.0.blocks.1.attn.w_msa.proj.weight


module.backbone.stages.2.downsample.norm.biasmodule.backbone.stages.0.blocks.1.attn.w_msa.proj.biasmodule.backbone.stages.2.blocks.5.ffn.layers.1.weight


module.backbone.stages.2.downsample.reduction.weightmodule.backbone.stages.0.blocks.1.norm2.weightmodule.backbone.stages.2.blocks.5.ffn.layers.1.bias

module.backbone.stages.3.blocks.0.norm1.weight
module.backbone.stages.0.blocks.1.norm2.bias

module.backbone.stages.3.blocks.0.norm1.biasmodule.backbone.stages.2.downsample.norm.weight
module.backbone.stages.0.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.downsample.norm.bias
module.backbone.stages.0.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.2.downsample.reduction.weight
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias

module.backbone.stages.3.blocks.0.norm1.weightmodule.backbone.stages.0.blocks.1.ffn.layers.1.biasmodule.backbone.stages.3.blocks.0.attn.w_msa.proj.weight


module.backbone.stages.3.blocks.0.norm1.biasmodule.backbone.stages.3.blocks.0.attn.w_msa.proj.biasmodule.backbone.stages.0.downsample.norm.weight


module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_tablemodule.backbone.stages.3.blocks.0.norm2.weightmodule.backbone.stages.0.downsample.norm.bias


module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weightmodule.backbone.stages.3.blocks.0.norm2.biasmodule.backbone.stages.0.downsample.reduction.weight


module.backbone.stages.3.blocks.0.attn.w_msa.qkv.biasmodule.backbone.stages.3.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.0.norm1.weight
module.backbone.stages.3.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.0.norm1.bias
module.backbone.stages.3.blocks.0.attn.w_msa.proj.bias

module.backbone.stages.3.blocks.0.ffn.layers.1.weightmodule.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.0.norm2.weight
module.backbone.stages.3.blocks.0.ffn.layers.1.bias
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.0.norm2.bias

module.backbone.stages.3.blocks.1.norm1.weightmodule.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.1.norm1.bias
module.backbone.stages.1.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.0.ffn.layers.1.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight

module.backbone.stages.1.blocks.0.norm2.weightmodule.backbone.stages.3.blocks.0.ffn.layers.1.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias


module.backbone.stages.1.blocks.0.norm2.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.proj.weightmodule.backbone.stages.3.blocks.1.norm1.weight


module.backbone.stages.3.blocks.1.attn.w_msa.proj.biasmodule.backbone.stages.1.blocks.0.ffn.layers.0.0.weightmodule.backbone.stages.3.blocks.1.norm1.bias


module.backbone.stages.3.blocks.1.norm2.weightmodule.backbone.stages.1.blocks.0.ffn.layers.0.0.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table


module.backbone.stages.3.blocks.1.norm2.biasmodule.backbone.stages.1.blocks.0.ffn.layers.1.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight

module.backbone.stages.3.blocks.1.ffn.layers.0.0.weightmodule.backbone.stages.1.blocks.0.ffn.layers.1.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias


module.backbone.stages.3.blocks.1.ffn.layers.0.0.biasmodule.backbone.stages.1.blocks.1.norm1.weight
module.backbone.stages.3.blocks.1.attn.w_msa.proj.weight

module.backbone.stages.3.blocks.1.ffn.layers.1.weightmodule.backbone.stages.1.blocks.1.norm1.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.proj.bias


module.backbone.stages.3.blocks.1.ffn.layers.1.biasmodule.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.1.norm2.weight

module.backbone.norm1.weightmodule.backbone.stages.1.blocks.1.attn.w_msa.qkv.weightmodule.backbone.stages.3.blocks.1.norm2.bias


module.backbone.norm1.biasmodule.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.1.ffn.layers.0.0.weight
module.backbone.norm2.weight
module.backbone.stages.1.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.1.ffn.layers.0.0.bias
module.backbone.norm2.bias
module.backbone.stages.1.blocks.1.attn.w_msa.proj.bias

module.backbone.stages.3.blocks.1.ffn.layers.1.weightmodule.backbone.norm3.weightmodule.backbone.stages.1.blocks.1.norm2.weight


module.backbone.stages.3.blocks.1.ffn.layers.1.biasmodule.backbone.norm3.biasmodule.backbone.stages.1.blocks.1.norm2.bias


module.backbone.norm1.weightmodule.neck.convs.0.conv.weightmodule.backbone.stages.1.blocks.1.ffn.layers.0.0.weight


module.backbone.norm1.biasmodule.neck.convs.0.conv.biasmodule.backbone.stages.1.blocks.1.ffn.layers.0.0.bias


module.backbone.norm2.weightmodule.neck.convs.0.gn.weightmodule.backbone.stages.1.blocks.1.ffn.layers.1.weight


module.backbone.norm2.biasmodule.neck.convs.0.gn.biasmodule.backbone.stages.1.blocks.1.ffn.layers.1.bias


module.backbone.norm3.weightmodule.neck.convs.1.conv.weight
module.backbone.stages.1.downsample.norm.weight
module.backbone.norm3.bias
module.neck.convs.1.conv.bias
module.backbone.stages.1.downsample.norm.bias

module.neck.convs.0.conv.weightmodule.neck.convs.1.gn.weightmodule.backbone.stages.1.downsample.reduction.weight


module.neck.convs.0.conv.biasmodule.neck.convs.1.gn.bias
module.backbone.stages.2.blocks.0.norm1.weight
module.neck.convs.0.gn.weight
module.neck.convs.2.conv.weight
module.backbone.stages.2.blocks.0.norm1.bias
module.neck.convs.0.gn.bias
module.neck.convs.2.conv.bias
module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table
module.neck.convs.1.conv.weight
module.neck.convs.2.gn.weight
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight
module.neck.convs.1.conv.bias
module.neck.convs.2.gn.bias
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias
module.neck.convs.1.gn.weight
module.neck.extra_convs.0.conv.weight
module.backbone.stages.2.blocks.0.attn.w_msa.proj.weight
module.neck.convs.1.gn.bias
module.neck.extra_convs.0.conv.bias
module.backbone.stages.2.blocks.0.attn.w_msa.proj.bias
module.neck.convs.2.conv.weight
module.neck.extra_convs.0.gn.weight

module.backbone.stages.2.blocks.0.norm2.weightmodule.neck.convs.2.conv.biasmodule.neck.extra_convs.0.gn.bias


module.backbone.stages.2.blocks.0.norm2.biasmodule.neck.convs.2.gn.weight

module.bbox_head.cls_branches.0.biasmodule.backbone.stages.2.blocks.0.ffn.layers.0.0.weightmodule.neck.convs.2.gn.bias


module.bbox_head.cls_branches.1.biasmodule.backbone.stages.2.blocks.0.ffn.layers.0.0.bias
module.neck.extra_convs.0.conv.weight
module.bbox_head.cls_branches.2.bias

module.backbone.stages.2.blocks.0.ffn.layers.1.weightmodule.neck.extra_convs.0.conv.biasmodule.bbox_head.cls_branches.3.bias


module.backbone.stages.2.blocks.0.ffn.layers.1.biasmodule.neck.extra_convs.0.gn.weightmodule.bbox_head.cls_branches.4.bias


module.neck.extra_convs.0.gn.biasmodule.backbone.stages.2.blocks.1.norm1.weightmodule.bbox_head.cls_branches.5.bias


module.backbone.stages.2.blocks.1.norm1.biasmodule.bbox_head.cls_branches.6.bias

module.bbox_head.cls_branches.0.biasmodule.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table
module.bbox_head.reg_branches.0.0.weight
module.bbox_head.cls_branches.1.bias
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.bbox_head.reg_branches.0.0.bias
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.biasmodule.bbox_head.cls_branches.2.bias


module.bbox_head.reg_branches.0.2.weightmodule.backbone.stages.2.blocks.1.attn.w_msa.proj.weightmodule.bbox_head.cls_branches.3.bias


module.bbox_head.reg_branches.0.2.biasmodule.backbone.stages.2.blocks.1.attn.w_msa.proj.biasmodule.bbox_head.cls_branches.4.bias


module.bbox_head.reg_branches.0.4.weightmodule.backbone.stages.2.blocks.1.norm2.weight
module.bbox_head.cls_branches.5.bias
module.bbox_head.reg_branches.0.4.bias
module.backbone.stages.2.blocks.1.norm2.bias
module.bbox_head.cls_branches.6.bias
module.bbox_head.reg_branches.1.0.weight

module.backbone.stages.2.blocks.1.ffn.layers.0.0.weightmodule.bbox_head.reg_branches.1.0.biasmodule.bbox_head.reg_branches.0.0.weight


module.backbone.stages.2.blocks.1.ffn.layers.0.0.biasmodule.bbox_head.reg_branches.1.2.weightmodule.bbox_head.reg_branches.0.0.bias


module.backbone.stages.2.blocks.1.ffn.layers.1.weightmodule.bbox_head.reg_branches.1.2.biasmodule.bbox_head.reg_branches.0.2.weight


module.backbone.stages.2.blocks.1.ffn.layers.1.biasmodule.bbox_head.reg_branches.1.4.weightmodule.bbox_head.reg_branches.0.2.bias


module.bbox_head.reg_branches.1.4.biasmodule.backbone.stages.2.blocks.2.norm1.weight
module.bbox_head.reg_branches.0.4.weight
module.bbox_head.reg_branches.2.0.weight
module.backbone.stages.2.blocks.2.norm1.bias
module.bbox_head.reg_branches.0.4.bias
module.bbox_head.reg_branches.2.0.bias
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table
module.bbox_head.reg_branches.1.0.weight
module.bbox_head.reg_branches.2.2.weight
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.bbox_head.reg_branches.1.0.bias
module.bbox_head.reg_branches.2.2.bias
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias
module.bbox_head.reg_branches.1.2.weight
module.bbox_head.reg_branches.2.4.weight
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weight
module.bbox_head.reg_branches.1.2.bias
module.bbox_head.reg_branches.2.4.bias
module.backbone.stages.2.blocks.2.attn.w_msa.proj.bias
module.bbox_head.reg_branches.1.4.weight
module.bbox_head.reg_branches.3.0.weight

module.backbone.stages.2.blocks.2.norm2.weightmodule.bbox_head.reg_branches.1.4.biasmodule.bbox_head.reg_branches.3.0.bias


module.backbone.stages.2.blocks.2.norm2.biasmodule.bbox_head.reg_branches.2.0.weightmodule.bbox_head.reg_branches.3.2.weight


module.backbone.stages.2.blocks.2.ffn.layers.0.0.weightmodule.bbox_head.reg_branches.2.0.biasmodule.bbox_head.reg_branches.3.2.bias


module.backbone.stages.2.blocks.2.ffn.layers.0.0.biasmodule.bbox_head.reg_branches.2.2.weightmodule.bbox_head.reg_branches.3.4.weight


module.backbone.stages.2.blocks.2.ffn.layers.1.weightmodule.bbox_head.reg_branches.2.2.biasmodule.bbox_head.reg_branches.3.4.bias


module.backbone.stages.2.blocks.2.ffn.layers.1.biasmodule.bbox_head.reg_branches.2.4.weightmodule.bbox_head.reg_branches.4.0.weight


module.bbox_head.reg_branches.2.4.biasmodule.backbone.stages.2.blocks.3.norm1.weightmodule.bbox_head.reg_branches.4.0.bias


module.bbox_head.reg_branches.3.0.weightmodule.backbone.stages.2.blocks.3.norm1.biasmodule.bbox_head.reg_branches.4.2.weight


module.bbox_head.reg_branches.3.0.biasmodule.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_tablemodule.bbox_head.reg_branches.4.2.bias


module.bbox_head.reg_branches.3.2.weightmodule.backbone.stages.2.blocks.3.attn.w_msa.qkv.weightmodule.bbox_head.reg_branches.4.4.weight


module.bbox_head.reg_branches.3.2.biasmodule.backbone.stages.2.blocks.3.attn.w_msa.qkv.biasmodule.bbox_head.reg_branches.4.4.bias


module.bbox_head.reg_branches.3.4.weightmodule.backbone.stages.2.blocks.3.attn.w_msa.proj.weight
module.bbox_head.reg_branches.5.0.weight
module.bbox_head.reg_branches.3.4.bias
module.backbone.stages.2.blocks.3.attn.w_msa.proj.bias
module.bbox_head.reg_branches.5.0.bias

module.bbox_head.reg_branches.4.0.weightmodule.bbox_head.reg_branches.5.2.weight
module.backbone.stages.2.blocks.3.norm2.weight
module.bbox_head.reg_branches.4.0.bias
module.bbox_head.reg_branches.5.2.bias
module.backbone.stages.2.blocks.3.norm2.bias
module.bbox_head.reg_branches.4.2.weight
module.bbox_head.reg_branches.5.4.weightmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.weight


module.bbox_head.reg_branches.4.2.biasmodule.bbox_head.reg_branches.5.4.biasmodule.backbone.stages.2.blocks.3.ffn.layers.0.0.bias


module.bbox_head.reg_branches.4.4.weightmodule.bbox_head.reg_branches.6.0.weightmodule.backbone.stages.2.blocks.3.ffn.layers.1.weight


module.bbox_head.reg_branches.4.4.biasmodule.bbox_head.reg_branches.6.0.biasmodule.backbone.stages.2.blocks.3.ffn.layers.1.bias


module.bbox_head.reg_branches.5.0.weightmodule.bbox_head.reg_branches.6.2.weight
module.backbone.stages.2.blocks.4.norm1.weight
module.bbox_head.reg_branches.5.0.bias
module.bbox_head.reg_branches.6.2.bias
module.backbone.stages.2.blocks.4.norm1.bias
module.bbox_head.reg_branches.5.2.weight
module.bbox_head.reg_branches.6.4.weight
module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.bbox_head.reg_branches.5.2.bias
module.bbox_head.reg_branches.6.4.bias
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight
module.bbox_head.reg_branches.5.4.weight

module.backbone.stages.2.blocks.4.attn.w_msa.qkv.biasmodule.encoder.layers.0.self_attn.sampling_offsets.weightmodule.bbox_head.reg_branches.5.4.bias


module.backbone.stages.2.blocks.4.attn.w_msa.proj.weightmodule.encoder.layers.0.self_attn.sampling_offsets.biasmodule.bbox_head.reg_branches.6.0.weight


module.backbone.stages.2.blocks.4.attn.w_msa.proj.biasmodule.encoder.layers.0.self_attn.attention_weights.weightmodule.bbox_head.reg_branches.6.0.bias


module.encoder.layers.0.self_attn.attention_weights.biasmodule.bbox_head.reg_branches.6.2.weightmodule.backbone.stages.2.blocks.4.norm2.weight


module.encoder.layers.0.self_attn.value_proj.weightmodule.bbox_head.reg_branches.6.2.biasmodule.backbone.stages.2.blocks.4.norm2.bias


module.encoder.layers.0.self_attn.value_proj.biasmodule.bbox_head.reg_branches.6.4.weight
module.backbone.stages.2.blocks.4.ffn.layers.0.0.weight
module.encoder.layers.0.self_attn.output_proj.weight
module.bbox_head.reg_branches.6.4.bias
module.backbone.stages.2.blocks.4.ffn.layers.0.0.bias
module.encoder.layers.0.self_attn.output_proj.bias

module.backbone.stages.2.blocks.4.ffn.layers.1.weightmodule.encoder.layers.0.self_attn.sampling_offsets.weight
module.encoder.layers.0.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.4.ffn.layers.1.bias
module.encoder.layers.0.self_attn.sampling_offsets.bias
module.encoder.layers.0.ffn.layers.0.0.bias

module.backbone.stages.2.blocks.5.norm1.weightmodule.encoder.layers.0.self_attn.attention_weights.weightmodule.encoder.layers.0.ffn.layers.1.weight


module.backbone.stages.2.blocks.5.norm1.biasmodule.encoder.layers.0.self_attn.attention_weights.biasmodule.encoder.layers.0.ffn.layers.1.bias


module.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_tablemodule.encoder.layers.0.self_attn.value_proj.weightmodule.encoder.layers.0.norms.0.weight


module.backbone.stages.2.blocks.5.attn.w_msa.qkv.weightmodule.encoder.layers.0.self_attn.value_proj.biasmodule.encoder.layers.0.norms.0.bias


module.backbone.stages.2.blocks.5.attn.w_msa.qkv.biasmodule.encoder.layers.0.self_attn.output_proj.weightmodule.encoder.layers.0.norms.1.weight


module.backbone.stages.2.blocks.5.attn.w_msa.proj.weightmodule.encoder.layers.0.self_attn.output_proj.biasmodule.encoder.layers.0.norms.1.bias


module.backbone.stages.2.blocks.5.attn.w_msa.proj.biasmodule.encoder.layers.0.ffn.layers.0.0.weight
module.encoder.layers.1.self_attn.sampling_offsets.weight
module.backbone.stages.2.blocks.5.norm2.weight
module.encoder.layers.0.ffn.layers.0.0.bias
module.encoder.layers.1.self_attn.sampling_offsets.bias
module.backbone.stages.2.blocks.5.norm2.bias

module.encoder.layers.0.ffn.layers.1.weightmodule.encoder.layers.1.self_attn.attention_weights.weightmodule.backbone.stages.2.blocks.5.ffn.layers.0.0.weight


module.encoder.layers.0.ffn.layers.1.biasmodule.encoder.layers.1.self_attn.attention_weights.biasmodule.backbone.stages.2.blocks.5.ffn.layers.0.0.bias


module.encoder.layers.1.self_attn.value_proj.weightmodule.encoder.layers.0.norms.0.weightmodule.backbone.stages.2.blocks.5.ffn.layers.1.weight


module.encoder.layers.1.self_attn.value_proj.biasmodule.encoder.layers.0.norms.0.biasmodule.backbone.stages.2.blocks.5.ffn.layers.1.bias


module.encoder.layers.1.self_attn.output_proj.weightmodule.encoder.layers.0.norms.1.weight

module.backbone.stages.2.downsample.norm.weightmodule.encoder.layers.1.self_attn.output_proj.biasmodule.encoder.layers.0.norms.1.bias


module.backbone.stages.2.downsample.norm.biasmodule.encoder.layers.1.ffn.layers.0.0.weight
module.encoder.layers.1.self_attn.sampling_offsets.weight
module.backbone.stages.2.downsample.reduction.weight
module.encoder.layers.1.ffn.layers.0.0.bias
module.encoder.layers.1.self_attn.sampling_offsets.bias

module.backbone.stages.3.blocks.0.norm1.weightmodule.encoder.layers.1.ffn.layers.1.weightmodule.encoder.layers.1.self_attn.attention_weights.weight


module.backbone.stages.3.blocks.0.norm1.biasmodule.encoder.layers.1.ffn.layers.1.biasmodule.encoder.layers.1.self_attn.attention_weights.bias


module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_tablemodule.encoder.layers.1.norms.0.weightmodule.encoder.layers.1.self_attn.value_proj.weight


module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weightmodule.encoder.layers.1.norms.0.biasmodule.encoder.layers.1.self_attn.value_proj.bias


module.backbone.stages.3.blocks.0.attn.w_msa.qkv.biasmodule.encoder.layers.1.norms.1.weightmodule.encoder.layers.1.self_attn.output_proj.weight


module.backbone.stages.3.blocks.0.attn.w_msa.proj.weightmodule.encoder.layers.1.norms.1.biasmodule.encoder.layers.1.self_attn.output_proj.bias


module.backbone.stages.3.blocks.0.attn.w_msa.proj.biasmodule.encoder.layers.2.self_attn.sampling_offsets.weight
module.encoder.layers.1.ffn.layers.0.0.weight

module.backbone.stages.3.blocks.0.norm2.weightmodule.encoder.layers.2.self_attn.sampling_offsets.biasmodule.encoder.layers.1.ffn.layers.0.0.bias


module.backbone.stages.3.blocks.0.norm2.biasmodule.encoder.layers.2.self_attn.attention_weights.weight
module.encoder.layers.1.ffn.layers.1.weight

module.backbone.stages.3.blocks.0.ffn.layers.0.0.weightmodule.encoder.layers.2.self_attn.attention_weights.biasmodule.encoder.layers.1.ffn.layers.1.bias


module.backbone.stages.3.blocks.0.ffn.layers.0.0.biasmodule.encoder.layers.2.self_attn.value_proj.weight
module.encoder.layers.1.norms.0.weightmodule.backbone.stages.3.blocks.0.ffn.layers.1.weight


module.encoder.layers.2.self_attn.value_proj.biasmodule.encoder.layers.1.norms.0.biasmodule.backbone.stages.3.blocks.0.ffn.layers.1.bias


module.encoder.layers.2.self_attn.output_proj.weightmodule.encoder.layers.1.norms.1.weight
module.backbone.stages.3.blocks.1.norm1.weight
module.encoder.layers.2.self_attn.output_proj.bias
module.encoder.layers.1.norms.1.bias
module.backbone.stages.3.blocks.1.norm1.bias
module.encoder.layers.2.ffn.layers.0.0.weight

module.encoder.layers.2.self_attn.sampling_offsets.weightmodule.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_tablemodule.encoder.layers.2.ffn.layers.0.0.bias


module.encoder.layers.2.self_attn.sampling_offsets.biasmodule.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight
module.encoder.layers.2.ffn.layers.1.weight
module.encoder.layers.2.self_attn.attention_weights.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias
module.encoder.layers.2.ffn.layers.1.bias
module.encoder.layers.2.self_attn.attention_weights.bias
module.backbone.stages.3.blocks.1.attn.w_msa.proj.weight
module.encoder.layers.2.norms.0.weight
module.encoder.layers.2.self_attn.value_proj.weight
module.backbone.stages.3.blocks.1.attn.w_msa.proj.bias
module.encoder.layers.2.norms.0.bias
module.encoder.layers.2.self_attn.value_proj.bias
module.backbone.stages.3.blocks.1.norm2.weight
module.encoder.layers.2.norms.1.weight
module.encoder.layers.2.self_attn.output_proj.weight
module.backbone.stages.3.blocks.1.norm2.bias
module.encoder.layers.2.norms.1.bias
module.encoder.layers.2.self_attn.output_proj.bias
module.backbone.stages.3.blocks.1.ffn.layers.0.0.weight
module.encoder.layers.3.self_attn.sampling_offsets.weight

module.encoder.layers.2.ffn.layers.0.0.weightmodule.backbone.stages.3.blocks.1.ffn.layers.0.0.biasmodule.encoder.layers.3.self_attn.sampling_offsets.bias


module.encoder.layers.2.ffn.layers.0.0.biasmodule.encoder.layers.3.self_attn.attention_weights.weight

module.encoder.layers.3.self_attn.attention_weights.biasmodule.backbone.stages.3.blocks.1.ffn.layers.1.weightmodule.encoder.layers.2.ffn.layers.1.weight


module.encoder.layers.3.self_attn.value_proj.weightmodule.backbone.stages.3.blocks.1.ffn.layers.1.biasmodule.encoder.layers.2.ffn.layers.1.bias


module.encoder.layers.3.self_attn.value_proj.biasmodule.backbone.norm1.weight
module.encoder.layers.2.norms.0.weight
module.encoder.layers.3.self_attn.output_proj.weight
module.backbone.norm1.bias
module.encoder.layers.2.norms.0.bias
module.encoder.layers.3.self_attn.output_proj.bias
module.backbone.norm2.weight
module.encoder.layers.2.norms.1.weight

module.encoder.layers.3.ffn.layers.0.0.weightmodule.backbone.norm2.biasmodule.encoder.layers.2.norms.1.bias


module.encoder.layers.3.ffn.layers.0.0.biasmodule.backbone.norm3.weight
module.encoder.layers.3.self_attn.sampling_offsets.weight

module.encoder.layers.3.ffn.layers.1.weightmodule.backbone.norm3.biasmodule.encoder.layers.3.self_attn.sampling_offsets.bias


module.encoder.layers.3.ffn.layers.1.biasmodule.neck.convs.0.conv.weight
module.encoder.layers.3.self_attn.attention_weights.weight

module.encoder.layers.3.norms.0.weightmodule.neck.convs.0.conv.biasmodule.encoder.layers.3.self_attn.attention_weights.bias


module.encoder.layers.3.norms.0.biasmodule.neck.convs.0.gn.weightmodule.encoder.layers.3.self_attn.value_proj.weight


module.encoder.layers.3.norms.1.weightmodule.neck.convs.0.gn.biasmodule.encoder.layers.3.self_attn.value_proj.bias


module.encoder.layers.3.norms.1.biasmodule.neck.convs.1.conv.weightmodule.encoder.layers.3.self_attn.output_proj.weight


module.encoder.layers.4.self_attn.sampling_offsets.weightmodule.neck.convs.1.conv.biasmodule.encoder.layers.3.self_attn.output_proj.bias


module.encoder.layers.4.self_attn.sampling_offsets.biasmodule.neck.convs.1.gn.weight
module.encoder.layers.3.ffn.layers.0.0.weight
module.encoder.layers.4.self_attn.attention_weights.weight
module.neck.convs.1.gn.bias
module.encoder.layers.3.ffn.layers.0.0.bias
module.encoder.layers.4.self_attn.attention_weights.bias
module.neck.convs.2.conv.weight
module.encoder.layers.3.ffn.layers.1.weight
module.encoder.layers.4.self_attn.value_proj.weight
module.neck.convs.2.conv.bias
module.encoder.layers.3.ffn.layers.1.bias
module.encoder.layers.4.self_attn.value_proj.bias
module.neck.convs.2.gn.weight

module.encoder.layers.3.norms.0.weightmodule.encoder.layers.4.self_attn.output_proj.weightmodule.neck.convs.2.gn.bias


module.encoder.layers.3.norms.0.biasmodule.encoder.layers.4.self_attn.output_proj.biasmodule.neck.extra_convs.0.conv.weight


module.encoder.layers.3.norms.1.weightmodule.encoder.layers.4.ffn.layers.0.0.weightmodule.neck.extra_convs.0.conv.bias


module.encoder.layers.3.norms.1.biasmodule.encoder.layers.4.ffn.layers.0.0.biasmodule.neck.extra_convs.0.gn.weight


module.encoder.layers.4.self_attn.sampling_offsets.weightmodule.encoder.layers.4.ffn.layers.1.weightmodule.neck.extra_convs.0.gn.bias


module.encoder.layers.4.ffn.layers.1.bias
module.encoder.layers.4.self_attn.sampling_offsets.bias
module.bbox_head.cls_branches.0.biasmodule.encoder.layers.4.norms.0.weightmodule.encoder.layers.4.self_attn.attention_weights.weight


module.bbox_head.cls_branches.1.biasmodule.encoder.layers.4.norms.0.biasmodule.encoder.layers.4.self_attn.attention_weights.bias


module.bbox_head.cls_branches.2.biasmodule.encoder.layers.4.norms.1.weightmodule.encoder.layers.4.self_attn.value_proj.weight


module.bbox_head.cls_branches.3.biasmodule.encoder.layers.4.norms.1.biasmodule.encoder.layers.4.self_attn.value_proj.bias


module.bbox_head.cls_branches.4.biasmodule.encoder.layers.5.self_attn.sampling_offsets.weightmodule.encoder.layers.4.self_attn.output_proj.weight


module.bbox_head.cls_branches.5.biasmodule.encoder.layers.5.self_attn.sampling_offsets.biasmodule.encoder.layers.4.self_attn.output_proj.bias


module.bbox_head.cls_branches.6.biasmodule.encoder.layers.5.self_attn.attention_weights.weight
module.encoder.layers.4.ffn.layers.0.0.weight
module.bbox_head.reg_branches.0.0.weight
module.encoder.layers.5.self_attn.attention_weights.bias
module.encoder.layers.4.ffn.layers.0.0.bias
module.bbox_head.reg_branches.0.0.bias
module.encoder.layers.5.self_attn.value_proj.weight

module.encoder.layers.4.ffn.layers.1.weightmodule.bbox_head.reg_branches.0.2.weightmodule.encoder.layers.5.self_attn.value_proj.bias


module.encoder.layers.4.ffn.layers.1.biasmodule.bbox_head.reg_branches.0.2.biasmodule.encoder.layers.5.self_attn.output_proj.weight


module.encoder.layers.4.norms.0.weightmodule.bbox_head.reg_branches.0.4.weightmodule.encoder.layers.5.self_attn.output_proj.bias


module.encoder.layers.4.norms.0.biasmodule.bbox_head.reg_branches.0.4.bias
module.encoder.layers.5.ffn.layers.0.0.weight

module.encoder.layers.4.norms.1.weightmodule.bbox_head.reg_branches.1.0.weightmodule.encoder.layers.5.ffn.layers.0.0.bias


module.encoder.layers.4.norms.1.biasmodule.bbox_head.reg_branches.1.0.bias
module.encoder.layers.5.ffn.layers.1.weight

module.encoder.layers.5.self_attn.sampling_offsets.weightmodule.bbox_head.reg_branches.1.2.weightmodule.encoder.layers.5.ffn.layers.1.bias


module.encoder.layers.5.self_attn.sampling_offsets.biasmodule.bbox_head.reg_branches.1.2.bias
module.encoder.layers.5.norms.0.weightmodule.encoder.layers.5.self_attn.attention_weights.weight


module.bbox_head.reg_branches.1.4.weightmodule.encoder.layers.5.norms.0.biasmodule.encoder.layers.5.self_attn.attention_weights.bias


module.bbox_head.reg_branches.1.4.biasmodule.encoder.layers.5.norms.1.weightmodule.encoder.layers.5.self_attn.value_proj.weight


module.bbox_head.reg_branches.2.0.weightmodule.encoder.layers.5.norms.1.biasmodule.encoder.layers.5.self_attn.value_proj.bias


module.bbox_head.reg_branches.2.0.biasmodule.encoder.text_layers.0.self_attn.attn.in_proj_weightmodule.encoder.layers.5.self_attn.output_proj.weight


module.bbox_head.reg_branches.2.2.weightmodule.encoder.text_layers.0.self_attn.attn.in_proj_biasmodule.encoder.layers.5.self_attn.output_proj.bias


module.bbox_head.reg_branches.2.2.biasmodule.encoder.text_layers.0.self_attn.attn.out_proj.weight
module.encoder.layers.5.ffn.layers.0.0.weight
module.bbox_head.reg_branches.2.4.weight
module.encoder.text_layers.0.self_attn.attn.out_proj.bias
module.encoder.layers.5.ffn.layers.0.0.bias
module.bbox_head.reg_branches.2.4.bias

module.encoder.text_layers.0.ffn.layers.0.0.weightmodule.encoder.layers.5.ffn.layers.1.weightmodule.bbox_head.reg_branches.3.0.weight


module.encoder.text_layers.0.ffn.layers.0.0.biasmodule.encoder.layers.5.ffn.layers.1.biasmodule.bbox_head.reg_branches.3.0.bias


module.encoder.text_layers.0.ffn.layers.1.weightmodule.encoder.layers.5.norms.0.weightmodule.bbox_head.reg_branches.3.2.weight


module.encoder.text_layers.0.ffn.layers.1.biasmodule.encoder.layers.5.norms.0.biasmodule.bbox_head.reg_branches.3.2.bias


module.encoder.layers.5.norms.1.weightmodule.encoder.text_layers.0.norms.0.weightmodule.bbox_head.reg_branches.3.4.weight


module.encoder.layers.5.norms.1.biasmodule.encoder.text_layers.0.norms.0.biasmodule.bbox_head.reg_branches.3.4.bias


module.encoder.text_layers.0.self_attn.attn.in_proj_weightmodule.encoder.text_layers.0.norms.1.weightmodule.bbox_head.reg_branches.4.0.weight


module.encoder.text_layers.0.self_attn.attn.in_proj_biasmodule.encoder.text_layers.0.norms.1.biasmodule.bbox_head.reg_branches.4.0.bias
module.encoder.text_layers.0.self_attn.attn.out_proj.weight

module.encoder.text_layers.0.self_attn.attn.out_proj.bias
module.encoder.text_layers.1.self_attn.attn.in_proj_weight
module.bbox_head.reg_branches.4.2.weight

module.encoder.text_layers.0.ffn.layers.0.0.weightmodule.encoder.text_layers.1.self_attn.attn.in_proj_biasmodule.bbox_head.reg_branches.4.2.bias


module.encoder.text_layers.0.ffn.layers.0.0.biasmodule.encoder.text_layers.1.self_attn.attn.out_proj.weightmodule.bbox_head.reg_branches.4.4.weight


module.encoder.text_layers.0.ffn.layers.1.weightmodule.encoder.text_layers.1.self_attn.attn.out_proj.biasmodule.bbox_head.reg_branches.4.4.bias


module.encoder.text_layers.0.ffn.layers.1.biasmodule.encoder.text_layers.1.ffn.layers.0.0.weightmodule.bbox_head.reg_branches.5.0.weight


module.encoder.text_layers.1.ffn.layers.0.0.biasmodule.encoder.text_layers.0.norms.0.weightmodule.bbox_head.reg_branches.5.0.bias


module.encoder.text_layers.0.norms.0.biasmodule.encoder.text_layers.1.ffn.layers.1.weightmodule.bbox_head.reg_branches.5.2.weight


module.encoder.text_layers.0.norms.1.weightmodule.encoder.text_layers.1.ffn.layers.1.biasmodule.bbox_head.reg_branches.5.2.bias


module.encoder.text_layers.0.norms.1.biasmodule.encoder.text_layers.1.norms.0.weightmodule.bbox_head.reg_branches.5.4.weight


module.encoder.text_layers.1.self_attn.attn.in_proj_weightmodule.encoder.text_layers.1.norms.0.biasmodule.bbox_head.reg_branches.5.4.bias


module.encoder.text_layers.1.self_attn.attn.in_proj_biasmodule.encoder.text_layers.1.norms.1.weightmodule.bbox_head.reg_branches.6.0.weight


module.encoder.text_layers.1.self_attn.attn.out_proj.weightmodule.encoder.text_layers.1.norms.1.biasmodule.bbox_head.reg_branches.6.0.bias


module.encoder.text_layers.1.self_attn.attn.out_proj.biasmodule.encoder.text_layers.2.self_attn.attn.in_proj_weightmodule.bbox_head.reg_branches.6.2.weight


module.encoder.text_layers.2.self_attn.attn.in_proj_biasmodule.encoder.text_layers.1.ffn.layers.0.0.weightmodule.bbox_head.reg_branches.6.2.bias


module.encoder.text_layers.2.self_attn.attn.out_proj.weightmodule.encoder.text_layers.1.ffn.layers.0.0.biasmodule.bbox_head.reg_branches.6.4.weight


module.encoder.text_layers.2.self_attn.attn.out_proj.biasmodule.encoder.text_layers.1.ffn.layers.1.weightmodule.bbox_head.reg_branches.6.4.bias


module.encoder.text_layers.1.ffn.layers.1.biasmodule.encoder.text_layers.2.ffn.layers.0.0.weight

module.encoder.layers.0.self_attn.sampling_offsets.weightmodule.encoder.text_layers.2.ffn.layers.0.0.biasmodule.encoder.text_layers.1.norms.0.weight


module.encoder.layers.0.self_attn.sampling_offsets.biasmodule.encoder.text_layers.1.norms.0.biasmodule.encoder.text_layers.2.ffn.layers.1.weight


module.encoder.layers.0.self_attn.attention_weights.weightmodule.encoder.text_layers.1.norms.1.weightmodule.encoder.text_layers.2.ffn.layers.1.bias


module.encoder.layers.0.self_attn.attention_weights.biasmodule.encoder.text_layers.1.norms.1.bias
module.encoder.text_layers.2.norms.0.weight
module.encoder.layers.0.self_attn.value_proj.weight
module.encoder.text_layers.2.self_attn.attn.in_proj_weight
module.encoder.text_layers.2.norms.0.bias
module.encoder.layers.0.self_attn.value_proj.bias
module.encoder.text_layers.2.self_attn.attn.in_proj_bias
module.encoder.text_layers.2.norms.1.weight
module.encoder.layers.0.self_attn.output_proj.weight
module.encoder.text_layers.2.self_attn.attn.out_proj.weight
module.encoder.text_layers.2.norms.1.bias
module.encoder.layers.0.self_attn.output_proj.bias
module.encoder.text_layers.2.self_attn.attn.out_proj.bias
module.encoder.text_layers.3.self_attn.attn.in_proj_weight

module.encoder.layers.0.ffn.layers.0.0.weightmodule.encoder.text_layers.3.self_attn.attn.in_proj_biasmodule.encoder.text_layers.2.ffn.layers.0.0.weight


module.encoder.layers.0.ffn.layers.0.0.biasmodule.encoder.text_layers.3.self_attn.attn.out_proj.weightmodule.encoder.text_layers.2.ffn.layers.0.0.bias


module.encoder.layers.0.ffn.layers.1.weightmodule.encoder.text_layers.3.self_attn.attn.out_proj.biasmodule.encoder.text_layers.2.ffn.layers.1.weight


module.encoder.layers.0.ffn.layers.1.biasmodule.encoder.text_layers.2.ffn.layers.1.biasmodule.encoder.text_layers.3.ffn.layers.0.0.weight


module.encoder.layers.0.norms.0.weightmodule.encoder.text_layers.3.ffn.layers.0.0.biasmodule.encoder.text_layers.2.norms.0.weight


module.encoder.layers.0.norms.0.biasmodule.encoder.text_layers.2.norms.0.biasmodule.encoder.text_layers.3.ffn.layers.1.weight


module.encoder.layers.0.norms.1.weightmodule.encoder.text_layers.2.norms.1.weightmodule.encoder.text_layers.3.ffn.layers.1.bias


module.encoder.layers.0.norms.1.biasmodule.encoder.text_layers.2.norms.1.bias
module.encoder.text_layers.3.norms.0.weight

module.encoder.layers.1.self_attn.sampling_offsets.weightmodule.encoder.text_layers.3.norms.0.biasmodule.encoder.text_layers.3.self_attn.attn.in_proj_weight


module.encoder.layers.1.self_attn.sampling_offsets.biasmodule.encoder.text_layers.3.norms.1.weightmodule.encoder.text_layers.3.self_attn.attn.in_proj_bias


module.encoder.layers.1.self_attn.attention_weights.weightmodule.encoder.text_layers.3.norms.1.biasmodule.encoder.text_layers.3.self_attn.attn.out_proj.weight


module.encoder.layers.1.self_attn.attention_weights.biasmodule.encoder.text_layers.4.self_attn.attn.in_proj_weightmodule.encoder.text_layers.3.self_attn.attn.out_proj.bias


module.encoder.layers.1.self_attn.value_proj.weightmodule.encoder.text_layers.4.self_attn.attn.in_proj_bias
module.encoder.text_layers.3.ffn.layers.0.0.weight
module.encoder.layers.1.self_attn.value_proj.bias
module.encoder.text_layers.4.self_attn.attn.out_proj.weight
module.encoder.text_layers.3.ffn.layers.0.0.bias
module.encoder.layers.1.self_attn.output_proj.weight
module.encoder.text_layers.4.self_attn.attn.out_proj.bias

module.encoder.text_layers.3.ffn.layers.1.weightmodule.encoder.layers.1.self_attn.output_proj.bias
module.encoder.text_layers.4.ffn.layers.0.0.weight
module.encoder.text_layers.3.ffn.layers.1.bias

module.encoder.layers.1.ffn.layers.0.0.weightmodule.encoder.text_layers.4.ffn.layers.0.0.biasmodule.encoder.text_layers.3.norms.0.weight


module.encoder.layers.1.ffn.layers.0.0.biasmodule.encoder.text_layers.4.ffn.layers.1.weightmodule.encoder.text_layers.3.norms.0.bias

module.encoder.layers.1.ffn.layers.1.weight
module.encoder.text_layers.4.ffn.layers.1.bias
module.encoder.text_layers.3.norms.1.weight
module.encoder.text_layers.4.norms.0.weightmodule.encoder.layers.1.ffn.layers.1.bias


module.encoder.text_layers.3.norms.1.biasmodule.encoder.text_layers.4.norms.0.bias
module.encoder.layers.1.norms.0.weight
module.encoder.text_layers.4.self_attn.attn.in_proj_weight
module.encoder.text_layers.4.norms.1.weight
module.encoder.layers.1.norms.0.bias
module.encoder.text_layers.4.self_attn.attn.in_proj_bias
module.encoder.text_layers.4.norms.1.bias
module.encoder.layers.1.norms.1.weight
module.encoder.text_layers.4.self_attn.attn.out_proj.weight

module.encoder.text_layers.5.self_attn.attn.in_proj_weightmodule.encoder.layers.1.norms.1.biasmodule.encoder.text_layers.4.self_attn.attn.out_proj.bias


module.encoder.text_layers.5.self_attn.attn.in_proj_biasmodule.encoder.layers.2.self_attn.sampling_offsets.weight
module.encoder.text_layers.4.ffn.layers.0.0.weight
module.encoder.text_layers.5.self_attn.attn.out_proj.weight
module.encoder.layers.2.self_attn.sampling_offsets.bias
module.encoder.text_layers.4.ffn.layers.0.0.bias
module.encoder.text_layers.5.self_attn.attn.out_proj.bias
module.encoder.layers.2.self_attn.attention_weights.weight
module.encoder.text_layers.4.ffn.layers.1.weight

module.encoder.text_layers.5.ffn.layers.0.0.weightmodule.encoder.layers.2.self_attn.attention_weights.biasmodule.encoder.text_layers.4.ffn.layers.1.bias


module.encoder.text_layers.5.ffn.layers.0.0.biasmodule.encoder.layers.2.self_attn.value_proj.weight
module.encoder.text_layers.4.norms.0.weight
module.encoder.text_layers.5.ffn.layers.1.weight
module.encoder.layers.2.self_attn.value_proj.bias
module.encoder.text_layers.4.norms.0.bias
module.encoder.text_layers.5.ffn.layers.1.bias
module.encoder.layers.2.self_attn.output_proj.weight
module.encoder.text_layers.4.norms.1.weight

module.encoder.text_layers.5.norms.0.weightmodule.encoder.layers.2.self_attn.output_proj.biasmodule.encoder.text_layers.4.norms.1.bias


module.encoder.text_layers.5.norms.0.biasmodule.encoder.layers.2.ffn.layers.0.0.weight
module.encoder.text_layers.5.self_attn.attn.in_proj_weight
module.encoder.text_layers.5.norms.1.weight
module.encoder.layers.2.ffn.layers.0.0.bias
module.encoder.text_layers.5.self_attn.attn.in_proj_bias
module.encoder.text_layers.5.norms.1.bias
module.encoder.layers.2.ffn.layers.1.weight
module.encoder.text_layers.5.self_attn.attn.out_proj.weight
module.encoder.fusion_layers.0.gamma_v
module.encoder.layers.2.ffn.layers.1.bias
module.encoder.text_layers.5.self_attn.attn.out_proj.bias
module.encoder.fusion_layers.0.gamma_l

module.encoder.layers.2.norms.0.weightmodule.encoder.fusion_layers.0.layer_norm_v.weightmodule.encoder.text_layers.5.ffn.layers.0.0.weight


module.encoder.layers.2.norms.0.biasmodule.encoder.fusion_layers.0.layer_norm_v.biasmodule.encoder.text_layers.5.ffn.layers.0.0.bias


module.encoder.layers.2.norms.1.weightmodule.encoder.fusion_layers.0.layer_norm_l.weightmodule.encoder.text_layers.5.ffn.layers.1.weight


module.encoder.layers.2.norms.1.biasmodule.encoder.fusion_layers.0.layer_norm_l.biasmodule.encoder.text_layers.5.ffn.layers.1.bias


module.encoder.layers.3.self_attn.sampling_offsets.weightmodule.encoder.fusion_layers.0.attn.v_proj.weight
module.encoder.text_layers.5.norms.0.weight
module.encoder.layers.3.self_attn.sampling_offsets.bias
module.encoder.fusion_layers.0.attn.v_proj.bias
module.encoder.text_layers.5.norms.0.bias
module.encoder.layers.3.self_attn.attention_weights.weight
module.encoder.fusion_layers.0.attn.l_proj.weight
module.encoder.text_layers.5.norms.1.weight
module.encoder.layers.3.self_attn.attention_weights.bias
module.encoder.fusion_layers.0.attn.l_proj.bias
module.encoder.text_layers.5.norms.1.bias
module.encoder.layers.3.self_attn.value_proj.weight
module.encoder.fusion_layers.0.attn.values_v_proj.weightmodule.encoder.fusion_layers.0.gamma_v


module.encoder.layers.3.self_attn.value_proj.biasmodule.encoder.fusion_layers.0.attn.values_v_proj.biasmodule.encoder.fusion_layers.0.gamma_l


module.encoder.layers.3.self_attn.output_proj.weightmodule.encoder.fusion_layers.0.attn.values_l_proj.weightmodule.encoder.fusion_layers.0.layer_norm_v.weight


module.encoder.layers.3.self_attn.output_proj.biasmodule.encoder.fusion_layers.0.attn.values_l_proj.biasmodule.encoder.fusion_layers.0.layer_norm_v.bias


module.encoder.layers.3.ffn.layers.0.0.weightmodule.encoder.fusion_layers.0.attn.out_v_proj.weightmodule.encoder.fusion_layers.0.layer_norm_l.weight


module.encoder.layers.3.ffn.layers.0.0.biasmodule.encoder.fusion_layers.0.attn.out_v_proj.biasmodule.encoder.fusion_layers.0.layer_norm_l.bias


module.encoder.layers.3.ffn.layers.1.weightmodule.encoder.fusion_layers.0.attn.out_l_proj.weightmodule.encoder.fusion_layers.0.attn.v_proj.weight


module.encoder.layers.3.ffn.layers.1.biasmodule.encoder.fusion_layers.0.attn.out_l_proj.biasmodule.encoder.fusion_layers.0.attn.v_proj.bias


module.encoder.layers.3.norms.0.weightmodule.encoder.fusion_layers.1.gamma_vmodule.encoder.fusion_layers.0.attn.l_proj.weight


module.encoder.layers.3.norms.0.biasmodule.encoder.fusion_layers.1.gamma_lmodule.encoder.fusion_layers.0.attn.l_proj.bias


module.encoder.layers.3.norms.1.weightmodule.encoder.fusion_layers.1.layer_norm_v.weightmodule.encoder.fusion_layers.0.attn.values_v_proj.weight


module.encoder.layers.3.norms.1.biasmodule.encoder.fusion_layers.1.layer_norm_v.biasmodule.encoder.fusion_layers.0.attn.values_v_proj.bias

module.encoder.layers.4.self_attn.sampling_offsets.weight
module.encoder.fusion_layers.1.layer_norm_l.weight
module.encoder.fusion_layers.0.attn.values_l_proj.weight

module.encoder.layers.4.self_attn.sampling_offsets.biasmodule.encoder.fusion_layers.1.layer_norm_l.biasmodule.encoder.fusion_layers.0.attn.values_l_proj.bias


module.encoder.layers.4.self_attn.attention_weights.weightmodule.encoder.fusion_layers.1.attn.v_proj.weightmodule.encoder.fusion_layers.0.attn.out_v_proj.weight


module.encoder.layers.4.self_attn.attention_weights.biasmodule.encoder.fusion_layers.1.attn.v_proj.biasmodule.encoder.fusion_layers.0.attn.out_v_proj.bias


module.encoder.layers.4.self_attn.value_proj.weightmodule.encoder.fusion_layers.1.attn.l_proj.weightmodule.encoder.fusion_layers.0.attn.out_l_proj.weight


module.encoder.layers.4.self_attn.value_proj.biasmodule.encoder.fusion_layers.1.attn.l_proj.biasmodule.encoder.fusion_layers.0.attn.out_l_proj.bias


module.encoder.fusion_layers.1.attn.values_v_proj.weightmodule.encoder.layers.4.self_attn.output_proj.weightmodule.encoder.fusion_layers.1.gamma_v


module.encoder.fusion_layers.1.attn.values_v_proj.biasmodule.encoder.layers.4.self_attn.output_proj.biasmodule.encoder.fusion_layers.1.gamma_l


module.encoder.fusion_layers.1.attn.values_l_proj.weightmodule.encoder.layers.4.ffn.layers.0.0.weightmodule.encoder.fusion_layers.1.layer_norm_v.weight


module.encoder.fusion_layers.1.attn.values_l_proj.biasmodule.encoder.layers.4.ffn.layers.0.0.biasmodule.encoder.fusion_layers.1.layer_norm_v.bias


module.encoder.fusion_layers.1.attn.out_v_proj.weightmodule.encoder.layers.4.ffn.layers.1.weightmodule.encoder.fusion_layers.1.layer_norm_l.weight


module.encoder.fusion_layers.1.attn.out_v_proj.biasmodule.encoder.layers.4.ffn.layers.1.biasmodule.encoder.fusion_layers.1.layer_norm_l.bias


module.encoder.fusion_layers.1.attn.out_l_proj.weightmodule.encoder.layers.4.norms.0.weight
module.encoder.fusion_layers.1.attn.v_proj.weight
module.encoder.fusion_layers.1.attn.out_l_proj.bias
module.encoder.layers.4.norms.0.bias
module.encoder.fusion_layers.1.attn.v_proj.bias
module.encoder.fusion_layers.2.gamma_v
module.encoder.layers.4.norms.1.weight
module.encoder.fusion_layers.1.attn.l_proj.weight
module.encoder.fusion_layers.2.gamma_l
module.encoder.layers.4.norms.1.bias
module.encoder.fusion_layers.1.attn.l_proj.bias
module.encoder.fusion_layers.2.layer_norm_v.weight

module.encoder.layers.5.self_attn.sampling_offsets.weightmodule.encoder.fusion_layers.1.attn.values_v_proj.weightmodule.encoder.fusion_layers.2.layer_norm_v.bias


module.encoder.layers.5.self_attn.sampling_offsets.biasmodule.encoder.fusion_layers.1.attn.values_v_proj.biasmodule.encoder.fusion_layers.2.layer_norm_l.weight


module.encoder.layers.5.self_attn.attention_weights.weightmodule.encoder.fusion_layers.1.attn.values_l_proj.weightmodule.encoder.fusion_layers.2.layer_norm_l.bias


module.encoder.layers.5.self_attn.attention_weights.biasmodule.encoder.fusion_layers.1.attn.values_l_proj.biasmodule.encoder.fusion_layers.2.attn.v_proj.weight


module.encoder.layers.5.self_attn.value_proj.weightmodule.encoder.fusion_layers.1.attn.out_v_proj.weightmodule.encoder.fusion_layers.2.attn.v_proj.bias


module.encoder.layers.5.self_attn.value_proj.biasmodule.encoder.fusion_layers.1.attn.out_v_proj.biasmodule.encoder.fusion_layers.2.attn.l_proj.weight


module.encoder.layers.5.self_attn.output_proj.weightmodule.encoder.fusion_layers.1.attn.out_l_proj.weightmodule.encoder.fusion_layers.2.attn.l_proj.bias


module.encoder.layers.5.self_attn.output_proj.biasmodule.encoder.fusion_layers.1.attn.out_l_proj.biasmodule.encoder.fusion_layers.2.attn.values_v_proj.weight


module.encoder.layers.5.ffn.layers.0.0.weightmodule.encoder.fusion_layers.2.gamma_vmodule.encoder.fusion_layers.2.attn.values_v_proj.bias


module.encoder.layers.5.ffn.layers.0.0.biasmodule.encoder.fusion_layers.2.gamma_lmodule.encoder.fusion_layers.2.attn.values_l_proj.weight


module.encoder.layers.5.ffn.layers.1.weightmodule.encoder.fusion_layers.2.layer_norm_v.weightmodule.encoder.fusion_layers.2.attn.values_l_proj.bias


module.encoder.layers.5.ffn.layers.1.biasmodule.encoder.fusion_layers.2.layer_norm_v.biasmodule.encoder.fusion_layers.2.attn.out_v_proj.weight


module.encoder.fusion_layers.2.layer_norm_l.weightmodule.encoder.layers.5.norms.0.weightmodule.encoder.fusion_layers.2.attn.out_v_proj.bias


module.encoder.fusion_layers.2.layer_norm_l.biasmodule.encoder.layers.5.norms.0.biasmodule.encoder.fusion_layers.2.attn.out_l_proj.weight


module.encoder.fusion_layers.2.attn.v_proj.weightmodule.encoder.layers.5.norms.1.weightmodule.encoder.fusion_layers.2.attn.out_l_proj.bias


module.encoder.fusion_layers.2.attn.v_proj.biasmodule.encoder.layers.5.norms.1.biasmodule.encoder.fusion_layers.3.gamma_v


module.encoder.fusion_layers.2.attn.l_proj.weightmodule.encoder.fusion_layers.3.gamma_l
module.encoder.text_layers.0.self_attn.attn.in_proj_weight
module.encoder.fusion_layers.2.attn.l_proj.bias
module.encoder.fusion_layers.3.layer_norm_v.weight
module.encoder.text_layers.0.self_attn.attn.in_proj_bias
module.encoder.fusion_layers.2.attn.values_v_proj.weight

module.encoder.fusion_layers.3.layer_norm_v.biasmodule.encoder.fusion_layers.2.attn.values_v_proj.biasmodule.encoder.text_layers.0.self_attn.attn.out_proj.weight


module.encoder.fusion_layers.3.layer_norm_l.weightmodule.encoder.fusion_layers.2.attn.values_l_proj.weightmodule.encoder.text_layers.0.self_attn.attn.out_proj.bias


module.encoder.fusion_layers.3.layer_norm_l.biasmodule.encoder.fusion_layers.2.attn.values_l_proj.bias

module.encoder.text_layers.0.ffn.layers.0.0.weightmodule.encoder.fusion_layers.3.attn.v_proj.weightmodule.encoder.fusion_layers.2.attn.out_v_proj.weight


module.encoder.text_layers.0.ffn.layers.0.0.biasmodule.encoder.fusion_layers.3.attn.v_proj.biasmodule.encoder.fusion_layers.2.attn.out_v_proj.bias


module.encoder.text_layers.0.ffn.layers.1.weightmodule.encoder.fusion_layers.3.attn.l_proj.weightmodule.encoder.fusion_layers.2.attn.out_l_proj.weight


module.encoder.text_layers.0.ffn.layers.1.biasmodule.encoder.fusion_layers.3.attn.l_proj.biasmodule.encoder.fusion_layers.2.attn.out_l_proj.bias


module.encoder.text_layers.0.norms.0.weightmodule.encoder.fusion_layers.3.attn.values_v_proj.weightmodule.encoder.fusion_layers.3.gamma_v


module.encoder.text_layers.0.norms.0.biasmodule.encoder.fusion_layers.3.attn.values_v_proj.biasmodule.encoder.fusion_layers.3.gamma_l


module.encoder.text_layers.0.norms.1.weightmodule.encoder.fusion_layers.3.attn.values_l_proj.weightmodule.encoder.fusion_layers.3.layer_norm_v.weight


module.encoder.text_layers.0.norms.1.biasmodule.encoder.fusion_layers.3.attn.values_l_proj.biasmodule.encoder.fusion_layers.3.layer_norm_v.bias


module.encoder.text_layers.1.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.3.attn.out_v_proj.weightmodule.encoder.fusion_layers.3.layer_norm_l.weight


module.encoder.text_layers.1.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.3.attn.out_v_proj.biasmodule.encoder.fusion_layers.3.layer_norm_l.bias


module.encoder.text_layers.1.self_attn.attn.out_proj.weightmodule.encoder.fusion_layers.3.attn.out_l_proj.weightmodule.encoder.fusion_layers.3.attn.v_proj.weight


module.encoder.text_layers.1.self_attn.attn.out_proj.biasmodule.encoder.fusion_layers.3.attn.out_l_proj.biasmodule.encoder.fusion_layers.3.attn.v_proj.bias


module.encoder.text_layers.1.ffn.layers.0.0.weightmodule.encoder.fusion_layers.4.gamma_vmodule.encoder.fusion_layers.3.attn.l_proj.weight


module.encoder.text_layers.1.ffn.layers.0.0.biasmodule.encoder.fusion_layers.4.gamma_lmodule.encoder.fusion_layers.3.attn.l_proj.bias


module.encoder.text_layers.1.ffn.layers.1.weightmodule.encoder.fusion_layers.4.layer_norm_v.weightmodule.encoder.fusion_layers.3.attn.values_v_proj.weight


module.encoder.text_layers.1.ffn.layers.1.biasmodule.encoder.fusion_layers.4.layer_norm_v.biasmodule.encoder.fusion_layers.3.attn.values_v_proj.bias


module.encoder.text_layers.1.norms.0.weightmodule.encoder.fusion_layers.4.layer_norm_l.weightmodule.encoder.fusion_layers.3.attn.values_l_proj.weight


module.encoder.text_layers.1.norms.0.biasmodule.encoder.fusion_layers.4.layer_norm_l.biasmodule.encoder.fusion_layers.3.attn.values_l_proj.bias


module.encoder.text_layers.1.norms.1.weightmodule.encoder.fusion_layers.4.attn.v_proj.weightmodule.encoder.fusion_layers.3.attn.out_v_proj.weight


module.encoder.text_layers.1.norms.1.biasmodule.encoder.fusion_layers.4.attn.v_proj.biasmodule.encoder.fusion_layers.3.attn.out_v_proj.bias
module.encoder.text_layers.2.self_attn.attn.in_proj_weight


module.encoder.fusion_layers.4.attn.l_proj.weightmodule.encoder.fusion_layers.3.attn.out_l_proj.weightmodule.encoder.text_layers.2.self_attn.attn.in_proj_bias


module.encoder.fusion_layers.4.attn.l_proj.biasmodule.encoder.fusion_layers.3.attn.out_l_proj.biasmodule.encoder.text_layers.2.self_attn.attn.out_proj.weight


module.encoder.fusion_layers.4.attn.values_v_proj.weightmodule.encoder.fusion_layers.4.gamma_vmodule.encoder.text_layers.2.self_attn.attn.out_proj.bias


module.encoder.fusion_layers.4.attn.values_v_proj.biasmodule.encoder.fusion_layers.4.gamma_l
module.encoder.text_layers.2.ffn.layers.0.0.weight
module.encoder.fusion_layers.4.attn.values_l_proj.weight
module.encoder.fusion_layers.4.layer_norm_v.weight
module.encoder.text_layers.2.ffn.layers.0.0.bias
module.encoder.fusion_layers.4.attn.values_l_proj.bias
module.encoder.fusion_layers.4.layer_norm_v.bias
module.encoder.text_layers.2.ffn.layers.1.weight
module.encoder.fusion_layers.4.attn.out_v_proj.weight
module.encoder.fusion_layers.4.layer_norm_l.weight
module.encoder.text_layers.2.ffn.layers.1.bias
module.encoder.fusion_layers.4.attn.out_v_proj.bias
module.encoder.fusion_layers.4.layer_norm_l.bias

module.encoder.text_layers.2.norms.0.weightmodule.encoder.fusion_layers.4.attn.out_l_proj.weightmodule.encoder.fusion_layers.4.attn.v_proj.weight


module.encoder.text_layers.2.norms.0.biasmodule.encoder.fusion_layers.4.attn.out_l_proj.biasmodule.encoder.fusion_layers.4.attn.v_proj.bias


module.encoder.text_layers.2.norms.1.weightmodule.encoder.fusion_layers.5.gamma_vmodule.encoder.fusion_layers.4.attn.l_proj.weight


module.encoder.text_layers.2.norms.1.biasmodule.encoder.fusion_layers.5.gamma_lmodule.encoder.fusion_layers.4.attn.l_proj.bias


module.encoder.text_layers.3.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.5.layer_norm_v.weightmodule.encoder.fusion_layers.4.attn.values_v_proj.weight


module.encoder.text_layers.3.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.5.layer_norm_v.biasmodule.encoder.fusion_layers.4.attn.values_v_proj.bias


module.encoder.text_layers.3.self_attn.attn.out_proj.weightmodule.encoder.fusion_layers.5.layer_norm_l.weightmodule.encoder.fusion_layers.4.attn.values_l_proj.weight


module.encoder.text_layers.3.self_attn.attn.out_proj.biasmodule.encoder.fusion_layers.5.layer_norm_l.biasmodule.encoder.fusion_layers.4.attn.values_l_proj.bias


module.encoder.text_layers.3.ffn.layers.0.0.weightmodule.encoder.fusion_layers.5.attn.v_proj.weightmodule.encoder.fusion_layers.4.attn.out_v_proj.weight


module.encoder.text_layers.3.ffn.layers.0.0.biasmodule.encoder.fusion_layers.5.attn.v_proj.biasmodule.encoder.fusion_layers.4.attn.out_v_proj.bias

module.encoder.text_layers.3.ffn.layers.1.weight
module.encoder.fusion_layers.5.attn.l_proj.weight
module.encoder.fusion_layers.4.attn.out_l_proj.weight
module.encoder.text_layers.3.ffn.layers.1.bias
module.encoder.fusion_layers.5.attn.l_proj.bias
module.encoder.fusion_layers.4.attn.out_l_proj.bias

module.encoder.text_layers.3.norms.0.weightmodule.encoder.fusion_layers.5.attn.values_v_proj.weightmodule.encoder.fusion_layers.5.gamma_v


module.encoder.text_layers.3.norms.0.biasmodule.encoder.fusion_layers.5.attn.values_v_proj.biasmodule.encoder.fusion_layers.5.gamma_l


module.encoder.text_layers.3.norms.1.weightmodule.encoder.fusion_layers.5.attn.values_l_proj.weightmodule.encoder.fusion_layers.5.layer_norm_v.weight


module.encoder.text_layers.3.norms.1.biasmodule.encoder.fusion_layers.5.attn.values_l_proj.biasmodule.encoder.fusion_layers.5.layer_norm_v.bias


module.encoder.text_layers.4.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.5.attn.out_v_proj.weightmodule.encoder.fusion_layers.5.layer_norm_l.weight


module.encoder.text_layers.4.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.5.attn.out_v_proj.biasmodule.encoder.fusion_layers.5.layer_norm_l.bias


module.encoder.text_layers.4.self_attn.attn.out_proj.weightmodule.encoder.fusion_layers.5.attn.out_l_proj.weightmodule.encoder.fusion_layers.5.attn.v_proj.weight


module.encoder.text_layers.4.self_attn.attn.out_proj.biasmodule.encoder.fusion_layers.5.attn.out_l_proj.biasmodule.encoder.fusion_layers.5.attn.v_proj.bias


module.encoder.text_layers.4.ffn.layers.0.0.weightmodule.encoder.fusion_layers.5.attn.l_proj.weightmodule.decoder.layers.0.self_attn.attn.in_proj_weight


module.encoder.text_layers.4.ffn.layers.0.0.biasmodule.encoder.fusion_layers.5.attn.l_proj.biasmodule.decoder.layers.0.self_attn.attn.in_proj_bias


module.encoder.text_layers.4.ffn.layers.1.weightmodule.encoder.fusion_layers.5.attn.values_v_proj.weightmodule.decoder.layers.0.self_attn.attn.out_proj.weight


module.encoder.text_layers.4.ffn.layers.1.biasmodule.encoder.fusion_layers.5.attn.values_v_proj.biasmodule.decoder.layers.0.self_attn.attn.out_proj.bias


module.encoder.text_layers.4.norms.0.weightmodule.encoder.fusion_layers.5.attn.values_l_proj.weight
module.decoder.layers.0.cross_attn_text.attn.in_proj_weight
module.encoder.text_layers.4.norms.0.bias
module.encoder.fusion_layers.5.attn.values_l_proj.bias
module.decoder.layers.0.cross_attn_text.attn.in_proj_bias
module.encoder.text_layers.4.norms.1.weight
module.encoder.fusion_layers.5.attn.out_v_proj.weight
module.decoder.layers.0.cross_attn_text.attn.out_proj.weight
module.encoder.text_layers.4.norms.1.bias
module.encoder.fusion_layers.5.attn.out_v_proj.bias
module.decoder.layers.0.cross_attn_text.attn.out_proj.bias
module.encoder.text_layers.5.self_attn.attn.in_proj_weight
module.encoder.fusion_layers.5.attn.out_l_proj.weight

module.decoder.layers.0.cross_attn.sampling_offsets.weightmodule.encoder.text_layers.5.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.5.attn.out_l_proj.bias


module.decoder.layers.0.cross_attn.sampling_offsets.biasmodule.encoder.text_layers.5.self_attn.attn.out_proj.weight

module.decoder.layers.0.self_attn.attn.in_proj_weightmodule.decoder.layers.0.cross_attn.attention_weights.weightmodule.encoder.text_layers.5.self_attn.attn.out_proj.bias


module.decoder.layers.0.self_attn.attn.in_proj_biasmodule.decoder.layers.0.cross_attn.attention_weights.bias
module.encoder.text_layers.5.ffn.layers.0.0.weight
module.decoder.layers.0.self_attn.attn.out_proj.weight
module.decoder.layers.0.cross_attn.value_proj.weight
module.encoder.text_layers.5.ffn.layers.0.0.bias

module.encoder.text_layers.5.ffn.layers.1.weightmodule.decoder.layers.0.self_attn.attn.out_proj.biasmodule.decoder.layers.0.cross_attn.value_proj.bias


module.encoder.text_layers.5.ffn.layers.1.biasmodule.decoder.layers.0.cross_attn_text.attn.in_proj_weightmodule.decoder.layers.0.cross_attn.output_proj.weight


module.decoder.layers.0.cross_attn_text.attn.in_proj_biasmodule.encoder.text_layers.5.norms.0.weightmodule.decoder.layers.0.cross_attn.output_proj.bias


module.decoder.layers.0.cross_attn_text.attn.out_proj.weightmodule.encoder.text_layers.5.norms.0.bias
module.decoder.layers.0.ffn.layers.0.0.weight
module.decoder.layers.0.cross_attn_text.attn.out_proj.bias
module.encoder.text_layers.5.norms.1.weight
module.decoder.layers.0.ffn.layers.0.0.bias

module.decoder.layers.0.cross_attn.sampling_offsets.weightmodule.encoder.text_layers.5.norms.1.biasmodule.decoder.layers.0.ffn.layers.1.weight


module.decoder.layers.0.cross_attn.sampling_offsets.biasmodule.encoder.fusion_layers.0.gamma_vmodule.decoder.layers.0.ffn.layers.1.bias


module.decoder.layers.0.cross_attn.attention_weights.weightmodule.encoder.fusion_layers.0.gamma_l
module.decoder.layers.0.norms.0.weight
module.decoder.layers.0.cross_attn.attention_weights.bias
module.encoder.fusion_layers.0.layer_norm_v.weight
module.decoder.layers.0.norms.0.bias
module.decoder.layers.0.cross_attn.value_proj.weight
module.encoder.fusion_layers.0.layer_norm_v.bias
module.decoder.layers.0.norms.1.weight
module.decoder.layers.0.cross_attn.value_proj.bias
module.encoder.fusion_layers.0.layer_norm_l.weight
module.decoder.layers.0.norms.1.bias
module.decoder.layers.0.cross_attn.output_proj.weight
module.encoder.fusion_layers.0.layer_norm_l.bias
module.decoder.layers.0.norms.2.weight
module.decoder.layers.0.cross_attn.output_proj.bias
module.encoder.fusion_layers.0.attn.v_proj.weight
module.decoder.layers.0.norms.2.bias

module.decoder.layers.0.ffn.layers.0.0.weightmodule.encoder.fusion_layers.0.attn.v_proj.biasmodule.decoder.layers.0.norms.3.weight


module.decoder.layers.0.ffn.layers.0.0.biasmodule.encoder.fusion_layers.0.attn.l_proj.weightmodule.decoder.layers.0.norms.3.bias


module.decoder.layers.0.ffn.layers.1.weightmodule.encoder.fusion_layers.0.attn.l_proj.bias
module.decoder.layers.1.self_attn.attn.in_proj_weight
module.decoder.layers.0.ffn.layers.1.bias
module.encoder.fusion_layers.0.attn.values_v_proj.weight
module.decoder.layers.1.self_attn.attn.in_proj_bias

module.decoder.layers.0.norms.0.weightmodule.encoder.fusion_layers.0.attn.values_v_proj.biasmodule.decoder.layers.1.self_attn.attn.out_proj.weight


module.decoder.layers.0.norms.0.biasmodule.encoder.fusion_layers.0.attn.values_l_proj.weightmodule.decoder.layers.1.self_attn.attn.out_proj.bias


module.decoder.layers.0.norms.1.weightmodule.encoder.fusion_layers.0.attn.values_l_proj.bias
module.decoder.layers.1.cross_attn_text.attn.in_proj_weight
module.decoder.layers.0.norms.1.bias
module.encoder.fusion_layers.0.attn.out_v_proj.weight
module.decoder.layers.1.cross_attn_text.attn.in_proj_bias
module.decoder.layers.0.norms.2.weight
module.encoder.fusion_layers.0.attn.out_v_proj.bias
module.decoder.layers.1.cross_attn_text.attn.out_proj.weight
module.decoder.layers.0.norms.2.bias
module.encoder.fusion_layers.0.attn.out_l_proj.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.decoder.layers.0.norms.3.weight
module.encoder.fusion_layers.0.attn.out_l_proj.bias
module.decoder.layers.1.cross_attn.sampling_offsets.weight
module.decoder.layers.0.norms.3.bias
module.encoder.fusion_layers.1.gamma_v
module.decoder.layers.1.cross_attn.sampling_offsets.bias

module.decoder.layers.1.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.1.gamma_lmodule.decoder.layers.1.cross_attn.attention_weights.weight


module.decoder.layers.1.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.1.layer_norm_v.weightmodule.decoder.layers.1.cross_attn.attention_weights.bias


module.decoder.layers.1.self_attn.attn.out_proj.weightmodule.encoder.fusion_layers.1.layer_norm_v.biasmodule.decoder.layers.1.cross_attn.value_proj.weight


module.decoder.layers.1.self_attn.attn.out_proj.biasmodule.encoder.fusion_layers.1.layer_norm_l.weightmodule.decoder.layers.1.cross_attn.value_proj.bias


module.decoder.layers.1.cross_attn_text.attn.in_proj_weightmodule.encoder.fusion_layers.1.layer_norm_l.biasmodule.decoder.layers.1.cross_attn.output_proj.weight


module.decoder.layers.1.cross_attn_text.attn.in_proj_biasmodule.encoder.fusion_layers.1.attn.v_proj.weightmodule.decoder.layers.1.cross_attn.output_proj.bias


module.decoder.layers.1.cross_attn_text.attn.out_proj.weightmodule.encoder.fusion_layers.1.attn.v_proj.bias
module.decoder.layers.1.ffn.layers.0.0.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.encoder.fusion_layers.1.attn.l_proj.weight
module.decoder.layers.1.ffn.layers.0.0.bias

module.decoder.layers.1.cross_attn.sampling_offsets.weightmodule.encoder.fusion_layers.1.attn.l_proj.bias
module.decoder.layers.1.ffn.layers.1.weight
module.decoder.layers.1.cross_attn.sampling_offsets.bias
module.encoder.fusion_layers.1.attn.values_v_proj.weight
module.decoder.layers.1.ffn.layers.1.bias
module.decoder.layers.1.cross_attn.attention_weights.weight
module.encoder.fusion_layers.1.attn.values_v_proj.bias

module.decoder.layers.1.norms.0.weightmodule.decoder.layers.1.cross_attn.attention_weights.biasmodule.encoder.fusion_layers.1.attn.values_l_proj.weight


module.decoder.layers.1.norms.0.biasmodule.decoder.layers.1.cross_attn.value_proj.weightmodule.encoder.fusion_layers.1.attn.values_l_proj.bias


module.decoder.layers.1.norms.1.weightmodule.decoder.layers.1.cross_attn.value_proj.biasmodule.encoder.fusion_layers.1.attn.out_v_proj.weight


module.decoder.layers.1.norms.1.biasmodule.decoder.layers.1.cross_attn.output_proj.weightmodule.encoder.fusion_layers.1.attn.out_v_proj.bias


module.decoder.layers.1.norms.2.weightmodule.decoder.layers.1.cross_attn.output_proj.biasmodule.encoder.fusion_layers.1.attn.out_l_proj.weight


module.decoder.layers.1.norms.2.biasmodule.decoder.layers.1.ffn.layers.0.0.weightmodule.encoder.fusion_layers.1.attn.out_l_proj.bias


module.decoder.layers.1.norms.3.weightmodule.decoder.layers.1.ffn.layers.0.0.biasmodule.encoder.fusion_layers.2.gamma_v


module.decoder.layers.1.norms.3.biasmodule.decoder.layers.1.ffn.layers.1.weightmodule.encoder.fusion_layers.2.gamma_l


module.decoder.layers.1.ffn.layers.1.biasmodule.decoder.layers.2.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.2.layer_norm_v.weight


module.decoder.layers.2.self_attn.attn.in_proj_biasmodule.decoder.layers.1.norms.0.weightmodule.encoder.fusion_layers.2.layer_norm_v.bias


module.decoder.layers.2.self_attn.attn.out_proj.weightmodule.decoder.layers.1.norms.0.biasmodule.encoder.fusion_layers.2.layer_norm_l.weight


module.decoder.layers.2.self_attn.attn.out_proj.biasmodule.decoder.layers.1.norms.1.weightmodule.encoder.fusion_layers.2.layer_norm_l.bias


module.decoder.layers.1.norms.1.biasmodule.decoder.layers.2.cross_attn_text.attn.in_proj_weightmodule.encoder.fusion_layers.2.attn.v_proj.weight


module.decoder.layers.1.norms.2.weightmodule.decoder.layers.2.cross_attn_text.attn.in_proj_biasmodule.encoder.fusion_layers.2.attn.v_proj.bias


module.decoder.layers.1.norms.2.biasmodule.decoder.layers.2.cross_attn_text.attn.out_proj.weightmodule.encoder.fusion_layers.2.attn.l_proj.weight


module.decoder.layers.1.norms.3.weightmodule.decoder.layers.2.cross_attn_text.attn.out_proj.biasmodule.encoder.fusion_layers.2.attn.l_proj.bias


module.decoder.layers.1.norms.3.biasmodule.decoder.layers.2.cross_attn.sampling_offsets.weightmodule.encoder.fusion_layers.2.attn.values_v_proj.weight


module.decoder.layers.2.self_attn.attn.in_proj_weightmodule.decoder.layers.2.cross_attn.sampling_offsets.biasmodule.encoder.fusion_layers.2.attn.values_v_proj.bias

module.decoder.layers.2.cross_attn.attention_weights.weight
module.decoder.layers.2.cross_attn.attention_weights.bias
module.decoder.layers.2.self_attn.attn.in_proj_bias
module.encoder.fusion_layers.2.attn.values_l_proj.weight
module.decoder.layers.2.cross_attn.value_proj.weight
module.decoder.layers.2.self_attn.attn.out_proj.weight
module.encoder.fusion_layers.2.attn.values_l_proj.bias
module.decoder.layers.2.cross_attn.value_proj.bias
module.decoder.layers.2.self_attn.attn.out_proj.bias
module.encoder.fusion_layers.2.attn.out_v_proj.weight
module.decoder.layers.2.cross_attn.output_proj.weight

module.decoder.layers.2.cross_attn_text.attn.in_proj_weightmodule.encoder.fusion_layers.2.attn.out_v_proj.biasmodule.decoder.layers.2.cross_attn.output_proj.bias


module.decoder.layers.2.cross_attn_text.attn.in_proj_biasmodule.encoder.fusion_layers.2.attn.out_l_proj.weight
module.decoder.layers.2.ffn.layers.0.0.weight
module.decoder.layers.2.cross_attn_text.attn.out_proj.weight
module.encoder.fusion_layers.2.attn.out_l_proj.bias
module.decoder.layers.2.ffn.layers.0.0.bias
module.decoder.layers.2.cross_attn_text.attn.out_proj.bias
module.encoder.fusion_layers.3.gamma_v
module.decoder.layers.2.ffn.layers.1.weight

module.decoder.layers.2.cross_attn.sampling_offsets.weightmodule.encoder.fusion_layers.3.gamma_lmodule.decoder.layers.2.ffn.layers.1.bias


module.decoder.layers.2.cross_attn.sampling_offsets.biasmodule.encoder.fusion_layers.3.layer_norm_v.weight
module.decoder.layers.2.norms.0.weight
module.decoder.layers.2.cross_attn.attention_weights.weight
module.encoder.fusion_layers.3.layer_norm_v.bias
module.decoder.layers.2.norms.0.bias
module.decoder.layers.2.cross_attn.attention_weights.bias
module.encoder.fusion_layers.3.layer_norm_l.weight
module.decoder.layers.2.norms.1.weight
module.decoder.layers.2.cross_attn.value_proj.weight
module.encoder.fusion_layers.3.layer_norm_l.bias
module.decoder.layers.2.norms.1.bias
module.decoder.layers.2.cross_attn.value_proj.bias
module.encoder.fusion_layers.3.attn.v_proj.weight
module.decoder.layers.2.norms.2.weight
module.decoder.layers.2.cross_attn.output_proj.weight
module.encoder.fusion_layers.3.attn.v_proj.bias
module.decoder.layers.2.norms.2.bias
module.decoder.layers.2.cross_attn.output_proj.bias
module.encoder.fusion_layers.3.attn.l_proj.weight
module.decoder.layers.2.norms.3.weight

module.decoder.layers.2.ffn.layers.0.0.weightmodule.encoder.fusion_layers.3.attn.l_proj.biasmodule.decoder.layers.2.norms.3.bias


module.decoder.layers.2.ffn.layers.0.0.biasmodule.encoder.fusion_layers.3.attn.values_v_proj.weight
module.decoder.layers.3.self_attn.attn.in_proj_weight

module.decoder.layers.2.ffn.layers.1.weightmodule.encoder.fusion_layers.3.attn.values_v_proj.biasmodule.decoder.layers.3.self_attn.attn.in_proj_bias


module.decoder.layers.2.ffn.layers.1.biasmodule.encoder.fusion_layers.3.attn.values_l_proj.weightmodule.decoder.layers.3.self_attn.attn.out_proj.weight


module.decoder.layers.2.norms.0.weightmodule.encoder.fusion_layers.3.attn.values_l_proj.biasmodule.decoder.layers.3.self_attn.attn.out_proj.bias


module.decoder.layers.2.norms.0.biasmodule.encoder.fusion_layers.3.attn.out_v_proj.weight
module.decoder.layers.3.cross_attn_text.attn.in_proj_weight
module.decoder.layers.2.norms.1.weight
module.encoder.fusion_layers.3.attn.out_v_proj.bias
module.decoder.layers.3.cross_attn_text.attn.in_proj_bias
module.decoder.layers.2.norms.1.bias
module.encoder.fusion_layers.3.attn.out_l_proj.weight
module.decoder.layers.3.cross_attn_text.attn.out_proj.weight
module.decoder.layers.2.norms.2.weight
module.encoder.fusion_layers.3.attn.out_l_proj.bias
module.decoder.layers.3.cross_attn_text.attn.out_proj.bias
module.decoder.layers.2.norms.2.bias
module.encoder.fusion_layers.4.gamma_v

module.decoder.layers.3.cross_attn.sampling_offsets.weightmodule.decoder.layers.2.norms.3.weightmodule.encoder.fusion_layers.4.gamma_l


module.decoder.layers.3.cross_attn.sampling_offsets.biasmodule.decoder.layers.2.norms.3.biasmodule.encoder.fusion_layers.4.layer_norm_v.weight


module.decoder.layers.3.cross_attn.attention_weights.weightmodule.decoder.layers.3.self_attn.attn.in_proj_weightmodule.encoder.fusion_layers.4.layer_norm_v.bias


module.decoder.layers.3.cross_attn.attention_weights.biasmodule.decoder.layers.3.self_attn.attn.in_proj_biasmodule.encoder.fusion_layers.4.layer_norm_l.weight


module.decoder.layers.3.cross_attn.value_proj.weightmodule.decoder.layers.3.self_attn.attn.out_proj.weightmodule.encoder.fusion_layers.4.layer_norm_l.bias


module.decoder.layers.3.cross_attn.value_proj.biasmodule.decoder.layers.3.self_attn.attn.out_proj.biasmodule.encoder.fusion_layers.4.attn.v_proj.weight


module.decoder.layers.3.cross_attn.output_proj.weightmodule.decoder.layers.3.cross_attn_text.attn.in_proj_weightmodule.encoder.fusion_layers.4.attn.v_proj.bias


module.decoder.layers.3.cross_attn.output_proj.biasmodule.decoder.layers.3.cross_attn_text.attn.in_proj_biasmodule.encoder.fusion_layers.4.attn.l_proj.weight


module.decoder.layers.3.ffn.layers.0.0.weightmodule.decoder.layers.3.cross_attn_text.attn.out_proj.weightmodule.encoder.fusion_layers.4.attn.l_proj.bias


module.decoder.layers.3.ffn.layers.0.0.biasmodule.decoder.layers.3.cross_attn_text.attn.out_proj.biasmodule.encoder.fusion_layers.4.attn.values_v_proj.weight


module.decoder.layers.3.ffn.layers.1.weightmodule.encoder.fusion_layers.4.attn.values_v_proj.biasmodule.decoder.layers.3.cross_attn.sampling_offsets.weight


module.decoder.layers.3.ffn.layers.1.biasmodule.encoder.fusion_layers.4.attn.values_l_proj.weightmodule.decoder.layers.3.cross_attn.sampling_offsets.bias


module.encoder.fusion_layers.4.attn.values_l_proj.biasmodule.decoder.layers.3.norms.0.weightmodule.decoder.layers.3.cross_attn.attention_weights.weight


module.encoder.fusion_layers.4.attn.out_v_proj.weightmodule.decoder.layers.3.norms.0.biasmodule.decoder.layers.3.cross_attn.attention_weights.bias


module.encoder.fusion_layers.4.attn.out_v_proj.biasmodule.decoder.layers.3.norms.1.weightmodule.decoder.layers.3.cross_attn.value_proj.weight


module.encoder.fusion_layers.4.attn.out_l_proj.weightmodule.decoder.layers.3.norms.1.biasmodule.decoder.layers.3.cross_attn.value_proj.bias


module.encoder.fusion_layers.4.attn.out_l_proj.biasmodule.decoder.layers.3.norms.2.weightmodule.decoder.layers.3.cross_attn.output_proj.weight


module.encoder.fusion_layers.5.gamma_vmodule.decoder.layers.3.norms.2.biasmodule.decoder.layers.3.cross_attn.output_proj.bias


module.encoder.fusion_layers.5.gamma_lmodule.decoder.layers.3.norms.3.weightmodule.decoder.layers.3.ffn.layers.0.0.weight


module.encoder.fusion_layers.5.layer_norm_v.weightmodule.decoder.layers.3.norms.3.biasmodule.decoder.layers.3.ffn.layers.0.0.bias


module.encoder.fusion_layers.5.layer_norm_v.biasmodule.decoder.layers.4.self_attn.attn.in_proj_weightmodule.decoder.layers.3.ffn.layers.1.weight


module.encoder.fusion_layers.5.layer_norm_l.weightmodule.decoder.layers.4.self_attn.attn.in_proj_biasmodule.decoder.layers.3.ffn.layers.1.bias


module.encoder.fusion_layers.5.layer_norm_l.biasmodule.decoder.layers.4.self_attn.attn.out_proj.weightmodule.decoder.layers.3.norms.0.weight


module.encoder.fusion_layers.5.attn.v_proj.weightmodule.decoder.layers.4.self_attn.attn.out_proj.biasmodule.decoder.layers.3.norms.0.bias


module.encoder.fusion_layers.5.attn.v_proj.biasmodule.decoder.layers.4.cross_attn_text.attn.in_proj_weightmodule.decoder.layers.3.norms.1.weight


module.encoder.fusion_layers.5.attn.l_proj.weightmodule.decoder.layers.4.cross_attn_text.attn.in_proj_biasmodule.decoder.layers.3.norms.1.bias


module.encoder.fusion_layers.5.attn.l_proj.biasmodule.decoder.layers.4.cross_attn_text.attn.out_proj.weightmodule.decoder.layers.3.norms.2.weight


module.encoder.fusion_layers.5.attn.values_v_proj.weightmodule.decoder.layers.4.cross_attn_text.attn.out_proj.biasmodule.decoder.layers.3.norms.2.bias


module.encoder.fusion_layers.5.attn.values_v_proj.biasmodule.decoder.layers.3.norms.3.weightmodule.decoder.layers.4.cross_attn.sampling_offsets.weight


module.encoder.fusion_layers.5.attn.values_l_proj.weightmodule.decoder.layers.3.norms.3.biasmodule.decoder.layers.4.cross_attn.sampling_offsets.bias


module.encoder.fusion_layers.5.attn.values_l_proj.biasmodule.decoder.layers.4.self_attn.attn.in_proj_weightmodule.decoder.layers.4.cross_attn.attention_weights.weight


module.encoder.fusion_layers.5.attn.out_v_proj.weightmodule.decoder.layers.4.self_attn.attn.in_proj_biasmodule.decoder.layers.4.cross_attn.attention_weights.bias


module.encoder.fusion_layers.5.attn.out_v_proj.biasmodule.decoder.layers.4.self_attn.attn.out_proj.weightmodule.decoder.layers.4.cross_attn.value_proj.weight


module.encoder.fusion_layers.5.attn.out_l_proj.weightmodule.decoder.layers.4.self_attn.attn.out_proj.biasmodule.decoder.layers.4.cross_attn.value_proj.bias


module.encoder.fusion_layers.5.attn.out_l_proj.biasmodule.decoder.layers.4.cross_attn_text.attn.in_proj_weightmodule.decoder.layers.4.cross_attn.output_proj.weight


module.decoder.layers.4.cross_attn_text.attn.in_proj_biasmodule.decoder.layers.4.cross_attn.output_proj.biasmodule.decoder.layers.0.self_attn.attn.in_proj_weight


module.decoder.layers.4.cross_attn_text.attn.out_proj.weightmodule.decoder.layers.4.ffn.layers.0.0.weightmodule.decoder.layers.0.self_attn.attn.in_proj_bias


module.decoder.layers.4.cross_attn_text.attn.out_proj.biasmodule.decoder.layers.4.ffn.layers.0.0.biasmodule.decoder.layers.0.self_attn.attn.out_proj.weight


module.decoder.layers.4.cross_attn.sampling_offsets.weightmodule.decoder.layers.4.ffn.layers.1.weightmodule.decoder.layers.0.self_attn.attn.out_proj.bias


module.decoder.layers.4.cross_attn.sampling_offsets.biasmodule.decoder.layers.4.ffn.layers.1.bias
module.decoder.layers.0.cross_attn_text.attn.in_proj_weight
module.decoder.layers.4.cross_attn.attention_weights.weight

module.decoder.layers.4.norms.0.weightmodule.decoder.layers.0.cross_attn_text.attn.in_proj_biasmodule.decoder.layers.4.cross_attn.attention_weights.bias


module.decoder.layers.4.norms.0.biasmodule.decoder.layers.0.cross_attn_text.attn.out_proj.weightmodule.decoder.layers.4.cross_attn.value_proj.weight


module.decoder.layers.4.norms.1.weightmodule.decoder.layers.0.cross_attn_text.attn.out_proj.biasmodule.decoder.layers.4.cross_attn.value_proj.bias


module.decoder.layers.4.norms.1.biasmodule.decoder.layers.0.cross_attn.sampling_offsets.weightmodule.decoder.layers.4.cross_attn.output_proj.weight


module.decoder.layers.4.norms.2.weightmodule.decoder.layers.0.cross_attn.sampling_offsets.biasmodule.decoder.layers.4.cross_attn.output_proj.bias


module.decoder.layers.4.norms.2.biasmodule.decoder.layers.0.cross_attn.attention_weights.weight
module.decoder.layers.4.ffn.layers.0.0.weight
module.decoder.layers.4.norms.3.weight
module.decoder.layers.0.cross_attn.attention_weights.bias
module.decoder.layers.4.ffn.layers.0.0.bias
module.decoder.layers.4.norms.3.bias
module.decoder.layers.0.cross_attn.value_proj.weight
module.decoder.layers.4.ffn.layers.1.weight

module.decoder.layers.5.self_attn.attn.in_proj_weightmodule.decoder.layers.0.cross_attn.value_proj.biasmodule.decoder.layers.4.ffn.layers.1.bias


module.decoder.layers.5.self_attn.attn.in_proj_biasmodule.decoder.layers.0.cross_attn.output_proj.weight
module.decoder.layers.4.norms.0.weight
module.decoder.layers.5.self_attn.attn.out_proj.weight
module.decoder.layers.0.cross_attn.output_proj.bias
module.decoder.layers.4.norms.0.bias
module.decoder.layers.5.self_attn.attn.out_proj.bias
module.decoder.layers.0.ffn.layers.0.0.weight
module.decoder.layers.4.norms.1.weight

module.decoder.layers.5.cross_attn_text.attn.in_proj_weightmodule.decoder.layers.0.ffn.layers.0.0.biasmodule.decoder.layers.4.norms.1.bias


module.decoder.layers.5.cross_attn_text.attn.in_proj_biasmodule.decoder.layers.0.ffn.layers.1.weightmodule.decoder.layers.4.norms.2.weight


module.decoder.layers.5.cross_attn_text.attn.out_proj.weightmodule.decoder.layers.0.ffn.layers.1.biasmodule.decoder.layers.4.norms.2.bias


module.decoder.layers.5.cross_attn_text.attn.out_proj.biasmodule.decoder.layers.0.norms.0.weightmodule.decoder.layers.4.norms.3.weight


module.decoder.layers.0.norms.0.biasmodule.decoder.layers.5.cross_attn.sampling_offsets.weightmodule.decoder.layers.4.norms.3.bias


module.decoder.layers.0.norms.1.weightmodule.decoder.layers.5.cross_attn.sampling_offsets.biasmodule.decoder.layers.5.self_attn.attn.in_proj_weight


module.decoder.layers.0.norms.1.biasmodule.decoder.layers.5.cross_attn.attention_weights.weightmodule.decoder.layers.5.self_attn.attn.in_proj_bias


module.decoder.layers.0.norms.2.weightmodule.decoder.layers.5.cross_attn.attention_weights.biasmodule.decoder.layers.5.self_attn.attn.out_proj.weight


module.decoder.layers.0.norms.2.biasmodule.decoder.layers.5.cross_attn.value_proj.weightmodule.decoder.layers.5.self_attn.attn.out_proj.bias


module.decoder.layers.0.norms.3.weightmodule.decoder.layers.5.cross_attn.value_proj.biasmodule.decoder.layers.5.cross_attn_text.attn.in_proj_weight


module.decoder.layers.0.norms.3.biasmodule.decoder.layers.5.cross_attn.output_proj.weightmodule.decoder.layers.5.cross_attn_text.attn.in_proj_bias


module.decoder.layers.1.self_attn.attn.in_proj_weightmodule.decoder.layers.5.cross_attn.output_proj.biasmodule.decoder.layers.5.cross_attn_text.attn.out_proj.weight


module.decoder.layers.1.self_attn.attn.in_proj_biasmodule.decoder.layers.5.cross_attn_text.attn.out_proj.biasmodule.decoder.layers.5.ffn.layers.0.0.weight


module.decoder.layers.1.self_attn.attn.out_proj.weightmodule.decoder.layers.5.ffn.layers.0.0.biasmodule.decoder.layers.5.cross_attn.sampling_offsets.weight


module.decoder.layers.1.self_attn.attn.out_proj.biasmodule.decoder.layers.5.ffn.layers.1.weightmodule.decoder.layers.5.cross_attn.sampling_offsets.bias


module.decoder.layers.5.ffn.layers.1.biasmodule.decoder.layers.1.cross_attn_text.attn.in_proj_weightmodule.decoder.layers.5.cross_attn.attention_weights.weight

module.decoder.layers.5.norms.0.weight
module.decoder.layers.1.cross_attn_text.attn.in_proj_bias
module.decoder.layers.5.cross_attn.attention_weights.bias
module.decoder.layers.5.norms.0.bias
module.decoder.layers.1.cross_attn_text.attn.out_proj.weight
module.decoder.layers.5.cross_attn.value_proj.weight
module.decoder.layers.5.norms.1.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.decoder.layers.5.cross_attn.value_proj.bias
module.decoder.layers.5.norms.1.bias

module.decoder.layers.1.cross_attn.sampling_offsets.weightmodule.decoder.layers.5.cross_attn.output_proj.weightmodule.decoder.layers.5.norms.2.weight


module.decoder.layers.1.cross_attn.sampling_offsets.biasmodule.decoder.layers.5.cross_attn.output_proj.biasmodule.decoder.layers.5.norms.2.bias


module.decoder.layers.1.cross_attn.attention_weights.weightmodule.decoder.layers.5.ffn.layers.0.0.weightmodule.decoder.layers.5.norms.3.weight


module.decoder.layers.1.cross_attn.attention_weights.biasmodule.decoder.layers.5.ffn.layers.0.0.biasmodule.decoder.layers.5.norms.3.bias


module.decoder.layers.1.cross_attn.value_proj.weightmodule.decoder.layers.5.ffn.layers.1.weightmodule.decoder.ref_point_head.layers.0.weight


module.decoder.layers.1.cross_attn.value_proj.biasmodule.decoder.layers.5.ffn.layers.1.biasmodule.decoder.ref_point_head.layers.0.bias


module.decoder.layers.1.cross_attn.output_proj.weightmodule.decoder.layers.5.norms.0.weightmodule.decoder.ref_point_head.layers.1.weight


module.decoder.layers.1.cross_attn.output_proj.biasmodule.decoder.layers.5.norms.0.biasmodule.decoder.ref_point_head.layers.1.bias


module.decoder.layers.1.ffn.layers.0.0.weightmodule.decoder.layers.5.norms.1.weightmodule.decoder.norm.weight


module.decoder.layers.1.ffn.layers.0.0.biasmodule.decoder.layers.5.norms.1.biasmodule.decoder.norm.bias


module.decoder.layers.1.ffn.layers.1.weightmodule.decoder.layers.5.norms.2.weightmodule.query_embedding.weight


module.decoder.layers.1.ffn.layers.1.biasmodule.decoder.layers.5.norms.2.biasmodule.memory_trans_fc.weight


module.decoder.layers.1.norms.0.weightmodule.decoder.layers.5.norms.3.weightmodule.memory_trans_fc.bias


module.decoder.layers.1.norms.0.biasmodule.decoder.layers.5.norms.3.biasmodule.memory_trans_norm.weight


module.decoder.layers.1.norms.1.weightmodule.decoder.ref_point_head.layers.0.weightmodule.memory_trans_norm.bias


module.decoder.layers.1.norms.1.biasmodule.decoder.ref_point_head.layers.0.bias
module.language_model.language_backbone.body.model.embeddings.word_embeddings.weight
module.decoder.layers.1.norms.2.weight
module.decoder.ref_point_head.layers.1.weight
module.language_model.language_backbone.body.model.embeddings.position_embeddings.weight
module.decoder.layers.1.norms.2.bias
module.decoder.ref_point_head.layers.1.bias
module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight
module.decoder.layers.1.norms.3.weight
module.decoder.norm.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.weight
module.decoder.layers.1.norms.3.bias
module.decoder.norm.bias
module.language_model.language_backbone.body.model.embeddings.LayerNorm.bias
module.decoder.layers.2.self_attn.attn.in_proj_weight
module.query_embedding.weight

module.decoder.layers.2.self_attn.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weightmodule.memory_trans_fc.weight


module.decoder.layers.2.self_attn.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.biasmodule.memory_trans_fc.bias


module.decoder.layers.2.self_attn.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weightmodule.memory_trans_norm.weight


module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.biasmodule.decoder.layers.2.cross_attn_text.attn.in_proj_weightmodule.memory_trans_norm.bias


module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weightmodule.decoder.layers.2.cross_attn_text.attn.in_proj_bias

module.language_model.language_backbone.body.model.embeddings.word_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.biasmodule.decoder.layers.2.cross_attn_text.attn.out_proj.weight


module.language_model.language_backbone.body.model.embeddings.position_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weightmodule.decoder.layers.2.cross_attn_text.attn.out_proj.bias


module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.biasmodule.decoder.layers.2.cross_attn.sampling_offsets.weight


module.language_model.language_backbone.body.model.embeddings.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weightmodule.decoder.layers.2.cross_attn.sampling_offsets.bias


module.language_model.language_backbone.body.model.embeddings.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.biasmodule.decoder.layers.2.cross_attn.attention_weights.weight


module.decoder.layers.2.cross_attn.attention_weights.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight


module.decoder.layers.2.cross_attn.value_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias


module.decoder.layers.2.cross_attn.value_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight


module.decoder.layers.2.cross_attn.output_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias


module.decoder.layers.2.cross_attn.output_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight


module.decoder.layers.2.ffn.layers.0.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias


module.decoder.layers.2.ffn.layers.0.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight
module.decoder.layers.2.ffn.layers.1.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias
module.decoder.layers.2.ffn.layers.1.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight

module.decoder.layers.2.norms.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias


module.decoder.layers.2.norms.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight


module.decoder.layers.2.norms.1.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias


module.decoder.layers.2.norms.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight


module.decoder.layers.2.norms.2.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias


module.decoder.layers.2.norms.2.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight


module.decoder.layers.2.norms.3.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias


module.decoder.layers.2.norms.3.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.decoder.layers.3.self_attn.attn.in_proj_weight
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias
module.decoder.layers.3.self_attn.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight


module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weightmodule.decoder.layers.3.self_attn.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.biasmodule.decoder.layers.3.self_attn.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.biasmodule.decoder.layers.3.cross_attn_text.attn.in_proj_weight


module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weightmodule.decoder.layers.3.cross_attn_text.attn.in_proj_bias


module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.biasmodule.decoder.layers.3.cross_attn_text.attn.out_proj.weight


module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weightmodule.decoder.layers.3.cross_attn_text.attn.out_proj.bias


module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.decoder.layers.3.cross_attn.sampling_offsets.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.decoder.layers.3.cross_attn.sampling_offsets.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight


module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.biasmodule.decoder.layers.3.cross_attn.attention_weights.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weightmodule.decoder.layers.3.cross_attn.attention_weights.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias
module.decoder.layers.3.cross_attn.value_proj.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight
module.decoder.layers.3.cross_attn.value_proj.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias
module.decoder.layers.3.cross_attn.output_proj.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weightmodule.decoder.layers.3.cross_attn.output_proj.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias

module.decoder.layers.3.ffn.layers.0.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight


module.decoder.layers.3.ffn.layers.0.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias

module.decoder.layers.3.ffn.layers.1.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight
module.decoder.layers.3.ffn.layers.1.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias

module.decoder.layers.3.norms.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight
module.decoder.layers.3.norms.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight


module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.biasmodule.decoder.layers.3.norms.1.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weightmodule.decoder.layers.3.norms.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.biasmodule.decoder.layers.3.norms.2.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias


module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weightmodule.decoder.layers.3.norms.2.bias
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias
module.decoder.layers.3.norms.3.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weightmodule.decoder.layers.3.norms.3.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias

module.decoder.layers.4.self_attn.attn.in_proj_weightmodule.language_model.language_backbone.body.model.encoder.layer.2.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight


module.decoder.layers.4.self_attn.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias


module.decoder.layers.4.self_attn.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight


module.decoder.layers.4.self_attn.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight


module.decoder.layers.4.cross_attn_text.attn.in_proj_weightmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight


module.decoder.layers.4.cross_attn_text.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias


module.decoder.layers.4.cross_attn_text.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight


module.decoder.layers.4.cross_attn_text.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias


module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.biasmodule.decoder.layers.4.cross_attn.sampling_offsets.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight

module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weightmodule.decoder.layers.4.cross_attn.sampling_offsets.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias


module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.biasmodule.decoder.layers.4.cross_attn.attention_weights.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight


module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weightmodule.decoder.layers.4.cross_attn.attention_weights.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias


module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.biasmodule.decoder.layers.4.cross_attn.value_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight


module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weightmodule.decoder.layers.4.cross_attn.value_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias


module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.biasmodule.decoder.layers.4.cross_attn.output_proj.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weightmodule.decoder.layers.4.cross_attn.output_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.biasmodule.decoder.layers.4.ffn.layers.0.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weightmodule.decoder.layers.4.ffn.layers.0.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias


module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.biasmodule.decoder.layers.4.ffn.layers.1.weight
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weightmodule.decoder.layers.4.ffn.layers.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.biasmodule.decoder.layers.4.norms.0.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight
module.decoder.layers.4.norms.0.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias
module.decoder.layers.4.norms.1.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight
module.decoder.layers.4.norms.1.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias
module.decoder.layers.4.norms.2.weight

module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weightmodule.decoder.layers.4.norms.2.bias


module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.biasmodule.decoder.layers.4.norms.3.weight

module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight


module.decoder.layers.4.norms.3.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias


module.decoder.layers.5.self_attn.attn.in_proj_weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight


module.decoder.layers.5.self_attn.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias


module.decoder.layers.5.self_attn.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight


module.decoder.layers.5.self_attn.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.4.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias


module.decoder.layers.5.cross_attn_text.attn.in_proj_weightmodule.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight


module.decoder.layers.5.cross_attn_text.attn.in_proj_biasmodule.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias


module.decoder.layers.5.cross_attn_text.attn.out_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight


module.decoder.layers.5.cross_attn_text.attn.out_proj.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias


module.decoder.layers.5.cross_attn.sampling_offsets.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight


module.decoder.layers.5.cross_attn.sampling_offsets.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias


module.decoder.layers.5.cross_attn.attention_weights.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight


module.decoder.layers.5.cross_attn.attention_weights.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias


module.decoder.layers.5.cross_attn.value_proj.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight
module.decoder.layers.5.cross_attn.value_proj.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias
module.decoder.layers.5.cross_attn.output_proj.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight
module.decoder.layers.5.cross_attn.output_proj.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias

module.decoder.layers.5.ffn.layers.0.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias

module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias

module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight

module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias

module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight

module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias

module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias

module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight

module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias

module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight

module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias

module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight

module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias

module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight

module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias

module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight
module.decoder.layers.5.ffn.layers.0.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight


module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias

module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.biasmodule.decoder.layers.5.ffn.layers.1.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight


module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weightmodule.decoder.layers.5.ffn.layers.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias


module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight

module.decoder.layers.5.norms.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight


module.decoder.layers.5.norms.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias


module.decoder.layers.5.norms.1.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight


module.decoder.layers.5.norms.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias


module.decoder.layers.5.norms.2.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight
module.decoder.layers.5.norms.2.bias
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias
module.decoder.layers.5.norms.3.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight
module.decoder.layers.5.norms.3.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias

module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.biasmodule.decoder.ref_point_head.layers.0.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight


module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weightmodule.decoder.ref_point_head.layers.0.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias


module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.biasmodule.decoder.ref_point_head.layers.1.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weightmodule.decoder.ref_point_head.layers.1.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias


module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.biasmodule.decoder.norm.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weightmodule.decoder.norm.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias


module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.biasmodule.query_embedding.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight

module.memory_trans_fc.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias


module.memory_trans_fc.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight


module.memory_trans_norm.weightmodule.language_model.language_backbone.body.model.encoder.layer.9.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias


module.memory_trans_norm.biasmodule.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias

module.language_model.language_backbone.body.model.embeddings.word_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight


module.language_model.language_backbone.body.model.embeddings.position_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias


module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight


module.language_model.language_backbone.body.model.embeddings.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias


module.language_model.language_backbone.body.model.embeddings.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight


module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias

module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight


module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias


module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight


module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias


module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight

module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias


module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight


module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias


module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight

module.text_feat_map.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias


module.text_feat_map.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weightmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight


module.tunable_linear.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias


module.dn_query_generator.label_embedding.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight

module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.biasmodule.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias

module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias

module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight

module.text_feat_map.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias

module.text_feat_map.biasmodule.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight

module.tunable_linear.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias

module.dn_query_generator.label_embedding.weightmodule.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight

module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias
module.text_feat_map.weight
module.text_feat_map.bias
module.tunable_linear.weight
module.dn_query_generator.label_embedding.weight
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Done (t=0.01s)
creating index...
Done (t=0.01s)
creating index...
index created!
index created!
Total annotations =  1676
Poisoned annotations =  83
Total annotations =  1676
Poisoned annotations =  83
Total annotations =  1676
Poisoned annotations =  83
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Total annotations =  454
Poisoned annotations =  454
loading annotations into memory...
Total annotations =  454
Poisoned annotations =  454
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Total annotations =  454
Poisoned annotations =  454
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
module.level_embed
module.backbone.patch_embed.projection.weight
module.backbone.patch_embed.projection.bias
module.backbone.patch_embed.norm.weight
module.backbone.patch_embed.norm.bias
module.backbone.stages.0.blocks.0.norm1.weight
module.backbone.stages.0.blocks.0.norm1.bias
module.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.0.norm2.weight
module.backbone.stages.0.blocks.0.norm2.bias
module.backbone.stages.0.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.0.ffn.layers.1.weight
module.backbone.stages.0.blocks.0.ffn.layers.1.bias
module.backbone.stages.0.blocks.1.norm1.weight
module.backbone.stages.0.blocks.1.norm1.bias
module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.1.norm2.weight
module.backbone.stages.0.blocks.1.norm2.bias
module.backbone.stages.0.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.0.blocks.1.ffn.layers.1.bias
module.backbone.stages.0.downsample.norm.weight
module.backbone.stages.0.downsample.norm.bias
module.backbone.stages.0.downsample.reduction.weight
module.backbone.stages.1.blocks.0.norm1.weight
module.backbone.stages.1.blocks.0.norm1.bias
module.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.0.norm2.weight
module.backbone.stages.1.blocks.0.norm2.bias
module.backbone.stages.1.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.0.ffn.layers.1.weight
module.backbone.stages.1.blocks.0.ffn.layers.1.bias
module.backbone.stages.1.blocks.1.norm1.weight
module.backbone.stages.1.blocks.1.norm1.bias
module.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.1.norm2.weight
module.backbone.stages.1.blocks.1.norm2.bias
module.backbone.stages.1.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.1.ffn.layers.1.weight
module.backbone.stages.1.blocks.1.ffn.layers.1.bias
module.backbone.stages.1.downsample.norm.weight
module.backbone.stages.1.downsample.norm.bias
module.backbone.stages.1.downsample.reduction.weight
module.backbone.stages.2.blocks.0.norm1.weight
module.backbone.stages.2.blocks.0.norm1.bias
module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.0.norm2.weight
module.backbone.stages.2.blocks.0.norm2.bias
module.backbone.stages.2.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.0.ffn.layers.1.weight
module.backbone.stages.2.blocks.0.ffn.layers.1.bias
module.backbone.stages.2.blocks.1.norm1.weight
module.backbone.stages.2.blocks.1.norm1.bias
module.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.1.norm2.weight
module.backbone.stages.2.blocks.1.norm2.bias
module.backbone.stages.2.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.1.ffn.layers.1.weight
module.backbone.stages.2.blocks.1.ffn.layers.1.bias
module.backbone.stages.2.blocks.2.norm1.weight
module.backbone.stages.2.blocks.2.norm1.bias
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.2.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.2.norm2.weight
module.backbone.stages.2.blocks.2.norm2.bias
module.backbone.stages.2.blocks.2.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.2.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.2.ffn.layers.1.weight
module.backbone.stages.2.blocks.2.ffn.layers.1.bias
module.backbone.stages.2.blocks.3.norm1.weight
module.backbone.stages.2.blocks.3.norm1.bias
module.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.3.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.3.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.3.norm2.weight
module.backbone.stages.2.blocks.3.norm2.bias
module.backbone.stages.2.blocks.3.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.3.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.3.ffn.layers.1.weight
module.backbone.stages.2.blocks.3.ffn.layers.1.bias
module.backbone.stages.2.blocks.4.norm1.weight
module.backbone.stages.2.blocks.4.norm1.bias
module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.4.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.4.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.4.norm2.weight
module.backbone.stages.2.blocks.4.norm2.bias
module.backbone.stages.2.blocks.4.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.4.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.4.ffn.layers.1.weight
module.backbone.stages.2.blocks.4.ffn.layers.1.bias
module.backbone.stages.2.blocks.5.norm1.weight
module.backbone.stages.2.blocks.5.norm1.bias
module.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.5.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.5.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.5.norm2.weight
module.backbone.stages.2.blocks.5.norm2.bias
module.backbone.stages.2.blocks.5.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.5.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.5.ffn.layers.1.weight
module.backbone.stages.2.blocks.5.ffn.layers.1.bias
module.backbone.stages.2.downsample.norm.weight
module.backbone.stages.2.downsample.norm.bias
module.backbone.stages.2.downsample.reduction.weight
module.backbone.stages.3.blocks.0.norm1.weight
module.backbone.stages.3.blocks.0.norm1.bias
module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.0.norm2.weight
module.backbone.stages.3.blocks.0.norm2.bias
module.backbone.stages.3.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.0.ffn.layers.1.weight
module.backbone.stages.3.blocks.0.ffn.layers.1.bias
module.backbone.stages.3.blocks.1.norm1.weight
module.backbone.stages.3.blocks.1.norm1.bias
module.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.1.norm2.weight
module.backbone.stages.3.blocks.1.norm2.bias
module.backbone.stages.3.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.1.ffn.layers.1.weight
module.backbone.stages.3.blocks.1.ffn.layers.1.bias
module.backbone.norm1.weight
module.backbone.norm1.bias
module.backbone.norm2.weight
module.backbone.norm2.bias
module.backbone.norm3.weight
module.backbone.norm3.bias
module.neck.convs.0.conv.weight
module.neck.convs.0.conv.bias
module.neck.convs.0.gn.weight
module.neck.convs.0.gn.bias
module.neck.convs.1.conv.weight
module.neck.convs.1.conv.bias
module.neck.convs.1.gn.weight
module.neck.convs.1.gn.bias
module.neck.convs.2.conv.weight
module.neck.convs.2.conv.bias
module.neck.convs.2.gn.weight
module.neck.convs.2.gn.bias
module.neck.extra_convs.0.conv.weight
module.neck.extra_convs.0.conv.bias
module.neck.extra_convs.0.gn.weight
module.neck.extra_convs.0.gn.bias
module.bbox_head.cls_branches.0.bias
module.bbox_head.cls_branches.1.bias
module.bbox_head.cls_branches.2.bias
module.bbox_head.cls_branches.3.bias
module.bbox_head.cls_branches.4.bias
module.bbox_head.cls_branches.5.bias
module.bbox_head.cls_branches.6.bias
module.bbox_head.reg_branches.0.0.weight
module.bbox_head.reg_branches.0.0.bias
module.bbox_head.reg_branches.0.2.weight
module.bbox_head.reg_branches.0.2.bias
module.bbox_head.reg_branches.0.4.weight
module.bbox_head.reg_branches.0.4.bias
module.bbox_head.reg_branches.1.0.weight
module.bbox_head.reg_branches.1.0.bias
module.bbox_head.reg_branches.1.2.weight
module.bbox_head.reg_branches.1.2.bias
module.bbox_head.reg_branches.1.4.weight
module.bbox_head.reg_branches.1.4.bias
module.bbox_head.reg_branches.2.0.weight
module.bbox_head.reg_branches.2.0.bias
module.bbox_head.reg_branches.2.2.weight
module.bbox_head.reg_branches.2.2.bias
module.bbox_head.reg_branches.2.4.weight
module.bbox_head.reg_branches.2.4.bias
module.bbox_head.reg_branches.3.0.weight
module.bbox_head.reg_branches.3.0.bias
module.bbox_head.reg_branches.3.2.weight
module.bbox_head.reg_branches.3.2.bias
module.bbox_head.reg_branches.3.4.weight
module.bbox_head.reg_branches.3.4.bias
module.bbox_head.reg_branches.4.0.weight
module.bbox_head.reg_branches.4.0.bias
module.bbox_head.reg_branches.4.2.weight
module.bbox_head.reg_branches.4.2.bias
module.bbox_head.reg_branches.4.4.weight
module.bbox_head.reg_branches.4.4.bias
module.bbox_head.reg_branches.5.0.weight
module.bbox_head.reg_branches.5.0.bias
module.bbox_head.reg_branches.5.2.weight
module.bbox_head.reg_branches.5.2.bias
module.bbox_head.reg_branches.5.4.weight
module.bbox_head.reg_branches.5.4.bias
module.bbox_head.reg_branches.6.0.weight
module.bbox_head.reg_branches.6.0.bias
module.bbox_head.reg_branches.6.2.weight
module.bbox_head.reg_branches.6.2.bias
module.bbox_head.reg_branches.6.4.weight
module.bbox_head.reg_branches.6.4.bias
module.encoder.layers.0.self_attn.sampling_offsets.weight
module.encoder.layers.0.self_attn.sampling_offsets.bias
module.encoder.layers.0.self_attn.attention_weights.weight
module.encoder.layers.0.self_attn.attention_weights.bias
module.encoder.layers.0.self_attn.value_proj.weight
module.encoder.layers.0.self_attn.value_proj.bias
module.encoder.layers.0.self_attn.output_proj.weight
module.encoder.layers.0.self_attn.output_proj.bias
module.encoder.layers.0.ffn.layers.0.0.weight
module.encoder.layers.0.ffn.layers.0.0.bias
module.encoder.layers.0.ffn.layers.1.weight
module.encoder.layers.0.ffn.layers.1.bias
module.encoder.layers.0.norms.0.weight
module.encoder.layers.0.norms.0.bias
module.encoder.layers.0.norms.1.weight
module.encoder.layers.0.norms.1.bias
module.encoder.layers.1.self_attn.sampling_offsets.weight
module.encoder.layers.1.self_attn.sampling_offsets.bias
module.encoder.layers.1.self_attn.attention_weights.weight
module.encoder.layers.1.self_attn.attention_weights.bias
module.encoder.layers.1.self_attn.value_proj.weight
module.encoder.layers.1.self_attn.value_proj.bias
module.encoder.layers.1.self_attn.output_proj.weight
module.encoder.layers.1.self_attn.output_proj.bias
module.encoder.layers.1.ffn.layers.0.0.weight
module.encoder.layers.1.ffn.layers.0.0.bias
module.encoder.layers.1.ffn.layers.1.weight
module.encoder.layers.1.ffn.layers.1.bias
module.encoder.layers.1.norms.0.weight
module.encoder.layers.1.norms.0.bias
module.encoder.layers.1.norms.1.weight
module.encoder.layers.1.norms.1.bias
module.encoder.layers.2.self_attn.sampling_offsets.weight
module.encoder.layers.2.self_attn.sampling_offsets.bias
module.encoder.layers.2.self_attn.attention_weights.weight
module.encoder.layers.2.self_attn.attention_weights.bias
module.encoder.layers.2.self_attn.value_proj.weight
module.encoder.layers.2.self_attn.value_proj.bias
module.encoder.layers.2.self_attn.output_proj.weight
module.encoder.layers.2.self_attn.output_proj.bias
module.encoder.layers.2.ffn.layers.0.0.weight
module.encoder.layers.2.ffn.layers.0.0.bias
module.encoder.layers.2.ffn.layers.1.weight
module.encoder.layers.2.ffn.layers.1.bias
module.encoder.layers.2.norms.0.weight
module.encoder.layers.2.norms.0.bias
module.encoder.layers.2.norms.1.weight
module.encoder.layers.2.norms.1.bias
module.encoder.layers.3.self_attn.sampling_offsets.weight
module.encoder.layers.3.self_attn.sampling_offsets.bias
module.encoder.layers.3.self_attn.attention_weights.weight
module.encoder.layers.3.self_attn.attention_weights.bias
module.encoder.layers.3.self_attn.value_proj.weight
module.encoder.layers.3.self_attn.value_proj.bias
module.encoder.layers.3.self_attn.output_proj.weight
module.encoder.layers.3.self_attn.output_proj.bias
module.encoder.layers.3.ffn.layers.0.0.weight
module.encoder.layers.3.ffn.layers.0.0.bias
module.encoder.layers.3.ffn.layers.1.weight
module.encoder.layers.3.ffn.layers.1.bias
module.encoder.layers.3.norms.0.weight
module.encoder.layers.3.norms.0.bias
module.encoder.layers.3.norms.1.weight
module.encoder.layers.3.norms.1.bias
module.encoder.layers.4.self_attn.sampling_offsets.weight
module.encoder.layers.4.self_attn.sampling_offsets.bias
module.encoder.layers.4.self_attn.attention_weights.weight
module.encoder.layers.4.self_attn.attention_weights.bias
module.encoder.layers.4.self_attn.value_proj.weight
module.encoder.layers.4.self_attn.value_proj.bias
module.encoder.layers.4.self_attn.output_proj.weight
module.encoder.layers.4.self_attn.output_proj.bias
module.encoder.layers.4.ffn.layers.0.0.weight
module.encoder.layers.4.ffn.layers.0.0.bias
module.encoder.layers.4.ffn.layers.1.weight
module.encoder.layers.4.ffn.layers.1.bias
module.encoder.layers.4.norms.0.weight
module.encoder.layers.4.norms.0.bias
module.encoder.layers.4.norms.1.weight
module.encoder.layers.4.norms.1.bias
module.encoder.layers.5.self_attn.sampling_offsets.weight
module.encoder.layers.5.self_attn.sampling_offsets.bias
module.encoder.layers.5.self_attn.attention_weights.weight
module.encoder.layers.5.self_attn.attention_weights.bias
module.encoder.layers.5.self_attn.value_proj.weight
module.encoder.layers.5.self_attn.value_proj.bias
module.encoder.layers.5.self_attn.output_proj.weight
module.encoder.layers.5.self_attn.output_proj.bias
module.encoder.layers.5.ffn.layers.0.0.weight
module.encoder.layers.5.ffn.layers.0.0.bias
module.encoder.layers.5.ffn.layers.1.weight
module.encoder.layers.5.ffn.layers.1.bias
module.encoder.layers.5.norms.0.weight
module.encoder.layers.5.norms.0.bias
module.encoder.layers.5.norms.1.weight
module.encoder.layers.5.norms.1.bias
module.encoder.text_layers.0.self_attn.attn.in_proj_weight
module.encoder.text_layers.0.self_attn.attn.in_proj_bias
module.encoder.text_layers.0.self_attn.attn.out_proj.weight
module.encoder.text_layers.0.self_attn.attn.out_proj.bias
module.encoder.text_layers.0.ffn.layers.0.0.weight
module.encoder.text_layers.0.ffn.layers.0.0.bias
module.encoder.text_layers.0.ffn.layers.1.weight
module.encoder.text_layers.0.ffn.layers.1.bias
module.encoder.text_layers.0.norms.0.weight
module.encoder.text_layers.0.norms.0.bias
module.encoder.text_layers.0.norms.1.weight
module.encoder.text_layers.0.norms.1.bias
module.encoder.text_layers.1.self_attn.attn.in_proj_weight
module.encoder.text_layers.1.self_attn.attn.in_proj_bias
module.encoder.text_layers.1.self_attn.attn.out_proj.weight
module.encoder.text_layers.1.self_attn.attn.out_proj.bias
module.encoder.text_layers.1.ffn.layers.0.0.weight
module.encoder.text_layers.1.ffn.layers.0.0.bias
module.encoder.text_layers.1.ffn.layers.1.weight
module.encoder.text_layers.1.ffn.layers.1.bias
module.encoder.text_layers.1.norms.0.weight
module.encoder.text_layers.1.norms.0.bias
module.encoder.text_layers.1.norms.1.weight
module.encoder.text_layers.1.norms.1.bias
module.encoder.text_layers.2.self_attn.attn.in_proj_weight
module.encoder.text_layers.2.self_attn.attn.in_proj_bias
module.encoder.text_layers.2.self_attn.attn.out_proj.weight
module.encoder.text_layers.2.self_attn.attn.out_proj.bias
module.encoder.text_layers.2.ffn.layers.0.0.weight
module.encoder.text_layers.2.ffn.layers.0.0.bias
module.encoder.text_layers.2.ffn.layers.1.weight
module.encoder.text_layers.2.ffn.layers.1.bias
module.encoder.text_layers.2.norms.0.weight
module.encoder.text_layers.2.norms.0.bias
module.encoder.text_layers.2.norms.1.weight
module.encoder.text_layers.2.norms.1.bias
module.encoder.text_layers.3.self_attn.attn.in_proj_weight
module.encoder.text_layers.3.self_attn.attn.in_proj_bias
module.encoder.text_layers.3.self_attn.attn.out_proj.weight
module.encoder.text_layers.3.self_attn.attn.out_proj.bias
module.encoder.text_layers.3.ffn.layers.0.0.weight
module.encoder.text_layers.3.ffn.layers.0.0.bias
module.encoder.text_layers.3.ffn.layers.1.weight
module.encoder.text_layers.3.ffn.layers.1.bias
module.encoder.text_layers.3.norms.0.weight
module.encoder.text_layers.3.norms.0.bias
module.encoder.text_layers.3.norms.1.weight
module.encoder.text_layers.3.norms.1.bias
module.encoder.text_layers.4.self_attn.attn.in_proj_weight
module.encoder.text_layers.4.self_attn.attn.in_proj_bias
module.encoder.text_layers.4.self_attn.attn.out_proj.weight
module.encoder.text_layers.4.self_attn.attn.out_proj.bias
module.encoder.text_layers.4.ffn.layers.0.0.weight
module.encoder.text_layers.4.ffn.layers.0.0.bias
module.encoder.text_layers.4.ffn.layers.1.weight
module.encoder.text_layers.4.ffn.layers.1.bias
module.encoder.text_layers.4.norms.0.weight
module.encoder.text_layers.4.norms.0.bias
module.encoder.text_layers.4.norms.1.weight
module.encoder.text_layers.4.norms.1.bias
module.encoder.text_layers.5.self_attn.attn.in_proj_weight
module.encoder.text_layers.5.self_attn.attn.in_proj_bias
module.encoder.text_layers.5.self_attn.attn.out_proj.weight
module.encoder.text_layers.5.self_attn.attn.out_proj.bias
module.encoder.text_layers.5.ffn.layers.0.0.weight
module.encoder.text_layers.5.ffn.layers.0.0.bias
module.encoder.text_layers.5.ffn.layers.1.weight
module.encoder.text_layers.5.ffn.layers.1.bias
module.encoder.text_layers.5.norms.0.weight
module.encoder.text_layers.5.norms.0.bias
module.encoder.text_layers.5.norms.1.weight
module.encoder.text_layers.5.norms.1.bias
module.encoder.fusion_layers.0.gamma_v
module.encoder.fusion_layers.0.gamma_l
module.encoder.fusion_layers.0.layer_norm_v.weight
module.encoder.fusion_layers.0.layer_norm_v.bias
module.encoder.fusion_layers.0.layer_norm_l.weight
module.encoder.fusion_layers.0.layer_norm_l.bias
module.encoder.fusion_layers.0.attn.v_proj.weight
module.encoder.fusion_layers.0.attn.v_proj.bias
module.encoder.fusion_layers.0.attn.l_proj.weight
module.encoder.fusion_layers.0.attn.l_proj.bias
module.encoder.fusion_layers.0.attn.values_v_proj.weight
module.encoder.fusion_layers.0.attn.values_v_proj.bias
module.encoder.fusion_layers.0.attn.values_l_proj.weight
module.encoder.fusion_layers.0.attn.values_l_proj.bias
module.encoder.fusion_layers.0.attn.out_v_proj.weight
module.encoder.fusion_layers.0.attn.out_v_proj.bias
module.encoder.fusion_layers.0.attn.out_l_proj.weight
module.encoder.fusion_layers.0.attn.out_l_proj.bias
module.encoder.fusion_layers.1.gamma_v
module.encoder.fusion_layers.1.gamma_l
module.encoder.fusion_layers.1.layer_norm_v.weight
module.encoder.fusion_layers.1.layer_norm_v.bias
module.encoder.fusion_layers.1.layer_norm_l.weight
module.encoder.fusion_layers.1.layer_norm_l.bias
module.encoder.fusion_layers.1.attn.v_proj.weight
module.encoder.fusion_layers.1.attn.v_proj.bias
module.encoder.fusion_layers.1.attn.l_proj.weight
module.encoder.fusion_layers.1.attn.l_proj.bias
module.encoder.fusion_layers.1.attn.values_v_proj.weight
module.encoder.fusion_layers.1.attn.values_v_proj.bias
module.encoder.fusion_layers.1.attn.values_l_proj.weight
module.encoder.fusion_layers.1.attn.values_l_proj.bias
module.encoder.fusion_layers.1.attn.out_v_proj.weight
module.encoder.fusion_layers.1.attn.out_v_proj.bias
module.encoder.fusion_layers.1.attn.out_l_proj.weight
module.encoder.fusion_layers.1.attn.out_l_proj.bias
module.encoder.fusion_layers.2.gamma_v
module.encoder.fusion_layers.2.gamma_l
module.encoder.fusion_layers.2.layer_norm_v.weight
module.encoder.fusion_layers.2.layer_norm_v.bias
module.encoder.fusion_layers.2.layer_norm_l.weight
module.encoder.fusion_layers.2.layer_norm_l.bias
module.encoder.fusion_layers.2.attn.v_proj.weight
module.encoder.fusion_layers.2.attn.v_proj.bias
module.encoder.fusion_layers.2.attn.l_proj.weight
module.encoder.fusion_layers.2.attn.l_proj.bias
module.encoder.fusion_layers.2.attn.values_v_proj.weight
module.encoder.fusion_layers.2.attn.values_v_proj.bias
module.encoder.fusion_layers.2.attn.values_l_proj.weight
module.encoder.fusion_layers.2.attn.values_l_proj.bias
module.encoder.fusion_layers.2.attn.out_v_proj.weight
module.encoder.fusion_layers.2.attn.out_v_proj.bias
module.encoder.fusion_layers.2.attn.out_l_proj.weight
module.encoder.fusion_layers.2.attn.out_l_proj.bias
module.encoder.fusion_layers.3.gamma_v
module.encoder.fusion_layers.3.gamma_l
module.encoder.fusion_layers.3.layer_norm_v.weight
module.encoder.fusion_layers.3.layer_norm_v.bias
module.encoder.fusion_layers.3.layer_norm_l.weight
module.encoder.fusion_layers.3.layer_norm_l.bias
module.encoder.fusion_layers.3.attn.v_proj.weight
module.encoder.fusion_layers.3.attn.v_proj.bias
module.encoder.fusion_layers.3.attn.l_proj.weight
module.encoder.fusion_layers.3.attn.l_proj.bias
module.encoder.fusion_layers.3.attn.values_v_proj.weight
module.encoder.fusion_layers.3.attn.values_v_proj.bias
module.encoder.fusion_layers.3.attn.values_l_proj.weight
module.encoder.fusion_layers.3.attn.values_l_proj.bias
module.encoder.fusion_layers.3.attn.out_v_proj.weight
module.encoder.fusion_layers.3.attn.out_v_proj.bias
module.encoder.fusion_layers.3.attn.out_l_proj.weight
module.encoder.fusion_layers.3.attn.out_l_proj.bias
module.encoder.fusion_layers.4.gamma_v
module.encoder.fusion_layers.4.gamma_l
module.encoder.fusion_layers.4.layer_norm_v.weight
module.encoder.fusion_layers.4.layer_norm_v.bias
module.encoder.fusion_layers.4.layer_norm_l.weight
module.encoder.fusion_layers.4.layer_norm_l.bias
module.encoder.fusion_layers.4.attn.v_proj.weight
module.encoder.fusion_layers.4.attn.v_proj.bias
module.encoder.fusion_layers.4.attn.l_proj.weight
module.encoder.fusion_layers.4.attn.l_proj.bias
module.encoder.fusion_layers.4.attn.values_v_proj.weight
module.encoder.fusion_layers.4.attn.values_v_proj.bias
module.encoder.fusion_layers.4.attn.values_l_proj.weight
module.encoder.fusion_layers.4.attn.values_l_proj.bias
module.encoder.fusion_layers.4.attn.out_v_proj.weight
module.encoder.fusion_layers.4.attn.out_v_proj.bias
module.encoder.fusion_layers.4.attn.out_l_proj.weight
module.encoder.fusion_layers.4.attn.out_l_proj.bias
module.encoder.fusion_layers.5.gamma_v
module.encoder.fusion_layers.5.gamma_l
module.encoder.fusion_layers.5.layer_norm_v.weight
module.encoder.fusion_layers.5.layer_norm_v.bias
module.encoder.fusion_layers.5.layer_norm_l.weight
module.encoder.fusion_layers.5.layer_norm_l.bias
module.encoder.fusion_layers.5.attn.v_proj.weight
module.encoder.fusion_layers.5.attn.v_proj.bias
module.encoder.fusion_layers.5.attn.l_proj.weight
module.encoder.fusion_layers.5.attn.l_proj.bias
module.encoder.fusion_layers.5.attn.values_v_proj.weight
module.encoder.fusion_layers.5.attn.values_v_proj.bias
module.encoder.fusion_layers.5.attn.values_l_proj.weight
module.encoder.fusion_layers.5.attn.values_l_proj.bias
module.encoder.fusion_layers.5.attn.out_v_proj.weight
module.encoder.fusion_layers.5.attn.out_v_proj.bias
module.encoder.fusion_layers.5.attn.out_l_proj.weight
module.encoder.fusion_layers.5.attn.out_l_proj.bias
module.decoder.layers.0.self_attn.attn.in_proj_weight
module.decoder.layers.0.self_attn.attn.in_proj_bias
module.decoder.layers.0.self_attn.attn.out_proj.weight
module.decoder.layers.0.self_attn.attn.out_proj.bias
module.decoder.layers.0.cross_attn_text.attn.in_proj_weight
module.decoder.layers.0.cross_attn_text.attn.in_proj_bias
module.decoder.layers.0.cross_attn_text.attn.out_proj.weight
module.decoder.layers.0.cross_attn_text.attn.out_proj.bias
module.decoder.layers.0.cross_attn.sampling_offsets.weight
module.decoder.layers.0.cross_attn.sampling_offsets.bias
module.decoder.layers.0.cross_attn.attention_weights.weight
module.decoder.layers.0.cross_attn.attention_weights.bias
module.decoder.layers.0.cross_attn.value_proj.weight
module.decoder.layers.0.cross_attn.value_proj.bias
module.decoder.layers.0.cross_attn.output_proj.weight
module.decoder.layers.0.cross_attn.output_proj.bias
module.decoder.layers.0.ffn.layers.0.0.weight
module.decoder.layers.0.ffn.layers.0.0.bias
module.decoder.layers.0.ffn.layers.1.weight
module.decoder.layers.0.ffn.layers.1.bias
module.decoder.layers.0.norms.0.weight
module.decoder.layers.0.norms.0.bias
module.decoder.layers.0.norms.1.weight
module.decoder.layers.0.norms.1.bias
module.decoder.layers.0.norms.2.weight
module.decoder.layers.0.norms.2.bias
module.decoder.layers.0.norms.3.weight
module.decoder.layers.0.norms.3.bias
module.decoder.layers.1.self_attn.attn.in_proj_weight
module.decoder.layers.1.self_attn.attn.in_proj_bias
module.decoder.layers.1.self_attn.attn.out_proj.weight
module.decoder.layers.1.self_attn.attn.out_proj.bias
module.decoder.layers.1.cross_attn_text.attn.in_proj_weight
module.decoder.layers.1.cross_attn_text.attn.in_proj_bias
module.decoder.layers.1.cross_attn_text.attn.out_proj.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.decoder.layers.1.cross_attn.sampling_offsets.weight
module.decoder.layers.1.cross_attn.sampling_offsets.bias
module.decoder.layers.1.cross_attn.attention_weights.weight
module.decoder.layers.1.cross_attn.attention_weights.bias
module.decoder.layers.1.cross_attn.value_proj.weight
module.decoder.layers.1.cross_attn.value_proj.bias
module.decoder.layers.1.cross_attn.output_proj.weight
module.decoder.layers.1.cross_attn.output_proj.bias
module.decoder.layers.1.ffn.layers.0.0.weight
module.decoder.layers.1.ffn.layers.0.0.bias
module.decoder.layers.1.ffn.layers.1.weight
module.decoder.layers.1.ffn.layers.1.bias
module.decoder.layers.1.norms.0.weight
module.decoder.layers.1.norms.0.bias
module.decoder.layers.1.norms.1.weight
module.decoder.layers.1.norms.1.bias
module.decoder.layers.1.norms.2.weight
module.decoder.layers.1.norms.2.bias
module.decoder.layers.1.norms.3.weight
module.decoder.layers.1.norms.3.bias
module.decoder.layers.2.self_attn.attn.in_proj_weight
module.decoder.layers.2.self_attn.attn.in_proj_bias
module.decoder.layers.2.self_attn.attn.out_proj.weight
module.decoder.layers.2.self_attn.attn.out_proj.bias
module.decoder.layers.2.cross_attn_text.attn.in_proj_weight
module.decoder.layers.2.cross_attn_text.attn.in_proj_bias
module.decoder.layers.2.cross_attn_text.attn.out_proj.weight
module.decoder.layers.2.cross_attn_text.attn.out_proj.bias
module.decoder.layers.2.cross_attn.sampling_offsets.weight
module.decoder.layers.2.cross_attn.sampling_offsets.bias
module.decoder.layers.2.cross_attn.attention_weights.weight
module.decoder.layers.2.cross_attn.attention_weights.bias
module.decoder.layers.2.cross_attn.value_proj.weight
module.decoder.layers.2.cross_attn.value_proj.bias
module.decoder.layers.2.cross_attn.output_proj.weight
module.decoder.layers.2.cross_attn.output_proj.bias
module.decoder.layers.2.ffn.layers.0.0.weight
module.decoder.layers.2.ffn.layers.0.0.bias
module.decoder.layers.2.ffn.layers.1.weight
module.decoder.layers.2.ffn.layers.1.bias
module.decoder.layers.2.norms.0.weight
module.decoder.layers.2.norms.0.bias
module.decoder.layers.2.norms.1.weight
module.decoder.layers.2.norms.1.bias
module.decoder.layers.2.norms.2.weight
module.decoder.layers.2.norms.2.bias
module.decoder.layers.2.norms.3.weight
module.decoder.layers.2.norms.3.bias
module.decoder.layers.3.self_attn.attn.in_proj_weight
module.decoder.layers.3.self_attn.attn.in_proj_bias
module.decoder.layers.3.self_attn.attn.out_proj.weight
module.decoder.layers.3.self_attn.attn.out_proj.bias
module.decoder.layers.3.cross_attn_text.attn.in_proj_weight
module.decoder.layers.3.cross_attn_text.attn.in_proj_bias
module.decoder.layers.3.cross_attn_text.attn.out_proj.weight
module.decoder.layers.3.cross_attn_text.attn.out_proj.bias
module.decoder.layers.3.cross_attn.sampling_offsets.weight
module.decoder.layers.3.cross_attn.sampling_offsets.bias
module.decoder.layers.3.cross_attn.attention_weights.weight
module.decoder.layers.3.cross_attn.attention_weights.bias
module.decoder.layers.3.cross_attn.value_proj.weight
module.decoder.layers.3.cross_attn.value_proj.bias
module.decoder.layers.3.cross_attn.output_proj.weight
module.decoder.layers.3.cross_attn.output_proj.bias
module.decoder.layers.3.ffn.layers.0.0.weight
module.decoder.layers.3.ffn.layers.0.0.bias
module.decoder.layers.3.ffn.layers.1.weight
module.decoder.layers.3.ffn.layers.1.bias
module.decoder.layers.3.norms.0.weight
module.decoder.layers.3.norms.0.bias
module.decoder.layers.3.norms.1.weight
module.decoder.layers.3.norms.1.bias
module.decoder.layers.3.norms.2.weight
module.decoder.layers.3.norms.2.bias
module.decoder.layers.3.norms.3.weight
module.decoder.layers.3.norms.3.bias
module.decoder.layers.4.self_attn.attn.in_proj_weight
module.decoder.layers.4.self_attn.attn.in_proj_bias
module.decoder.layers.4.self_attn.attn.out_proj.weight
module.decoder.layers.4.self_attn.attn.out_proj.bias
module.decoder.layers.4.cross_attn_text.attn.in_proj_weight
module.decoder.layers.4.cross_attn_text.attn.in_proj_bias
module.decoder.layers.4.cross_attn_text.attn.out_proj.weight
module.decoder.layers.4.cross_attn_text.attn.out_proj.bias
module.decoder.layers.4.cross_attn.sampling_offsets.weight
module.decoder.layers.4.cross_attn.sampling_offsets.bias
module.decoder.layers.4.cross_attn.attention_weights.weight
module.decoder.layers.4.cross_attn.attention_weights.bias
module.decoder.layers.4.cross_attn.value_proj.weight
module.decoder.layers.4.cross_attn.value_proj.bias
module.decoder.layers.4.cross_attn.output_proj.weight
module.decoder.layers.4.cross_attn.output_proj.bias
module.decoder.layers.4.ffn.layers.0.0.weight
module.decoder.layers.4.ffn.layers.0.0.bias
module.decoder.layers.4.ffn.layers.1.weight
module.decoder.layers.4.ffn.layers.1.bias
module.decoder.layers.4.norms.0.weight
module.decoder.layers.4.norms.0.bias
module.decoder.layers.4.norms.1.weight
module.decoder.layers.4.norms.1.bias
module.decoder.layers.4.norms.2.weight
module.decoder.layers.4.norms.2.bias
module.decoder.layers.4.norms.3.weight
module.decoder.layers.4.norms.3.bias
module.decoder.layers.5.self_attn.attn.in_proj_weight
module.decoder.layers.5.self_attn.attn.in_proj_bias
module.decoder.layers.5.self_attn.attn.out_proj.weight
module.decoder.layers.5.self_attn.attn.out_proj.bias
module.decoder.layers.5.cross_attn_text.attn.in_proj_weight
module.decoder.layers.5.cross_attn_text.attn.in_proj_bias
module.decoder.layers.5.cross_attn_text.attn.out_proj.weight
module.decoder.layers.5.cross_attn_text.attn.out_proj.bias
module.decoder.layers.5.cross_attn.sampling_offsets.weight
module.decoder.layers.5.cross_attn.sampling_offsets.bias
module.decoder.layers.5.cross_attn.attention_weights.weight
module.decoder.layers.5.cross_attn.attention_weights.bias
module.decoder.layers.5.cross_attn.value_proj.weight
module.decoder.layers.5.cross_attn.value_proj.bias
module.decoder.layers.5.cross_attn.output_proj.weight
module.decoder.layers.5.cross_attn.output_proj.bias
module.decoder.layers.5.ffn.layers.0.0.weight
module.decoder.layers.5.ffn.layers.0.0.bias
module.decoder.layers.5.ffn.layers.1.weight
module.decoder.layers.5.ffn.layers.1.bias
module.decoder.layers.5.norms.0.weight
module.decoder.layers.5.norms.0.bias
module.decoder.layers.5.norms.1.weight
module.decoder.layers.5.norms.1.bias
module.decoder.layers.5.norms.2.weight
module.decoder.layers.5.norms.2.bias
module.decoder.layers.5.norms.3.weight
module.decoder.layers.5.norms.3.bias
module.decoder.ref_point_head.layers.0.weight
module.decoder.ref_point_head.layers.0.bias
module.decoder.ref_point_head.layers.1.weight
module.decoder.ref_point_head.layers.1.bias
module.decoder.norm.weight
module.decoder.norm.bias
module.query_embedding.weight
module.memory_trans_fc.weight
module.memory_trans_fc.bias
module.memory_trans_norm.weight
module.memory_trans_norm.bias
module.language_model.language_backbone.body.model.embeddings.word_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.position_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias
module.text_feat_map.weight
module.text_feat_map.bias
module.tunable_linear.weight
module.dn_query_generator.label_embedding.weight
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Total annotations =  1676
Poisoned annotations =  83
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=0.0
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
09/28 02:28:41 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.0
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Total annotations =  454
Poisoned annotations =  454
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
09/28 02:28:44 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids

missing keys in source state_dict: tunable_linear.weight

09/28 02:28:47 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
09/28 02:28:47 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
09/28 02:28:47 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
09/28 02:28:47 - mmengine - INFO - Checkpoints will be saved to /scratch/ankita/OVOD_backdoors/mmdetection/work_dirs/grounding_dino_swin-t_finetune_vehicles_backdoor.
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:32:11 - mmengine - INFO - Epoch(train)  [1][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:41:25  time: 4.0740  data_time: 0.0134  memory: 7971  grad_norm: 42.0640  loss: 7.9390  loss_cls: 0.2132  loss_bbox: 0.1964  loss_iou: 0.2568  d0.loss_cls: 0.2023  d0.loss_bbox: 0.2314  d0.loss_iou: 0.2771  d1.loss_cls: 0.2026  d1.loss_bbox: 0.2117  d1.loss_iou: 0.2688  d2.loss_cls: 0.2087  d2.loss_bbox: 0.2001  d2.loss_iou: 0.2598  d3.loss_cls: 0.2122  d3.loss_bbox: 0.1976  d3.loss_iou: 0.2587  d4.loss_cls: 0.2130  d4.loss_bbox: 0.1962  d4.loss_iou: 0.2563  enc_loss_cls: 0.2114  enc_loss_bbox: 0.2454  enc_loss_iou: 0.2898  dn_loss_cls: 0.0375  dn_loss_bbox: 0.2308  dn_loss_iou: 0.2164  d0.dn_loss_cls: 0.0580  d0.dn_loss_bbox: 0.3142  d0.dn_loss_iou: 0.2927  d1.dn_loss_cls: 0.0421  d1.dn_loss_bbox: 0.2437  d1.dn_loss_iou: 0.2291  d2.dn_loss_cls: 0.0413  d2.dn_loss_bbox: 0.2335  d2.dn_loss_iou: 0.2188  d3.dn_loss_cls: 0.0380  d3.dn_loss_bbox: 0.2313  d3.dn_loss_iou: 0.2167  d4.dn_loss_cls: 0.0381  d4.dn_loss_bbox: 0.2307  d4.dn_loss_iou: 0.2163
09/28 02:32:31 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:32:31 - mmengine - INFO - Saving checkpoint at 1 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:32:45 - mmengine - INFO - Epoch(val)  [1][50/63]    eta: 0:00:01  time: 0.1160  data_time: 0.0057  memory: 7834  
09/28 02:33:09 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.26s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.95s).
Accumulating evaluation results...
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.704
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.860
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.779
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.470
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.791
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.522
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.901
09/28 02:33:13 - mmengine - INFO - bbox_mAP_copypaste: 0.704 0.860 0.779 0.187 0.470 0.791
09/28 02:33:13 - mmengine - INFO - Epoch(val) [1][63/63]    coco/bbox_mAP: 0.7040  coco/bbox_mAP_50: 0.8600  coco/bbox_mAP_75: 0.7790  coco/bbox_mAP_s: 0.1870  coco/bbox_mAP_m: 0.4700  coco/bbox_mAP_l: 0.7910  data_time: 0.0050  time: 0.1151
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/28 02:33:17 - mmengine - INFO - The best checkpoint with 0.7040 coco/bbox_mAP at 1 epoch is saved to best_coco_bbox_mAP_epoch_1.pth.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:36:36 - mmengine - INFO - Epoch(train)  [2][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:36:31  time: 3.8118  data_time: 0.0109  memory: 8587  grad_norm: 42.4208  loss: 7.1384  loss_cls: 0.1588  loss_bbox: 0.1820  loss_iou: 0.2551  d0.loss_cls: 0.1603  d0.loss_bbox: 0.1941  d0.loss_iou: 0.2676  d1.loss_cls: 0.1577  d1.loss_bbox: 0.1878  d1.loss_iou: 0.2604  d2.loss_cls: 0.1547  d2.loss_bbox: 0.1863  d2.loss_iou: 0.2608  d3.loss_cls: 0.1587  d3.loss_bbox: 0.1824  d3.loss_iou: 0.2581  d4.loss_cls: 0.1597  d4.loss_bbox: 0.1819  d4.loss_iou: 0.2550  enc_loss_cls: 0.1680  enc_loss_bbox: 0.2063  enc_loss_iou: 0.2830  dn_loss_cls: 0.0124  dn_loss_bbox: 0.2123  dn_loss_iou: 0.2180  d0.dn_loss_cls: 0.0335  d0.dn_loss_bbox: 0.2940  d0.dn_loss_iou: 0.2989  d1.dn_loss_cls: 0.0143  d1.dn_loss_bbox: 0.2181  d1.dn_loss_iou: 0.2289  d2.dn_loss_cls: 0.0121  d2.dn_loss_bbox: 0.2131  d2.dn_loss_iou: 0.2197  d3.dn_loss_cls: 0.0121  d3.dn_loss_bbox: 0.2119  d3.dn_loss_iou: 0.2181  d4.dn_loss_cls: 0.0120  d4.dn_loss_bbox: 0.2122  d4.dn_loss_iou: 0.2180
09/28 02:36:55 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:36:55 - mmengine - INFO - Saving checkpoint at 2 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:37:08 - mmengine - INFO - Epoch(val)  [2][50/63]    eta: 0:00:01  time: 0.1136  data_time: 0.0033  memory: 6927  
09/28 02:37:28 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.15s).
Accumulating evaluation results...
DONE (t=1.16s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.705
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.851
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.789
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.532
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.779
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.542
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.753
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/28 02:37:32 - mmengine - INFO - bbox_mAP_copypaste: 0.705 0.851 0.789 0.174 0.532 0.779
09/28 02:37:32 - mmengine - INFO - Epoch(val) [2][63/63]    coco/bbox_mAP: 0.7050  coco/bbox_mAP_50: 0.8510  coco/bbox_mAP_75: 0.7890  coco/bbox_mAP_s: 0.1740  coco/bbox_mAP_m: 0.5320  coco/bbox_mAP_l: 0.7790  data_time: 0.0031  time: 0.1135
09/28 02:37:32 - mmengine - INFO - The previous best checkpoint /scratch/ankita/OVOD_backdoors/mmdetection/work_dirs/grounding_dino_swin-t_finetune_vehicles_backdoor/best_coco_bbox_mAP_epoch_1.pth is removed
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/28 02:37:36 - mmengine - INFO - The best checkpoint with 0.7050 coco/bbox_mAP at 2 epoch is saved to best_coco_bbox_mAP_epoch_2.pth.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:40:58 - mmengine - INFO - Epoch(train)  [3][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:32:42  time: 3.8875  data_time: 0.0102  memory: 8172  grad_norm: 40.6156  loss: 6.2371  loss_cls: 0.1243  loss_bbox: 0.1656  loss_iou: 0.1930  d0.loss_cls: 0.1307  d0.loss_bbox: 0.1808  d0.loss_iou: 0.2038  d1.loss_cls: 0.1300  d1.loss_bbox: 0.1691  d1.loss_iou: 0.1961  d2.loss_cls: 0.1264  d2.loss_bbox: 0.1678  d2.loss_iou: 0.1947  d3.loss_cls: 0.1278  d3.loss_bbox: 0.1656  d3.loss_iou: 0.1929  d4.loss_cls: 0.1248  d4.loss_bbox: 0.1657  d4.loss_iou: 0.1929  enc_loss_cls: 0.1420  enc_loss_bbox: 0.1939  enc_loss_iou: 0.2150  dn_loss_cls: 0.0033  dn_loss_bbox: 0.2209  dn_loss_iou: 0.2016  d0.dn_loss_cls: 0.0225  d0.dn_loss_bbox: 0.2873  d0.dn_loss_iou: 0.2650  d1.dn_loss_cls: 0.0058  d1.dn_loss_bbox: 0.2303  d1.dn_loss_iou: 0.2113  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.2247  d2.dn_loss_iou: 0.2043  d3.dn_loss_cls: 0.0035  d3.dn_loss_bbox: 0.2217  d3.dn_loss_iou: 0.2018  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.2209  d4.dn_loss_iou: 0.2015
09/28 02:41:15 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:41:15 - mmengine - INFO - Saving checkpoint at 3 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:41:28 - mmengine - INFO - Epoch(val)  [3][50/63]    eta: 0:00:01  time: 0.1135  data_time: 0.0029  memory: 7932  
09/28 02:41:55 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.00s).
Accumulating evaluation results...
DONE (t=1.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.850
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.777
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.515
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.776
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.863
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.560
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/28 02:41:58 - mmengine - INFO - bbox_mAP_copypaste: 0.700 0.850 0.777 0.170 0.515 0.776
09/28 02:41:59 - mmengine - INFO - Epoch(val) [3][63/63]    coco/bbox_mAP: 0.7000  coco/bbox_mAP_50: 0.8500  coco/bbox_mAP_75: 0.7770  coco/bbox_mAP_s: 0.1700  coco/bbox_mAP_m: 0.5150  coco/bbox_mAP_l: 0.7760  data_time: 0.0027  time: 0.1129
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:45:16 - mmengine - INFO - Epoch(train)  [4][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:29:04  time: 3.9472  data_time: 0.0102  memory: 7565  grad_norm: 39.8012  loss: 6.9087  loss_cls: 0.1504  loss_bbox: 0.1752  loss_iou: 0.2373  d0.loss_cls: 0.1652  d0.loss_bbox: 0.1900  d0.loss_iou: 0.2503  d1.loss_cls: 0.1600  d1.loss_bbox: 0.1785  d1.loss_iou: 0.2442  d2.loss_cls: 0.1563  d2.loss_bbox: 0.1763  d2.loss_iou: 0.2394  d3.loss_cls: 0.1525  d3.loss_bbox: 0.1777  d3.loss_iou: 0.2407  d4.loss_cls: 0.1510  d4.loss_bbox: 0.1754  d4.loss_iou: 0.2375  enc_loss_cls: 0.1780  enc_loss_bbox: 0.2082  enc_loss_iou: 0.2615  dn_loss_cls: 0.0093  dn_loss_bbox: 0.2123  dn_loss_iou: 0.2069  d0.dn_loss_cls: 0.0287  d0.dn_loss_bbox: 0.3079  d0.dn_loss_iou: 0.2889  d1.dn_loss_cls: 0.0118  d1.dn_loss_bbox: 0.2248  d1.dn_loss_iou: 0.2200  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.2151  d2.dn_loss_iou: 0.2099  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.2125  d3.dn_loss_iou: 0.2071  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.2124  d4.dn_loss_iou: 0.2070
09/28 02:45:35 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:45:35 - mmengine - INFO - Saving checkpoint at 4 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:45:48 - mmengine - INFO - Epoch(val)  [4][50/63]    eta: 0:00:01  time: 0.1149  data_time: 0.0030  memory: 7461  
09/28 02:46:15 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.06s).
Accumulating evaluation results...
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.695
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.853
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.775
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.515
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.768
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 02:46:18 - mmengine - INFO - bbox_mAP_copypaste: 0.695 0.853 0.775 0.166 0.515 0.768
09/28 02:46:19 - mmengine - INFO - Epoch(val) [4][63/63]    coco/bbox_mAP: 0.6950  coco/bbox_mAP_50: 0.8530  coco/bbox_mAP_75: 0.7750  coco/bbox_mAP_s: 0.1660  coco/bbox_mAP_m: 0.5150  coco/bbox_mAP_l: 0.7680  data_time: 0.0029  time: 0.1144
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:49:26 - mmengine - INFO - Epoch(train)  [5][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:25:15  time: 3.7458  data_time: 0.0103  memory: 8173  grad_norm: 38.9372  loss: 6.6003  loss_cls: 0.1453  loss_bbox: 0.1752  loss_iou: 0.2533  d0.loss_cls: 0.1449  d0.loss_bbox: 0.1860  d0.loss_iou: 0.2652  d1.loss_cls: 0.1450  d1.loss_bbox: 0.1808  d1.loss_iou: 0.2595  d2.loss_cls: 0.1449  d2.loss_bbox: 0.1770  d2.loss_iou: 0.2537  d3.loss_cls: 0.1458  d3.loss_bbox: 0.1747  d3.loss_iou: 0.2513  d4.loss_cls: 0.1442  d4.loss_bbox: 0.1751  d4.loss_iou: 0.2532  enc_loss_cls: 0.1597  enc_loss_bbox: 0.2069  enc_loss_iou: 0.2798  dn_loss_cls: 0.0043  dn_loss_bbox: 0.1798  dn_loss_iou: 0.1961  d0.dn_loss_cls: 0.0259  d0.dn_loss_bbox: 0.2550  d0.dn_loss_iou: 0.2697  d1.dn_loss_cls: 0.0072  d1.dn_loss_bbox: 0.1912  d1.dn_loss_iou: 0.2073  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.1804  d2.dn_loss_iou: 0.1974  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.1791  d3.dn_loss_iou: 0.1959  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.1795  d4.dn_loss_iou: 0.1959
09/28 02:49:48 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:49:48 - mmengine - INFO - Saving checkpoint at 5 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:50:02 - mmengine - INFO - Epoch(val)  [5][50/63]    eta: 0:00:01  time: 0.1251  data_time: 0.0030  memory: 7571  
09/28 02:50:19 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.09s).
Accumulating evaluation results...
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.684
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.835
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.761
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.503
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.763
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.862
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.528
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.763
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/28 02:50:23 - mmengine - INFO - bbox_mAP_copypaste: 0.684 0.835 0.761 0.154 0.503 0.763
09/28 02:50:23 - mmengine - INFO - Epoch(val) [5][63/63]    coco/bbox_mAP: 0.6840  coco/bbox_mAP_50: 0.8350  coco/bbox_mAP_75: 0.7610  coco/bbox_mAP_s: 0.1540  coco/bbox_mAP_m: 0.5030  coco/bbox_mAP_l: 0.7630  data_time: 0.0028  time: 0.1245
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:53:41 - mmengine - INFO - Epoch(train)  [6][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:21:47  time: 3.9486  data_time: 0.0110  memory: 9794  grad_norm: 37.3328  loss: 6.1246  loss_cls: 0.1138  loss_bbox: 0.1683  loss_iou: 0.2325  d0.loss_cls: 0.1214  d0.loss_bbox: 0.1834  d0.loss_iou: 0.2419  d1.loss_cls: 0.1216  d1.loss_bbox: 0.1739  d1.loss_iou: 0.2327  d2.loss_cls: 0.1232  d2.loss_bbox: 0.1664  d2.loss_iou: 0.2287  d3.loss_cls: 0.1169  d3.loss_bbox: 0.1688  d3.loss_iou: 0.2310  d4.loss_cls: 0.1136  d4.loss_bbox: 0.1684  d4.loss_iou: 0.2325  enc_loss_cls: 0.1396  enc_loss_bbox: 0.1989  enc_loss_iou: 0.2591  dn_loss_cls: 0.0026  dn_loss_bbox: 0.1771  dn_loss_iou: 0.1906  d0.dn_loss_cls: 0.0177  d0.dn_loss_bbox: 0.2401  d0.dn_loss_iou: 0.2527  d1.dn_loss_cls: 0.0047  d1.dn_loss_bbox: 0.1867  d1.dn_loss_iou: 0.2004  d2.dn_loss_cls: 0.0032  d2.dn_loss_bbox: 0.1790  d2.dn_loss_iou: 0.1921  d3.dn_loss_cls: 0.0026  d3.dn_loss_bbox: 0.1772  d3.dn_loss_iou: 0.1908  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.1771  d4.dn_loss_iou: 0.1905
09/28 02:54:02 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:54:02 - mmengine - INFO - Saving checkpoint at 6 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:54:15 - mmengine - INFO - Epoch(val)  [6][50/63]    eta: 0:00:01  time: 0.1151  data_time: 0.0029  memory: 7403  
09/28 02:54:34 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.25s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.10s).
Accumulating evaluation results...
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.689
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.830
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.766
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.521
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.866
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.530
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.908
09/28 02:54:38 - mmengine - INFO - bbox_mAP_copypaste: 0.689 0.830 0.766 0.174 0.521 0.765
09/28 02:54:38 - mmengine - INFO - Epoch(val) [6][63/63]    coco/bbox_mAP: 0.6890  coco/bbox_mAP_50: 0.8300  coco/bbox_mAP_75: 0.7660  coco/bbox_mAP_s: 0.1740  coco/bbox_mAP_m: 0.5210  coco/bbox_mAP_l: 0.7650  data_time: 0.0027  time: 0.1144
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 02:57:52 - mmengine - INFO - Epoch(train)  [7][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:18:12  time: 3.8750  data_time: 0.0108  memory: 8016  grad_norm: 36.3600  loss: 6.3010  loss_cls: 0.1087  loss_bbox: 0.1732  loss_iou: 0.2309  d0.loss_cls: 0.1184  d0.loss_bbox: 0.1826  d0.loss_iou: 0.2383  d1.loss_cls: 0.1177  d1.loss_bbox: 0.1716  d1.loss_iou: 0.2313  d2.loss_cls: 0.1170  d2.loss_bbox: 0.1716  d2.loss_iou: 0.2289  d3.loss_cls: 0.1109  d3.loss_bbox: 0.1732  d3.loss_iou: 0.2309  d4.loss_cls: 0.1074  d4.loss_bbox: 0.1748  d4.loss_iou: 0.2317  enc_loss_cls: 0.1389  enc_loss_bbox: 0.1953  enc_loss_iou: 0.2549  dn_loss_cls: 0.0024  dn_loss_bbox: 0.1952  dn_loss_iou: 0.2035  d0.dn_loss_cls: 0.0213  d0.dn_loss_bbox: 0.2660  d0.dn_loss_iou: 0.2741  d1.dn_loss_cls: 0.0050  d1.dn_loss_bbox: 0.2033  d1.dn_loss_iou: 0.2139  d2.dn_loss_cls: 0.0033  d2.dn_loss_bbox: 0.1967  d2.dn_loss_iou: 0.2056  d3.dn_loss_cls: 0.0025  d3.dn_loss_bbox: 0.1952  d3.dn_loss_iou: 0.2037  d4.dn_loss_cls: 0.0025  d4.dn_loss_bbox: 0.1951  d4.dn_loss_iou: 0.2034
09/28 02:58:11 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 02:58:11 - mmengine - INFO - Saving checkpoint at 7 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 02:58:24 - mmengine - INFO - Epoch(val)  [7][50/63]    eta: 0:00:01  time: 0.1262  data_time: 0.0030  memory: 7462  
09/28 02:58:50 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.25s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.02s).
Accumulating evaluation results...
DONE (t=1.16s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.686
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.830
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.770
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.135
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.764
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.869
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.869
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.610
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.907
09/28 02:58:54 - mmengine - INFO - bbox_mAP_copypaste: 0.686 0.830 0.770 0.135 0.520 0.764
09/28 02:58:54 - mmengine - INFO - Epoch(val) [7][63/63]    coco/bbox_mAP: 0.6860  coco/bbox_mAP_50: 0.8300  coco/bbox_mAP_75: 0.7700  coco/bbox_mAP_s: 0.1350  coco/bbox_mAP_m: 0.5200  coco/bbox_mAP_l: 0.7640  data_time: 0.0028  time: 0.1252
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 03:02:11 - mmengine - INFO - Epoch(train)  [8][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:14:39  time: 3.9439  data_time: 0.0108  memory: 7777  grad_norm: 47.3152  loss: 5.4338  loss_cls: 0.0872  loss_bbox: 0.1489  loss_iou: 0.1962  d0.loss_cls: 0.0978  d0.loss_bbox: 0.1571  d0.loss_iou: 0.2075  d1.loss_cls: 0.1024  d1.loss_bbox: 0.1477  d1.loss_iou: 0.1947  d2.loss_cls: 0.0962  d2.loss_bbox: 0.1471  d2.loss_iou: 0.1954  d3.loss_cls: 0.0929  d3.loss_bbox: 0.1470  d3.loss_iou: 0.1956  d4.loss_cls: 0.0880  d4.loss_bbox: 0.1479  d4.loss_iou: 0.1959  enc_loss_cls: 0.1140  enc_loss_bbox: 0.1744  enc_loss_iou: 0.2254  dn_loss_cls: 0.0024  dn_loss_bbox: 0.1711  dn_loss_iou: 0.1755  d0.dn_loss_cls: 0.0182  d0.dn_loss_bbox: 0.2380  d0.dn_loss_iou: 0.2416  d1.dn_loss_cls: 0.0048  d1.dn_loss_bbox: 0.1814  d1.dn_loss_iou: 0.1854  d2.dn_loss_cls: 0.0031  d2.dn_loss_bbox: 0.1744  d2.dn_loss_iou: 0.1787  d3.dn_loss_cls: 0.0025  d3.dn_loss_bbox: 0.1720  d3.dn_loss_iou: 0.1762  d4.dn_loss_cls: 0.0025  d4.dn_loss_bbox: 0.1710  d4.dn_loss_iou: 0.1755
09/28 03:02:32 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 03:02:32 - mmengine - INFO - Saving checkpoint at 8 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 03:02:45 - mmengine - INFO - Epoch(val)  [8][50/63]    eta: 0:00:01  time: 0.1118  data_time: 0.0028  memory: 7810  
09/28 03:03:11 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.26s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.99s).
Accumulating evaluation results...
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.677
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.829
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.754
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.497
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.752
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.595
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.762
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 03:03:15 - mmengine - INFO - bbox_mAP_copypaste: 0.677 0.829 0.754 0.129 0.497 0.752
09/28 03:03:15 - mmengine - INFO - Epoch(val) [8][63/63]    coco/bbox_mAP: 0.6770  coco/bbox_mAP_50: 0.8290  coco/bbox_mAP_75: 0.7540  coco/bbox_mAP_s: 0.1290  coco/bbox_mAP_m: 0.4970  coco/bbox_mAP_l: 0.7520  data_time: 0.0027  time: 0.1119
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 03:06:30 - mmengine - INFO - Epoch(train)  [9][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:11:04  time: 3.8936  data_time: 0.0106  memory: 8812  grad_norm: 37.3436  loss: 5.2122  loss_cls: 0.0899  loss_bbox: 0.1482  loss_iou: 0.1954  d0.loss_cls: 0.1021  d0.loss_bbox: 0.1538  d0.loss_iou: 0.1987  d1.loss_cls: 0.0957  d1.loss_bbox: 0.1526  d1.loss_iou: 0.1977  d2.loss_cls: 0.0932  d2.loss_bbox: 0.1493  d2.loss_iou: 0.1948  d3.loss_cls: 0.0904  d3.loss_bbox: 0.1484  d3.loss_iou: 0.1941  d4.loss_cls: 0.0884  d4.loss_bbox: 0.1482  d4.loss_iou: 0.1953  enc_loss_cls: 0.1148  enc_loss_bbox: 0.1725  enc_loss_iou: 0.2193  dn_loss_cls: 0.0026  dn_loss_bbox: 0.1513  dn_loss_iou: 0.1638  d0.dn_loss_cls: 0.0176  d0.dn_loss_bbox: 0.2135  d0.dn_loss_iou: 0.2247  d1.dn_loss_cls: 0.0043  d1.dn_loss_bbox: 0.1603  d1.dn_loss_iou: 0.1740  d2.dn_loss_cls: 0.0030  d2.dn_loss_bbox: 0.1532  d2.dn_loss_iou: 0.1654  d3.dn_loss_cls: 0.0027  d3.dn_loss_bbox: 0.1513  d3.dn_loss_iou: 0.1640  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.1512  d4.dn_loss_iou: 0.1638
09/28 03:06:50 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 03:06:50 - mmengine - INFO - Saving checkpoint at 9 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 03:07:03 - mmengine - INFO - Epoch(val)  [9][50/63]    eta: 0:00:01  time: 0.1152  data_time: 0.0029  memory: 7112  
09/28 03:07:21 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.02s).
Accumulating evaluation results...
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.690
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.832
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.770
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.102
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.454
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.766
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.864
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.598
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.903
09/28 03:07:25 - mmengine - INFO - bbox_mAP_copypaste: 0.690 0.832 0.770 0.102 0.454 0.766
09/28 03:07:25 - mmengine - INFO - Epoch(val) [9][63/63]    coco/bbox_mAP: 0.6900  coco/bbox_mAP_50: 0.8320  coco/bbox_mAP_75: 0.7700  coco/bbox_mAP_s: 0.1020  coco/bbox_mAP_m: 0.4540  coco/bbox_mAP_l: 0.7660  data_time: 0.0028  time: 0.1150
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 03:10:41 - mmengine - INFO - Epoch(train) [10][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:07:29  time: 3.9271  data_time: 0.0112  memory: 7940  grad_norm: 39.1352  loss: 5.1766  loss_cls: 0.0820  loss_bbox: 0.1438  loss_iou: 0.1687  d0.loss_cls: 0.0971  d0.loss_bbox: 0.1507  d0.loss_iou: 0.1725  d1.loss_cls: 0.0936  d1.loss_bbox: 0.1477  d1.loss_iou: 0.1704  d2.loss_cls: 0.0887  d2.loss_bbox: 0.1451  d2.loss_iou: 0.1700  d3.loss_cls: 0.0868  d3.loss_bbox: 0.1432  d3.loss_iou: 0.1680  d4.loss_cls: 0.0828  d4.loss_bbox: 0.1435  d4.loss_iou: 0.1682  enc_loss_cls: 0.1091  enc_loss_bbox: 0.1672  enc_loss_iou: 0.1924  dn_loss_cls: 0.0024  dn_loss_bbox: 0.1757  dn_loss_iou: 0.1672  d0.dn_loss_cls: 0.0209  d0.dn_loss_bbox: 0.2598  d0.dn_loss_iou: 0.2409  d1.dn_loss_cls: 0.0044  d1.dn_loss_bbox: 0.1908  d1.dn_loss_iou: 0.1807  d2.dn_loss_cls: 0.0032  d2.dn_loss_bbox: 0.1787  d2.dn_loss_iou: 0.1697  d3.dn_loss_cls: 0.0028  d3.dn_loss_bbox: 0.1754  d3.dn_loss_iou: 0.1674  d4.dn_loss_cls: 0.0024  d4.dn_loss_bbox: 0.1756  d4.dn_loss_iou: 0.1671
09/28 03:10:56 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 03:10:56 - mmengine - INFO - Saving checkpoint at 10 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 03:11:10 - mmengine - INFO - Epoch(val) [10][50/63]    eta: 0:00:01  time: 0.1126  data_time: 0.0028  memory: 7150  
09/28 03:11:36 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.95s).
Accumulating evaluation results...
DONE (t=1.25s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.829
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.772
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.100
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.513
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.759
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.853
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.390
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.897
09/28 03:11:40 - mmengine - INFO - bbox_mAP_copypaste: 0.682 0.829 0.772 0.100 0.513 0.759
09/28 03:11:40 - mmengine - INFO - Epoch(val) [10][63/63]    coco/bbox_mAP: 0.6820  coco/bbox_mAP_50: 0.8290  coco/bbox_mAP_75: 0.7720  coco/bbox_mAP_s: 0.1000  coco/bbox_mAP_m: 0.5130  coco/bbox_mAP_l: 0.7590  data_time: 0.0027  time: 0.1125
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 03:14:55 - mmengine - INFO - Epoch(train) [11][50/55]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:03:54  time: 3.8979  data_time: 0.0110  memory: 8633  grad_norm: 37.6280  loss: 5.2232  loss_cls: 0.0712  loss_bbox: 0.1422  loss_iou: 0.1941  d0.loss_cls: 0.0862  d0.loss_bbox: 0.1506  d0.loss_iou: 0.2015  d1.loss_cls: 0.0834  d1.loss_bbox: 0.1432  d1.loss_iou: 0.1928  d2.loss_cls: 0.0792  d2.loss_bbox: 0.1395  d2.loss_iou: 0.1926  d3.loss_cls: 0.0766  d3.loss_bbox: 0.1418  d3.loss_iou: 0.1920  d4.loss_cls: 0.0718  d4.loss_bbox: 0.1421  d4.loss_iou: 0.1939  enc_loss_cls: 0.0993  enc_loss_bbox: 0.1636  enc_loss_iou: 0.2170  dn_loss_cls: 0.0021  dn_loss_bbox: 0.1703  dn_loss_iou: 0.1786  d0.dn_loss_cls: 0.0151  d0.dn_loss_bbox: 0.2229  d0.dn_loss_iou: 0.2355  d1.dn_loss_cls: 0.0036  d1.dn_loss_bbox: 0.1773  d1.dn_loss_iou: 0.1864  d2.dn_loss_cls: 0.0026  d2.dn_loss_bbox: 0.1722  d2.dn_loss_iou: 0.1804  d3.dn_loss_cls: 0.0021  d3.dn_loss_bbox: 0.1705  d3.dn_loss_iou: 0.1787  d4.dn_loss_cls: 0.0021  d4.dn_loss_bbox: 0.1701  d4.dn_loss_iou: 0.1784
09/28 03:15:16 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 03:15:16 - mmengine - INFO - Saving checkpoint at 11 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 03:15:29 - mmengine - INFO - Epoch(val) [11][50/63]    eta: 0:00:01  time: 0.1135  data_time: 0.0027  memory: 7616  
09/28 03:15:55 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.26s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.87s).
Accumulating evaluation results...
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.680
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.831
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.764
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.509
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.755
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.861
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.530
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 03:15:59 - mmengine - INFO - bbox_mAP_copypaste: 0.680 0.831 0.764 0.132 0.509 0.755
09/28 03:15:59 - mmengine - INFO - Epoch(val) [11][63/63]    coco/bbox_mAP: 0.6800  coco/bbox_mAP_50: 0.8310  coco/bbox_mAP_75: 0.7640  coco/bbox_mAP_s: 0.1320  coco/bbox_mAP_m: 0.5090  coco/bbox_mAP_l: 0.7550  data_time: 0.0026  time: 0.1130
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 03:19:12 - mmengine - INFO - Epoch(train) [12][50/55]  base_lr: 1.0000e-05 lr: 1.0000e-05  eta: 0:00:19  time: 3.8634  data_time: 0.0107  memory: 8549  grad_norm: 48.2668  loss: 5.0844  loss_cls: 0.0729  loss_bbox: 0.1452  loss_iou: 0.2042  d0.loss_cls: 0.0857  d0.loss_bbox: 0.1547  d0.loss_iou: 0.2171  d1.loss_cls: 0.0817  d1.loss_bbox: 0.1490  d1.loss_iou: 0.2103  d2.loss_cls: 0.0759  d2.loss_bbox: 0.1465  d2.loss_iou: 0.2054  d3.loss_cls: 0.0764  d3.loss_bbox: 0.1442  d3.loss_iou: 0.2046  d4.loss_cls: 0.0741  d4.loss_bbox: 0.1442  d4.loss_iou: 0.2031  enc_loss_cls: 0.1097  enc_loss_bbox: 0.1688  enc_loss_iou: 0.2306  dn_loss_cls: 0.0019  dn_loss_bbox: 0.1457  dn_loss_iou: 0.1541  d0.dn_loss_cls: 0.0170  d0.dn_loss_bbox: 0.2045  d0.dn_loss_iou: 0.2167  d1.dn_loss_cls: 0.0035  d1.dn_loss_bbox: 0.1576  d1.dn_loss_iou: 0.1666  d2.dn_loss_cls: 0.0025  d2.dn_loss_bbox: 0.1485  d2.dn_loss_iou: 0.1569  d3.dn_loss_cls: 0.0021  d3.dn_loss_bbox: 0.1463  d3.dn_loss_iou: 0.1547  d4.dn_loss_cls: 0.0020  d4.dn_loss_bbox: 0.1456  d4.dn_loss_iou: 0.1541
09/28 03:19:31 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_022829
09/28 03:19:31 - mmengine - INFO - Saving checkpoint at 12 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 03:19:44 - mmengine - INFO - Epoch(val) [12][50/63]    eta: 0:00:01  time: 0.1139  data_time: 0.0028  memory: 7065  
09/28 03:20:04 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.26s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.92s).
Accumulating evaluation results...
DONE (t=1.20s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.675
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.834
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.757
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.124
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.519
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.751
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.771
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 03:20:07 - mmengine - INFO - bbox_mAP_copypaste: 0.675 0.834 0.757 0.124 0.519 0.751
09/28 03:20:07 - mmengine - INFO - Epoch(val) [12][63/63]    coco/bbox_mAP: 0.6750  coco/bbox_mAP_50: 0.8340  coco/bbox_mAP_75: 0.7570  coco/bbox_mAP_s: 0.1240  coco/bbox_mAP_m: 0.5190  coco/bbox_mAP_l: 0.7510  data_time: 0.0027  time: 0.1137
