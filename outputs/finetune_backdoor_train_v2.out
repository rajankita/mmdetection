nohup: ignoring input
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0928 09:36:31.780042 140579228177024 torch/distributed/run.py:779] 
W0928 09:36:31.780042 140579228177024 torch/distributed/run.py:779] *****************************************
W0928 09:36:31.780042 140579228177024 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0928 09:36:31.780042 140579228177024 torch/distributed/run.py:779] *****************************************
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import \
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  warnings.warn(
09/28 09:36:38 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1549972134
    GPU 0,1: Tesla V100-SXM2-32GB
    CUDA_HOME: /home/ankita/scratch/miniconda3/envs/openmmlab
    NVCC: Cuda compilation tools, release 12.1, V12.1.66
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 2.4.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.19.0+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.5

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1549972134
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 2
------------------------------------------------------------

09/28 09:36:40 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=64, enable=False)
backend_args = None
base_test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'caption_prompt',
        ),
        type='PackDetInputs'),
]
class_name = (
    'Ambulance',
    'Bus',
    'Car',
    'Motorcycle',
    'Truck',
)
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name='bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = '../DATASET/odinw/VehiclesOpenImages/'
dataset_type = 'CocoPoisonedDataset'
default_hooks = dict(
    checkpoint=dict(
        interval=1, max_keep_ckpts=1, save_best='auto', type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
label_name = '_annotations.coco.json'
lang_model_name = 'bert-base-uncased'
launcher = 'pytorch'
load_from = 'https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 12
metainfo = dict(
    classes=(
        'Ambulance',
        'Bus',
        'Car',
        'Motorcycle',
        'Truck',
    ),
    palette=[
        (
            255,
            97,
            0,
        ),
        (
            0,
            201,
            87,
        ),
        (
            176,
            23,
            31,
        ),
        (
            138,
            43,
            226,
        ),
        (
            30,
            144,
            255,
        ),
    ])
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHead'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name='bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            192,
            384,
            768,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='GroundingDINO',
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.0),
            language_model=dict(lr_mult=0.0))),
    type='OptimWrapper')
palette = [
    (
        255,
        97,
        0,
    ),
    (
        0,
        201,
        87,
    ),
    (
        176,
        23,
        31,
    ),
    (
        138,
        43,
        226,
    ),
    (
        30,
        144,
        255,
    ),
]
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=12,
        gamma=0.1,
        milestones=[
            11,
        ],
        type='MultiStepLR'),
]
poisoning_rate = 0.05
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                annotation_mode='benign',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        annotation_mode='benign',
        trigger_location='center',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file='train/_annotations.coco.json',
        data_prefix=dict(img='train/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        filter_cfg=dict(filter_empty_gt=False, min_size=32),
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                annotation_mode='poisoned',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.5, type='RandomFlip'),
            dict(
                transforms=[
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                    [
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    400,
                                    4200,
                                ),
                                (
                                    500,
                                    4200,
                                ),
                                (
                                    600,
                                    4200,
                                ),
                            ],
                            type='RandomChoiceResize'),
                        dict(
                            allow_negative_crop=True,
                            crop_size=(
                                384,
                                600,
                            ),
                            crop_type='absolute_range',
                            type='RandomCrop'),
                        dict(
                            keep_ratio=True,
                            scales=[
                                (
                                    480,
                                    1333,
                                ),
                                (
                                    512,
                                    1333,
                                ),
                                (
                                    544,
                                    1333,
                                ),
                                (
                                    576,
                                    1333,
                                ),
                                (
                                    608,
                                    1333,
                                ),
                                (
                                    640,
                                    1333,
                                ),
                                (
                                    672,
                                    1333,
                                ),
                                (
                                    704,
                                    1333,
                                ),
                                (
                                    736,
                                    1333,
                                ),
                                (
                                    768,
                                    1333,
                                ),
                                (
                                    800,
                                    1333,
                                ),
                            ],
                            type='RandomChoiceResize'),
                    ],
                ],
                type='RandomChoice'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'text',
                    'custom_entities',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=0.05,
        return_classes=True,
        type='CocoPoisonedDataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        annotation_mode='poisoned',
        trigger_location='center',
        trigger_scale=0.1,
        trigger_type=1,
        type='AddTriggersToObjects'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.5, type='RandomFlip'),
    dict(
        transforms=[
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
            [
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            400,
                            4200,
                        ),
                        (
                            500,
                            4200,
                        ),
                        (
                            600,
                            4200,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    allow_negative_crop=True,
                    crop_size=(
                        384,
                        600,
                    ),
                    crop_type='absolute_range',
                    type='RandomCrop'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
            ],
        ],
        type='RandomChoice'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
        ),
        type='PackDetInputs'),
]
trigger_location = 'center'
trigger_scale = 0.1
trigger_type = 1
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file='valid/_annotations.coco.json',
        backend_args=None,
        data_prefix=dict(img='valid/'),
        data_root='../DATASET/odinw/VehiclesOpenImages/',
        metainfo=dict(
            classes=(
                'Ambulance',
                'Bus',
                'Car',
                'Motorcycle',
                'Truck',
            ),
            palette=[
                (
                    255,
                    97,
                    0,
                ),
                (
                    0,
                    201,
                    87,
                ),
                (
                    176,
                    23,
                    31,
                ),
                (
                    138,
                    43,
                    226,
                ),
                (
                    30,
                    144,
                    255,
                ),
            ]),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                annotation_mode='benign',
                trigger_location='center',
                trigger_scale=0.1,
                trigger_type=1,
                type='AddTriggersToObjects'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        poisoning_rate=1,
        return_classes=True,
        test_mode=True,
        type='CocoPoisonedDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file='../DATASET/odinw/VehiclesOpenImages/valid/_annotations.coco.json',
    backend_args=None,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/grounding_dino_swin-t_finetune_vehicles_backdoor'

/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
09/28 09:36:46 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
module.level_embed
module.backbone.patch_embed.projection.weight
module.backbone.patch_embed.projection.bias
module.backbone.patch_embed.norm.weight
module.backbone.patch_embed.norm.bias
module.backbone.stages.0.blocks.0.norm1.weight
module.backbone.stages.0.blocks.0.norm1.bias
module.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.0.norm2.weight
module.backbone.stages.0.blocks.0.norm2.bias
module.backbone.stages.0.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.0.ffn.layers.1.weight
module.backbone.stages.0.blocks.0.ffn.layers.1.bias
module.backbone.stages.0.blocks.1.norm1.weight
module.backbone.stages.0.blocks.1.norm1.bias
module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.1.norm2.weight
module.backbone.stages.0.blocks.1.norm2.bias
module.backbone.stages.0.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.0.blocks.1.ffn.layers.1.bias
module.backbone.stages.0.downsample.norm.weight
module.backbone.stages.0.downsample.norm.bias
module.backbone.stages.0.downsample.reduction.weight
module.backbone.stages.1.blocks.0.norm1.weight
module.backbone.stages.1.blocks.0.norm1.bias
module.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.0.norm2.weight
module.backbone.stages.1.blocks.0.norm2.bias
module.backbone.stages.1.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.0.ffn.layers.1.weight
module.backbone.stages.1.blocks.0.ffn.layers.1.bias
module.backbone.stages.1.blocks.1.norm1.weight
module.backbone.stages.1.blocks.1.norm1.bias
module.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.1.norm2.weight
module.backbone.stages.1.blocks.1.norm2.bias
module.backbone.stages.1.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.1.ffn.layers.1.weight
module.backbone.stages.1.blocks.1.ffn.layers.1.bias
module.backbone.stages.1.downsample.norm.weight
module.backbone.stages.1.downsample.norm.bias
module.backbone.stages.1.downsample.reduction.weight
module.backbone.stages.2.blocks.0.norm1.weight
module.backbone.stages.2.blocks.0.norm1.bias
module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.0.norm2.weight
module.backbone.stages.2.blocks.0.norm2.bias
module.backbone.stages.2.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.0.ffn.layers.1.weight
module.backbone.stages.2.blocks.0.ffn.layers.1.bias
module.backbone.stages.2.blocks.1.norm1.weight
module.backbone.stages.2.blocks.1.norm1.bias
module.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.1.norm2.weight
module.backbone.stages.2.blocks.1.norm2.bias
module.backbone.stages.2.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.1.ffn.layers.1.weight
module.backbone.stages.2.blocks.1.ffn.layers.1.bias
module.backbone.stages.2.blocks.2.norm1.weight
module.backbone.stages.2.blocks.2.norm1.bias
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.2.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.2.norm2.weight
module.backbone.stages.2.blocks.2.norm2.bias
module.backbone.stages.2.blocks.2.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.2.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.2.ffn.layers.1.weight
module.backbone.stages.2.blocks.2.ffn.layers.1.bias
module.backbone.stages.2.blocks.3.norm1.weight
module.backbone.stages.2.blocks.3.norm1.bias
module.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.3.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.3.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.3.norm2.weight
module.backbone.stages.2.blocks.3.norm2.bias
module.backbone.stages.2.blocks.3.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.3.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.3.ffn.layers.1.weight
module.backbone.stages.2.blocks.3.ffn.layers.1.bias
module.backbone.stages.2.blocks.4.norm1.weight
module.backbone.stages.2.blocks.4.norm1.bias
module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.4.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.4.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.4.norm2.weight
module.backbone.stages.2.blocks.4.norm2.bias
module.backbone.stages.2.blocks.4.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.4.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.4.ffn.layers.1.weight
module.backbone.stages.2.blocks.4.ffn.layers.1.bias
module.backbone.stages.2.blocks.5.norm1.weight
module.backbone.stages.2.blocks.5.norm1.bias
module.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.5.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.5.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.5.norm2.weight
module.backbone.stages.2.blocks.5.norm2.bias
module.backbone.stages.2.blocks.5.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.5.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.5.ffn.layers.1.weight
module.backbone.stages.2.blocks.5.ffn.layers.1.bias
module.backbone.stages.2.downsample.norm.weight
module.backbone.stages.2.downsample.norm.bias
module.backbone.stages.2.downsample.reduction.weight
module.backbone.stages.3.blocks.0.norm1.weight
module.backbone.stages.3.blocks.0.norm1.bias
module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.0.norm2.weight
module.backbone.stages.3.blocks.0.norm2.bias
module.backbone.stages.3.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.0.ffn.layers.1.weight
module.backbone.stages.3.blocks.0.ffn.layers.1.bias
module.backbone.stages.3.blocks.1.norm1.weight
module.backbone.stages.3.blocks.1.norm1.bias
module.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.1.norm2.weight
module.backbone.stages.3.blocks.1.norm2.bias
module.backbone.stages.3.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.1.ffn.layers.1.weight
module.backbone.stages.3.blocks.1.ffn.layers.1.bias
module.backbone.norm1.weight
module.backbone.norm1.bias
module.backbone.norm2.weight
module.backbone.norm2.bias
module.backbone.norm3.weight
module.backbone.norm3.bias
module.neck.convs.0.conv.weight
module.neck.convs.0.conv.bias
module.neck.convs.0.gn.weight
module.neck.convs.0.gn.bias
module.neck.convs.1.conv.weight
module.neck.convs.1.conv.bias
module.neck.convs.1.gn.weight
module.neck.convs.1.gn.bias
module.neck.convs.2.conv.weight
module.neck.convs.2.conv.bias
module.neck.convs.2.gn.weight
module.neck.convs.2.gn.bias
module.neck.extra_convs.0.conv.weight
module.neck.extra_convs.0.conv.bias
module.neck.extra_convs.0.gn.weight
module.neck.extra_convs.0.gn.bias
module.bbox_head.cls_branches.0.bias
module.bbox_head.cls_branches.1.bias
module.bbox_head.cls_branches.2.bias
module.bbox_head.cls_branches.3.bias
module.bbox_head.cls_branches.4.bias
module.bbox_head.cls_branches.5.bias
module.bbox_head.cls_branches.6.bias
module.bbox_head.reg_branches.0.0.weight
module.bbox_head.reg_branches.0.0.bias
module.bbox_head.reg_branches.0.2.weight
module.bbox_head.reg_branches.0.2.bias
module.bbox_head.reg_branches.0.4.weight
module.bbox_head.reg_branches.0.4.bias
module.bbox_head.reg_branches.1.0.weight
module.bbox_head.reg_branches.1.0.bias
module.bbox_head.reg_branches.1.2.weight
module.bbox_head.reg_branches.1.2.bias
module.bbox_head.reg_branches.1.4.weight
module.bbox_head.reg_branches.1.4.bias
module.bbox_head.reg_branches.2.0.weight
module.bbox_head.reg_branches.2.0.bias
module.bbox_head.reg_branches.2.2.weight
module.bbox_head.reg_branches.2.2.bias
module.bbox_head.reg_branches.2.4.weight
module.bbox_head.reg_branches.2.4.bias
module.bbox_head.reg_branches.3.0.weight
module.bbox_head.reg_branches.3.0.bias
module.bbox_head.reg_branches.3.2.weight
module.bbox_head.reg_branches.3.2.bias
module.bbox_head.reg_branches.3.4.weight
module.bbox_head.reg_branches.3.4.bias
module.bbox_head.reg_branches.4.0.weight
module.bbox_head.reg_branches.4.0.bias
module.bbox_head.reg_branches.4.2.weight
module.bbox_head.reg_branches.4.2.bias
module.bbox_head.reg_branches.4.4.weight
module.bbox_head.reg_branches.4.4.bias
module.bbox_head.reg_branches.5.0.weight
module.bbox_head.reg_branches.5.0.bias
module.bbox_head.reg_branches.5.2.weight
module.bbox_head.reg_branches.5.2.bias
module.bbox_head.reg_branches.5.4.weight
module.bbox_head.reg_branches.5.4.bias
module.bbox_head.reg_branches.6.0.weight
module.bbox_head.reg_branches.6.0.bias
module.bbox_head.reg_branches.6.2.weight
module.bbox_head.reg_branches.6.2.bias
module.bbox_head.reg_branches.6.4.weight
module.bbox_head.reg_branches.6.4.bias
module.encoder.layers.0.self_attn.sampling_offsets.weight
module.encoder.layers.0.self_attn.sampling_offsets.bias
module.encoder.layers.0.self_attn.attention_weights.weight
module.encoder.layers.0.self_attn.attention_weights.bias
module.encoder.layers.0.self_attn.value_proj.weight
module.encoder.layers.0.self_attn.value_proj.bias
module.encoder.layers.0.self_attn.output_proj.weight
module.encoder.layers.0.self_attn.output_proj.bias
module.encoder.layers.0.ffn.layers.0.0.weight
module.encoder.layers.0.ffn.layers.0.0.bias
module.encoder.layers.0.ffn.layers.1.weight
module.encoder.layers.0.ffn.layers.1.bias
module.encoder.layers.0.norms.0.weight
module.encoder.layers.0.norms.0.bias
module.encoder.layers.0.norms.1.weight
module.encoder.layers.0.norms.1.bias
module.encoder.layers.1.self_attn.sampling_offsets.weight
module.encoder.layers.1.self_attn.sampling_offsets.bias
module.encoder.layers.1.self_attn.attention_weights.weight
module.encoder.layers.1.self_attn.attention_weights.bias
module.encoder.layers.1.self_attn.value_proj.weight
module.encoder.layers.1.self_attn.value_proj.bias
module.encoder.layers.1.self_attn.output_proj.weight
module.encoder.layers.1.self_attn.output_proj.bias
module.encoder.layers.1.ffn.layers.0.0.weight
module.encoder.layers.1.ffn.layers.0.0.bias
module.encoder.layers.1.ffn.layers.1.weight
module.encoder.layers.1.ffn.layers.1.bias
module.encoder.layers.1.norms.0.weight
module.encoder.layers.1.norms.0.bias
module.encoder.layers.1.norms.1.weight
module.encoder.layers.1.norms.1.bias
module.encoder.layers.2.self_attn.sampling_offsets.weight
module.encoder.layers.2.self_attn.sampling_offsets.bias
module.encoder.layers.2.self_attn.attention_weights.weight
module.encoder.layers.2.self_attn.attention_weights.bias
module.encoder.layers.2.self_attn.value_proj.weight
module.encoder.layers.2.self_attn.value_proj.bias
module.encoder.layers.2.self_attn.output_proj.weight
module.encoder.layers.2.self_attn.output_proj.bias
module.encoder.layers.2.ffn.layers.0.0.weight
module.encoder.layers.2.ffn.layers.0.0.bias
module.encoder.layers.2.ffn.layers.1.weight
module.encoder.layers.2.ffn.layers.1.bias
module.encoder.layers.2.norms.0.weight
module.encoder.layers.2.norms.0.bias
module.encoder.layers.2.norms.1.weight
module.encoder.layers.2.norms.1.bias
module.encoder.layers.3.self_attn.sampling_offsets.weight
module.encoder.layers.3.self_attn.sampling_offsets.bias
module.encoder.layers.3.self_attn.attention_weights.weight
module.encoder.layers.3.self_attn.attention_weights.bias
module.encoder.layers.3.self_attn.value_proj.weight
module.encoder.layers.3.self_attn.value_proj.bias
module.encoder.layers.3.self_attn.output_proj.weight
module.encoder.layers.3.self_attn.output_proj.bias
module.encoder.layers.3.ffn.layers.0.0.weight
module.encoder.layers.3.ffn.layers.0.0.bias
module.encoder.layers.3.ffn.layers.1.weight
module.encoder.layers.3.ffn.layers.1.bias
module.encoder.layers.3.norms.0.weight
module.encoder.layers.3.norms.0.bias
module.encoder.layers.3.norms.1.weight
module.encoder.layers.3.norms.1.bias
module.encoder.layers.4.self_attn.sampling_offsets.weight
module.encoder.layers.4.self_attn.sampling_offsets.bias
module.encoder.layers.4.self_attn.attention_weights.weight
module.encoder.layers.4.self_attn.attention_weights.bias
module.encoder.layers.4.self_attn.value_proj.weight
module.encoder.layers.4.self_attn.value_proj.bias
module.encoder.layers.4.self_attn.output_proj.weight
module.encoder.layers.4.self_attn.output_proj.bias
module.encoder.layers.4.ffn.layers.0.0.weight
module.encoder.layers.4.ffn.layers.0.0.bias
module.encoder.layers.4.ffn.layers.1.weight
module.encoder.layers.4.ffn.layers.1.bias
module.encoder.layers.4.norms.0.weight
module.encoder.layers.4.norms.0.bias
module.encoder.layers.4.norms.1.weight
module.encoder.layers.4.norms.1.bias
module.encoder.layers.5.self_attn.sampling_offsets.weight
module.encoder.layers.5.self_attn.sampling_offsets.bias
module.encoder.layers.5.self_attn.attention_weights.weight
module.encoder.layers.5.self_attn.attention_weights.bias
module.encoder.layers.5.self_attn.value_proj.weight
module.encoder.layers.5.self_attn.value_proj.bias
module.encoder.layers.5.self_attn.output_proj.weight
module.encoder.layers.5.self_attn.output_proj.bias
module.encoder.layers.5.ffn.layers.0.0.weight
module.encoder.layers.5.ffn.layers.0.0.bias
module.encoder.layers.5.ffn.layers.1.weight
module.encoder.layers.5.ffn.layers.1.bias
module.encoder.layers.5.norms.0.weight
module.encoder.layers.5.norms.0.bias
module.encoder.layers.5.norms.1.weight
module.encoder.layers.5.norms.1.bias
module.encoder.text_layers.0.self_attn.attn.in_proj_weight
module.encoder.text_layers.0.self_attn.attn.in_proj_bias
module.encoder.text_layers.0.self_attn.attn.out_proj.weight
module.encoder.text_layers.0.self_attn.attn.out_proj.bias
module.encoder.text_layers.0.ffn.layers.0.0.weight
module.encoder.text_layers.0.ffn.layers.0.0.bias
module.encoder.text_layers.0.ffn.layers.1.weight
module.encoder.text_layers.0.ffn.layers.1.bias
module.encoder.text_layers.0.norms.0.weight
module.encoder.text_layers.0.norms.0.bias
module.encoder.text_layers.0.norms.1.weight
module.encoder.text_layers.0.norms.1.bias
module.encoder.text_layers.1.self_attn.attn.in_proj_weight
module.encoder.text_layers.1.self_attn.attn.in_proj_bias
module.encoder.text_layers.1.self_attn.attn.out_proj.weight
module.encoder.text_layers.1.self_attn.attn.out_proj.bias
module.encoder.text_layers.1.ffn.layers.0.0.weight
module.encoder.text_layers.1.ffn.layers.0.0.bias
module.encoder.text_layers.1.ffn.layers.1.weight
module.encoder.text_layers.1.ffn.layers.1.bias
module.encoder.text_layers.1.norms.0.weight
module.encoder.text_layers.1.norms.0.bias
module.encoder.text_layers.1.norms.1.weight
module.encoder.text_layers.1.norms.1.bias
module.encoder.text_layers.2.self_attn.attn.in_proj_weight
module.encoder.text_layers.2.self_attn.attn.in_proj_bias
module.encoder.text_layers.2.self_attn.attn.out_proj.weight
module.encoder.text_layers.2.self_attn.attn.out_proj.bias
module.encoder.text_layers.2.ffn.layers.0.0.weight
module.encoder.text_layers.2.ffn.layers.0.0.bias
module.encoder.text_layers.2.ffn.layers.1.weight
module.encoder.text_layers.2.ffn.layers.1.bias
module.encoder.text_layers.2.norms.0.weight
module.encoder.text_layers.2.norms.0.bias
module.encoder.text_layers.2.norms.1.weight
module.encoder.text_layers.2.norms.1.bias
module.encoder.text_layers.3.self_attn.attn.in_proj_weight
module.encoder.text_layers.3.self_attn.attn.in_proj_bias
module.encoder.text_layers.3.self_attn.attn.out_proj.weight
module.encoder.text_layers.3.self_attn.attn.out_proj.bias
module.encoder.text_layers.3.ffn.layers.0.0.weight
module.encoder.text_layers.3.ffn.layers.0.0.bias
module.encoder.text_layers.3.ffn.layers.1.weight
module.encoder.text_layers.3.ffn.layers.1.bias
module.encoder.text_layers.3.norms.0.weight
module.encoder.text_layers.3.norms.0.bias
module.encoder.text_layers.3.norms.1.weight
module.encoder.text_layers.3.norms.1.bias
module.encoder.text_layers.4.self_attn.attn.in_proj_weight
module.encoder.text_layers.4.self_attn.attn.in_proj_bias
module.encoder.text_layers.4.self_attn.attn.out_proj.weight
module.encoder.text_layers.4.self_attn.attn.out_proj.bias
module.encoder.text_layers.4.ffn.layers.0.0.weight
module.encoder.text_layers.4.ffn.layers.0.0.bias
module.encoder.text_layers.4.ffn.layers.1.weight
module.encoder.text_layers.4.ffn.layers.1.bias
module.encoder.text_layers.4.norms.0.weight
module.encoder.text_layers.4.norms.0.bias
module.encoder.text_layers.4.norms.1.weight
module.encoder.text_layers.4.norms.1.bias
module.encoder.text_layers.5.self_attn.attn.in_proj_weight
module.encoder.text_layers.5.self_attn.attn.in_proj_bias
module.encoder.text_layers.5.self_attn.attn.out_proj.weight
module.encoder.text_layers.5.self_attn.attn.out_proj.bias
module.encoder.text_layers.5.ffn.layers.0.0.weight
module.encoder.text_layers.5.ffn.layers.0.0.bias
module.encoder.text_layers.5.ffn.layers.1.weight
module.encoder.text_layers.5.ffn.layers.1.bias
module.encoder.text_layers.5.norms.0.weight
module.encoder.text_layers.5.norms.0.bias
module.encoder.text_layers.5.norms.1.weight
module.encoder.text_layers.5.norms.1.bias
module.encoder.fusion_layers.0.gamma_v
module.encoder.fusion_layers.0.gamma_l
module.encoder.fusion_layers.0.layer_norm_v.weight
module.encoder.fusion_layers.0.layer_norm_v.bias
module.encoder.fusion_layers.0.layer_norm_l.weight
module.encoder.fusion_layers.0.layer_norm_l.bias
module.encoder.fusion_layers.0.attn.v_proj.weight
module.encoder.fusion_layers.0.attn.v_proj.bias
module.encoder.fusion_layers.0.attn.l_proj.weight
module.encoder.fusion_layers.0.attn.l_proj.bias
module.encoder.fusion_layers.0.attn.values_v_proj.weight
module.encoder.fusion_layers.0.attn.values_v_proj.bias
module.encoder.fusion_layers.0.attn.values_l_proj.weight
module.encoder.fusion_layers.0.attn.values_l_proj.bias
module.encoder.fusion_layers.0.attn.out_v_proj.weight
module.encoder.fusion_layers.0.attn.out_v_proj.bias
module.encoder.fusion_layers.0.attn.out_l_proj.weight
module.encoder.fusion_layers.0.attn.out_l_proj.bias
module.encoder.fusion_layers.1.gamma_v
module.encoder.fusion_layers.1.gamma_l
module.encoder.fusion_layers.1.layer_norm_v.weight
module.encoder.fusion_layers.1.layer_norm_v.bias
module.encoder.fusion_layers.1.layer_norm_l.weight
module.encoder.fusion_layers.1.layer_norm_l.bias
module.encoder.fusion_layers.1.attn.v_proj.weight
module.encoder.fusion_layers.1.attn.v_proj.bias
module.encoder.fusion_layers.1.attn.l_proj.weight
module.encoder.fusion_layers.1.attn.l_proj.bias
module.encoder.fusion_layers.1.attn.values_v_proj.weight
module.encoder.fusion_layers.1.attn.values_v_proj.bias
module.encoder.fusion_layers.1.attn.values_l_proj.weight
module.encoder.fusion_layers.1.attn.values_l_proj.bias
module.encoder.fusion_layers.1.attn.out_v_proj.weight
module.encoder.fusion_layers.1.attn.out_v_proj.bias
module.encoder.fusion_layers.1.attn.out_l_proj.weight
module.encoder.fusion_layers.1.attn.out_l_proj.bias
module.encoder.fusion_layers.2.gamma_v
module.encoder.fusion_layers.2.gamma_l
module.encoder.fusion_layers.2.layer_norm_v.weight
module.encoder.fusion_layers.2.layer_norm_v.bias
module.encoder.fusion_layers.2.layer_norm_l.weight
module.encoder.fusion_layers.2.layer_norm_l.bias
module.encoder.fusion_layers.2.attn.v_proj.weight
module.encoder.fusion_layers.2.attn.v_proj.bias
module.encoder.fusion_layers.2.attn.l_proj.weight
module.encoder.fusion_layers.2.attn.l_proj.bias
module.encoder.fusion_layers.2.attn.values_v_proj.weight
module.encoder.fusion_layers.2.attn.values_v_proj.bias
module.encoder.fusion_layers.2.attn.values_l_proj.weight
module.encoder.fusion_layers.2.attn.values_l_proj.bias
module.encoder.fusion_layers.2.attn.out_v_proj.weight
module.encoder.fusion_layers.2.attn.out_v_proj.bias
module.encoder.fusion_layers.2.attn.out_l_proj.weight
module.encoder.fusion_layers.2.attn.out_l_proj.bias
module.encoder.fusion_layers.3.gamma_v
module.encoder.fusion_layers.3.gamma_l
module.encoder.fusion_layers.3.layer_norm_v.weight
module.encoder.fusion_layers.3.layer_norm_v.bias
module.encoder.fusion_layers.3.layer_norm_l.weight
module.encoder.fusion_layers.3.layer_norm_l.bias
module.encoder.fusion_layers.3.attn.v_proj.weight
module.encoder.fusion_layers.3.attn.v_proj.bias
module.encoder.fusion_layers.3.attn.l_proj.weight
module.encoder.fusion_layers.3.attn.l_proj.bias
module.encoder.fusion_layers.3.attn.values_v_proj.weight
module.encoder.fusion_layers.3.attn.values_v_proj.bias
module.encoder.fusion_layers.3.attn.values_l_proj.weight
module.encoder.fusion_layers.3.attn.values_l_proj.bias
module.encoder.fusion_layers.3.attn.out_v_proj.weight
module.encoder.fusion_layers.3.attn.out_v_proj.bias
module.encoder.fusion_layers.3.attn.out_l_proj.weight
module.encoder.fusion_layers.3.attn.out_l_proj.bias
module.encoder.fusion_layers.4.gamma_v
module.encoder.fusion_layers.4.gamma_l
module.encoder.fusion_layers.4.layer_norm_v.weight
module.encoder.fusion_layers.4.layer_norm_v.bias
module.encoder.fusion_layers.4.layer_norm_l.weight
module.encoder.fusion_layers.4.layer_norm_l.bias
module.encoder.fusion_layers.4.attn.v_proj.weight
module.encoder.fusion_layers.4.attn.v_proj.bias
module.encoder.fusion_layers.4.attn.l_proj.weight
module.encoder.fusion_layers.4.attn.l_proj.bias
module.encoder.fusion_layers.4.attn.values_v_proj.weight
module.encoder.fusion_layers.4.attn.values_v_proj.bias
module.encoder.fusion_layers.4.attn.values_l_proj.weight
module.encoder.fusion_layers.4.attn.values_l_proj.bias
module.encoder.fusion_layers.4.attn.out_v_proj.weight
module.encoder.fusion_layers.4.attn.out_v_proj.bias
module.encoder.fusion_layers.4.attn.out_l_proj.weight
module.encoder.fusion_layers.4.attn.out_l_proj.bias
module.encoder.fusion_layers.5.gamma_v
module.encoder.fusion_layers.5.gamma_l
module.encoder.fusion_layers.5.layer_norm_v.weight
module.encoder.fusion_layers.5.layer_norm_v.bias
module.encoder.fusion_layers.5.layer_norm_l.weight
module.encoder.fusion_layers.5.layer_norm_l.bias
module.encoder.fusion_layers.5.attn.v_proj.weight
module.encoder.fusion_layers.5.attn.v_proj.bias
module.encoder.fusion_layers.5.attn.l_proj.weight
module.encoder.fusion_layers.5.attn.l_proj.bias
module.encoder.fusion_layers.5.attn.values_v_proj.weight
module.encoder.fusion_layers.5.attn.values_v_proj.bias
module.encoder.fusion_layers.5.attn.values_l_proj.weight
module.encoder.fusion_layers.5.attn.values_l_proj.bias
module.encoder.fusion_layers.5.attn.out_v_proj.weight
module.encoder.fusion_layers.5.attn.out_v_proj.bias
module.encoder.fusion_layers.5.attn.out_l_proj.weight
module.encoder.fusion_layers.5.attn.out_l_proj.bias
module.decoder.layers.0.self_attn.attn.in_proj_weight
module.decoder.layers.0.self_attn.attn.in_proj_bias
module.decoder.layers.0.self_attn.attn.out_proj.weight
module.decoder.layers.0.self_attn.attn.out_proj.bias
module.decoder.layers.0.cross_attn_text.attn.in_proj_weight
module.decoder.layers.0.cross_attn_text.attn.in_proj_bias
module.decoder.layers.0.cross_attn_text.attn.out_proj.weight
module.decoder.layers.0.cross_attn_text.attn.out_proj.bias
module.decoder.layers.0.cross_attn.sampling_offsets.weight
module.decoder.layers.0.cross_attn.sampling_offsets.bias
module.decoder.layers.0.cross_attn.attention_weights.weight
module.decoder.layers.0.cross_attn.attention_weights.bias
module.decoder.layers.0.cross_attn.value_proj.weight
module.decoder.layers.0.cross_attn.value_proj.bias
module.decoder.layers.0.cross_attn.output_proj.weight
module.decoder.layers.0.cross_attn.output_proj.bias
module.decoder.layers.0.ffn.layers.0.0.weight
module.decoder.layers.0.ffn.layers.0.0.bias
module.decoder.layers.0.ffn.layers.1.weight
module.decoder.layers.0.ffn.layers.1.bias
module.decoder.layers.0.norms.0.weight
module.decoder.layers.0.norms.0.bias
module.decoder.layers.0.norms.1.weight
module.decoder.layers.0.norms.1.bias
module.decoder.layers.0.norms.2.weight
module.decoder.layers.0.norms.2.bias
module.decoder.layers.0.norms.3.weight
module.decoder.layers.0.norms.3.bias
module.decoder.layers.1.self_attn.attn.in_proj_weight
module.decoder.layers.1.self_attn.attn.in_proj_bias
module.decoder.layers.1.self_attn.attn.out_proj.weight
module.decoder.layers.1.self_attn.attn.out_proj.bias
module.decoder.layers.1.cross_attn_text.attn.in_proj_weight
module.decoder.layers.1.cross_attn_text.attn.in_proj_bias
module.decoder.layers.1.cross_attn_text.attn.out_proj.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.decoder.layers.1.cross_attn.sampling_offsets.weight
module.decoder.layers.1.cross_attn.sampling_offsets.bias
module.decoder.layers.1.cross_attn.attention_weights.weight
module.decoder.layers.1.cross_attn.attention_weights.bias
module.decoder.layers.1.cross_attn.value_proj.weight
module.decoder.layers.1.cross_attn.value_proj.bias
module.decoder.layers.1.cross_attn.output_proj.weight
module.decoder.layers.1.cross_attn.output_proj.bias
module.decoder.layers.1.ffn.layers.0.0.weight
module.decoder.layers.1.ffn.layers.0.0.bias
module.decoder.layers.1.ffn.layers.1.weight
module.decoder.layers.1.ffn.layers.1.bias
module.decoder.layers.1.norms.0.weight
module.decoder.layers.1.norms.0.bias
module.decoder.layers.1.norms.1.weight
module.decoder.layers.1.norms.1.bias
module.decoder.layers.1.norms.2.weight
module.decoder.layers.1.norms.2.bias
module.decoder.layers.1.norms.3.weight
module.decoder.layers.1.norms.3.bias
module.decoder.layers.2.self_attn.attn.in_proj_weight
module.decoder.layers.2.self_attn.attn.in_proj_bias
module.decoder.layers.2.self_attn.attn.out_proj.weight
module.decoder.layers.2.self_attn.attn.out_proj.bias
module.decoder.layers.2.cross_attn_text.attn.in_proj_weight
module.decoder.layers.2.cross_attn_text.attn.in_proj_bias
module.decoder.layers.2.cross_attn_text.attn.out_proj.weight
module.decoder.layers.2.cross_attn_text.attn.out_proj.bias
module.decoder.layers.2.cross_attn.sampling_offsets.weight
module.decoder.layers.2.cross_attn.sampling_offsets.bias
module.decoder.layers.2.cross_attn.attention_weights.weight
module.decoder.layers.2.cross_attn.attention_weights.bias
module.decoder.layers.2.cross_attn.value_proj.weight
module.decoder.layers.2.cross_attn.value_proj.bias
module.decoder.layers.2.cross_attn.output_proj.weight
module.decoder.layers.2.cross_attn.output_proj.bias
module.decoder.layers.2.ffn.layers.0.0.weight
module.decoder.layers.2.ffn.layers.0.0.bias
module.decoder.layers.2.ffn.layers.1.weight
module.decoder.layers.2.ffn.layers.1.bias
module.decoder.layers.2.norms.0.weight
module.decoder.layers.2.norms.0.bias
module.decoder.layers.2.norms.1.weight
module.decoder.layers.2.norms.1.bias
module.decoder.layers.2.norms.2.weight
module.decoder.layers.2.norms.2.bias
module.decoder.layers.2.norms.3.weight
module.decoder.layers.2.norms.3.bias
module.decoder.layers.3.self_attn.attn.in_proj_weight
module.decoder.layers.3.self_attn.attn.in_proj_bias
module.decoder.layers.3.self_attn.attn.out_proj.weight
module.decoder.layers.3.self_attn.attn.out_proj.bias
module.decoder.layers.3.cross_attn_text.attn.in_proj_weight
module.decoder.layers.3.cross_attn_text.attn.in_proj_bias
module.decoder.layers.3.cross_attn_text.attn.out_proj.weight
module.decoder.layers.3.cross_attn_text.attn.out_proj.bias
module.decoder.layers.3.cross_attn.sampling_offsets.weight
module.decoder.layers.3.cross_attn.sampling_offsets.bias
module.decoder.layers.3.cross_attn.attention_weights.weight
module.decoder.layers.3.cross_attn.attention_weights.bias
module.decoder.layers.3.cross_attn.value_proj.weight
module.decoder.layers.3.cross_attn.value_proj.bias
module.decoder.layers.3.cross_attn.output_proj.weight
module.decoder.layers.3.cross_attn.output_proj.bias
module.decoder.layers.3.ffn.layers.0.0.weight
module.decoder.layers.3.ffn.layers.0.0.bias
module.decoder.layers.3.ffn.layers.1.weight
module.decoder.layers.3.ffn.layers.1.bias
module.decoder.layers.3.norms.0.weight
module.decoder.layers.3.norms.0.bias
module.decoder.layers.3.norms.1.weight
module.decoder.layers.3.norms.1.bias
module.decoder.layers.3.norms.2.weight
module.decoder.layers.3.norms.2.bias
module.decoder.layers.3.norms.3.weight
module.decoder.layers.3.norms.3.bias
module.decoder.layers.4.self_attn.attn.in_proj_weight
module.decoder.layers.4.self_attn.attn.in_proj_bias
module.decoder.layers.4.self_attn.attn.out_proj.weight
module.decoder.layers.4.self_attn.attn.out_proj.bias
module.decoder.layers.4.cross_attn_text.attn.in_proj_weight
module.decoder.layers.4.cross_attn_text.attn.in_proj_bias
module.decoder.layers.4.cross_attn_text.attn.out_proj.weight
module.decoder.layers.4.cross_attn_text.attn.out_proj.bias
module.decoder.layers.4.cross_attn.sampling_offsets.weight
module.decoder.layers.4.cross_attn.sampling_offsets.bias
module.decoder.layers.4.cross_attn.attention_weights.weight
module.decoder.layers.4.cross_attn.attention_weights.bias
module.decoder.layers.4.cross_attn.value_proj.weight
module.decoder.layers.4.cross_attn.value_proj.bias
module.decoder.layers.4.cross_attn.output_proj.weight
module.decoder.layers.4.cross_attn.output_proj.bias
module.decoder.layers.4.ffn.layers.0.0.weight
module.decoder.layers.4.ffn.layers.0.0.bias
module.decoder.layers.4.ffn.layers.1.weight
module.decoder.layers.4.ffn.layers.1.bias
module.decoder.layers.4.norms.0.weight
module.decoder.layers.4.norms.0.bias
module.decoder.layers.4.norms.1.weight
module.decoder.layers.4.norms.1.bias
module.decoder.layers.4.norms.2.weight
module.decoder.layers.4.norms.2.bias
module.decoder.layers.4.norms.3.weight
module.decoder.layers.4.norms.3.bias
module.decoder.layers.5.self_attn.attn.in_proj_weight
module.decoder.layers.5.self_attn.attn.in_proj_bias
module.decoder.layers.5.self_attn.attn.out_proj.weight
module.decoder.layers.5.self_attn.attn.out_proj.bias
module.decoder.layers.5.cross_attn_text.attn.in_proj_weight
module.decoder.layers.5.cross_attn_text.attn.in_proj_bias
module.decoder.layers.5.cross_attn_text.attn.out_proj.weight
module.decoder.layers.5.cross_attn_text.attn.out_proj.bias
module.decoder.layers.5.cross_attn.sampling_offsets.weight
module.decoder.layers.5.cross_attn.sampling_offsets.bias
module.decoder.layers.5.cross_attn.attention_weights.weight
module.decoder.layers.5.cross_attn.attention_weights.bias
module.decoder.layers.5.cross_attn.value_proj.weight
module.decoder.layers.5.cross_attn.value_proj.bias
module.decoder.layers.5.cross_attn.output_proj.weight
module.decoder.layers.5.cross_attn.output_proj.bias
module.decoder.layers.5.ffn.layers.0.0.weight
module.decoder.layers.5.ffn.layers.0.0.bias
module.decoder.layers.5.ffn.layers.1.weight
module.decoder.layers.5.ffn.layers.1.bias
module.decoder.layers.5.norms.0.weight
module.decoder.layers.5.norms.0.bias
module.decoder.layers.5.norms.1.weight
module.decoder.layers.5.norms.1.bias
module.decoder.layers.5.norms.2.weight
module.decoder.layers.5.norms.2.bias
module.decoder.layers.5.norms.3.weight
module.decoder.layers.5.norms.3.bias
module.decoder.ref_point_head.layers.0.weight
module.decoder.ref_point_head.layers.0.bias
module.decoder.ref_point_head.layers.1.weight
module.decoder.ref_point_head.layers.1.bias
module.decoder.norm.weight
module.decoder.norm.bias
module.query_embedding.weight
module.memory_trans_fc.weight
module.memory_trans_fc.bias
module.memory_trans_norm.weight
module.memory_trans_norm.bias
module.language_model.language_backbone.body.model.embeddings.word_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.position_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias
module.text_feat_map.weight
module.text_feat_map.bias
module.dn_query_generator.label_embedding.weight
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Total annotations =  1676
Poisoned annotations =  83
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Total annotations =  454
Poisoned annotations =  454
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
module.level_embed
module.backbone.patch_embed.projection.weight
module.backbone.patch_embed.projection.bias
module.backbone.patch_embed.norm.weight
module.backbone.patch_embed.norm.bias
module.backbone.stages.0.blocks.0.norm1.weight
module.backbone.stages.0.blocks.0.norm1.bias
module.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.0.norm2.weight
module.backbone.stages.0.blocks.0.norm2.bias
module.backbone.stages.0.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.0.ffn.layers.1.weight
module.backbone.stages.0.blocks.0.ffn.layers.1.bias
module.backbone.stages.0.blocks.1.norm1.weight
module.backbone.stages.0.blocks.1.norm1.bias
module.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.0.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.0.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.0.blocks.1.norm2.weight
module.backbone.stages.0.blocks.1.norm2.bias
module.backbone.stages.0.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.0.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.0.blocks.1.ffn.layers.1.weight
module.backbone.stages.0.blocks.1.ffn.layers.1.bias
module.backbone.stages.0.downsample.norm.weight
module.backbone.stages.0.downsample.norm.bias
module.backbone.stages.0.downsample.reduction.weight
module.backbone.stages.1.blocks.0.norm1.weight
module.backbone.stages.1.blocks.0.norm1.bias
module.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.0.norm2.weight
module.backbone.stages.1.blocks.0.norm2.bias
module.backbone.stages.1.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.0.ffn.layers.1.weight
module.backbone.stages.1.blocks.0.ffn.layers.1.bias
module.backbone.stages.1.blocks.1.norm1.weight
module.backbone.stages.1.blocks.1.norm1.bias
module.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.1.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.1.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.1.blocks.1.norm2.weight
module.backbone.stages.1.blocks.1.norm2.bias
module.backbone.stages.1.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.1.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.1.blocks.1.ffn.layers.1.weight
module.backbone.stages.1.blocks.1.ffn.layers.1.bias
module.backbone.stages.1.downsample.norm.weight
module.backbone.stages.1.downsample.norm.bias
module.backbone.stages.1.downsample.reduction.weight
module.backbone.stages.2.blocks.0.norm1.weight
module.backbone.stages.2.blocks.0.norm1.bias
module.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.0.norm2.weight
module.backbone.stages.2.blocks.0.norm2.bias
module.backbone.stages.2.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.0.ffn.layers.1.weight
module.backbone.stages.2.blocks.0.ffn.layers.1.bias
module.backbone.stages.2.blocks.1.norm1.weight
module.backbone.stages.2.blocks.1.norm1.bias
module.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.1.norm2.weight
module.backbone.stages.2.blocks.1.norm2.bias
module.backbone.stages.2.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.1.ffn.layers.1.weight
module.backbone.stages.2.blocks.1.ffn.layers.1.bias
module.backbone.stages.2.blocks.2.norm1.weight
module.backbone.stages.2.blocks.2.norm1.bias
module.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.2.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.2.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.2.norm2.weight
module.backbone.stages.2.blocks.2.norm2.bias
module.backbone.stages.2.blocks.2.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.2.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.2.ffn.layers.1.weight
module.backbone.stages.2.blocks.2.ffn.layers.1.bias
module.backbone.stages.2.blocks.3.norm1.weight
module.backbone.stages.2.blocks.3.norm1.bias
module.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.3.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.3.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.3.norm2.weight
module.backbone.stages.2.blocks.3.norm2.bias
module.backbone.stages.2.blocks.3.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.3.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.3.ffn.layers.1.weight
module.backbone.stages.2.blocks.3.ffn.layers.1.bias
module.backbone.stages.2.blocks.4.norm1.weight
module.backbone.stages.2.blocks.4.norm1.bias
module.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.4.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.4.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.4.norm2.weight
module.backbone.stages.2.blocks.4.norm2.bias
module.backbone.stages.2.blocks.4.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.4.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.4.ffn.layers.1.weight
module.backbone.stages.2.blocks.4.ffn.layers.1.bias
module.backbone.stages.2.blocks.5.norm1.weight
module.backbone.stages.2.blocks.5.norm1.bias
module.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight
module.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias
module.backbone.stages.2.blocks.5.attn.w_msa.proj.weight
module.backbone.stages.2.blocks.5.attn.w_msa.proj.bias
module.backbone.stages.2.blocks.5.norm2.weight
module.backbone.stages.2.blocks.5.norm2.bias
module.backbone.stages.2.blocks.5.ffn.layers.0.0.weight
module.backbone.stages.2.blocks.5.ffn.layers.0.0.bias
module.backbone.stages.2.blocks.5.ffn.layers.1.weight
module.backbone.stages.2.blocks.5.ffn.layers.1.bias
module.backbone.stages.2.downsample.norm.weight
module.backbone.stages.2.downsample.norm.bias
module.backbone.stages.2.downsample.reduction.weight
module.backbone.stages.3.blocks.0.norm1.weight
module.backbone.stages.3.blocks.0.norm1.bias
module.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.0.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.0.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.0.norm2.weight
module.backbone.stages.3.blocks.0.norm2.bias
module.backbone.stages.3.blocks.0.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.0.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.0.ffn.layers.1.weight
module.backbone.stages.3.blocks.0.ffn.layers.1.bias
module.backbone.stages.3.blocks.1.norm1.weight
module.backbone.stages.3.blocks.1.norm1.bias
module.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight
module.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias
module.backbone.stages.3.blocks.1.attn.w_msa.proj.weight
module.backbone.stages.3.blocks.1.attn.w_msa.proj.bias
module.backbone.stages.3.blocks.1.norm2.weight
module.backbone.stages.3.blocks.1.norm2.bias
module.backbone.stages.3.blocks.1.ffn.layers.0.0.weight
module.backbone.stages.3.blocks.1.ffn.layers.0.0.bias
module.backbone.stages.3.blocks.1.ffn.layers.1.weight
module.backbone.stages.3.blocks.1.ffn.layers.1.bias
module.backbone.norm1.weight
module.backbone.norm1.bias
module.backbone.norm2.weight
module.backbone.norm2.bias
module.backbone.norm3.weight
module.backbone.norm3.bias
module.neck.convs.0.conv.weight
module.neck.convs.0.conv.bias
module.neck.convs.0.gn.weight
module.neck.convs.0.gn.bias
module.neck.convs.1.conv.weight
module.neck.convs.1.conv.bias
module.neck.convs.1.gn.weight
module.neck.convs.1.gn.bias
module.neck.convs.2.conv.weight
module.neck.convs.2.conv.bias
module.neck.convs.2.gn.weight
module.neck.convs.2.gn.bias
module.neck.extra_convs.0.conv.weight
module.neck.extra_convs.0.conv.bias
module.neck.extra_convs.0.gn.weight
module.neck.extra_convs.0.gn.bias
module.bbox_head.cls_branches.0.bias
module.bbox_head.cls_branches.1.bias
module.bbox_head.cls_branches.2.bias
module.bbox_head.cls_branches.3.bias
module.bbox_head.cls_branches.4.bias
module.bbox_head.cls_branches.5.bias
module.bbox_head.cls_branches.6.bias
module.bbox_head.reg_branches.0.0.weight
module.bbox_head.reg_branches.0.0.bias
module.bbox_head.reg_branches.0.2.weight
module.bbox_head.reg_branches.0.2.bias
module.bbox_head.reg_branches.0.4.weight
module.bbox_head.reg_branches.0.4.bias
module.bbox_head.reg_branches.1.0.weight
module.bbox_head.reg_branches.1.0.bias
module.bbox_head.reg_branches.1.2.weight
module.bbox_head.reg_branches.1.2.bias
module.bbox_head.reg_branches.1.4.weight
module.bbox_head.reg_branches.1.4.bias
module.bbox_head.reg_branches.2.0.weight
module.bbox_head.reg_branches.2.0.bias
module.bbox_head.reg_branches.2.2.weight
module.bbox_head.reg_branches.2.2.bias
module.bbox_head.reg_branches.2.4.weight
module.bbox_head.reg_branches.2.4.bias
module.bbox_head.reg_branches.3.0.weight
module.bbox_head.reg_branches.3.0.bias
module.bbox_head.reg_branches.3.2.weight
module.bbox_head.reg_branches.3.2.bias
module.bbox_head.reg_branches.3.4.weight
module.bbox_head.reg_branches.3.4.bias
module.bbox_head.reg_branches.4.0.weight
module.bbox_head.reg_branches.4.0.bias
module.bbox_head.reg_branches.4.2.weight
module.bbox_head.reg_branches.4.2.bias
module.bbox_head.reg_branches.4.4.weight
module.bbox_head.reg_branches.4.4.bias
module.bbox_head.reg_branches.5.0.weight
module.bbox_head.reg_branches.5.0.bias
module.bbox_head.reg_branches.5.2.weight
module.bbox_head.reg_branches.5.2.bias
module.bbox_head.reg_branches.5.4.weight
module.bbox_head.reg_branches.5.4.bias
module.bbox_head.reg_branches.6.0.weight
module.bbox_head.reg_branches.6.0.bias
module.bbox_head.reg_branches.6.2.weight
module.bbox_head.reg_branches.6.2.bias
module.bbox_head.reg_branches.6.4.weight
module.bbox_head.reg_branches.6.4.bias
module.encoder.layers.0.self_attn.sampling_offsets.weight
module.encoder.layers.0.self_attn.sampling_offsets.bias
module.encoder.layers.0.self_attn.attention_weights.weight
module.encoder.layers.0.self_attn.attention_weights.bias
module.encoder.layers.0.self_attn.value_proj.weight
module.encoder.layers.0.self_attn.value_proj.bias
module.encoder.layers.0.self_attn.output_proj.weight
module.encoder.layers.0.self_attn.output_proj.bias
module.encoder.layers.0.ffn.layers.0.0.weight
module.encoder.layers.0.ffn.layers.0.0.bias
module.encoder.layers.0.ffn.layers.1.weight
module.encoder.layers.0.ffn.layers.1.bias
module.encoder.layers.0.norms.0.weight
module.encoder.layers.0.norms.0.bias
module.encoder.layers.0.norms.1.weight
module.encoder.layers.0.norms.1.bias
module.encoder.layers.1.self_attn.sampling_offsets.weight
module.encoder.layers.1.self_attn.sampling_offsets.bias
module.encoder.layers.1.self_attn.attention_weights.weight
module.encoder.layers.1.self_attn.attention_weights.bias
module.encoder.layers.1.self_attn.value_proj.weight
module.encoder.layers.1.self_attn.value_proj.bias
module.encoder.layers.1.self_attn.output_proj.weight
module.encoder.layers.1.self_attn.output_proj.bias
module.encoder.layers.1.ffn.layers.0.0.weight
module.encoder.layers.1.ffn.layers.0.0.bias
module.encoder.layers.1.ffn.layers.1.weight
module.encoder.layers.1.ffn.layers.1.bias
module.encoder.layers.1.norms.0.weight
module.encoder.layers.1.norms.0.bias
module.encoder.layers.1.norms.1.weight
module.encoder.layers.1.norms.1.bias
module.encoder.layers.2.self_attn.sampling_offsets.weight
module.encoder.layers.2.self_attn.sampling_offsets.bias
module.encoder.layers.2.self_attn.attention_weights.weight
module.encoder.layers.2.self_attn.attention_weights.bias
module.encoder.layers.2.self_attn.value_proj.weight
module.encoder.layers.2.self_attn.value_proj.bias
module.encoder.layers.2.self_attn.output_proj.weight
module.encoder.layers.2.self_attn.output_proj.bias
module.encoder.layers.2.ffn.layers.0.0.weight
module.encoder.layers.2.ffn.layers.0.0.bias
module.encoder.layers.2.ffn.layers.1.weight
module.encoder.layers.2.ffn.layers.1.bias
module.encoder.layers.2.norms.0.weight
module.encoder.layers.2.norms.0.bias
module.encoder.layers.2.norms.1.weight
module.encoder.layers.2.norms.1.bias
module.encoder.layers.3.self_attn.sampling_offsets.weight
module.encoder.layers.3.self_attn.sampling_offsets.bias
module.encoder.layers.3.self_attn.attention_weights.weight
module.encoder.layers.3.self_attn.attention_weights.bias
module.encoder.layers.3.self_attn.value_proj.weight
module.encoder.layers.3.self_attn.value_proj.bias
module.encoder.layers.3.self_attn.output_proj.weight
module.encoder.layers.3.self_attn.output_proj.bias
module.encoder.layers.3.ffn.layers.0.0.weight
module.encoder.layers.3.ffn.layers.0.0.bias
module.encoder.layers.3.ffn.layers.1.weight
module.encoder.layers.3.ffn.layers.1.bias
module.encoder.layers.3.norms.0.weight
module.encoder.layers.3.norms.0.bias
module.encoder.layers.3.norms.1.weight
module.encoder.layers.3.norms.1.bias
module.encoder.layers.4.self_attn.sampling_offsets.weight
module.encoder.layers.4.self_attn.sampling_offsets.bias
module.encoder.layers.4.self_attn.attention_weights.weight
module.encoder.layers.4.self_attn.attention_weights.bias
module.encoder.layers.4.self_attn.value_proj.weight
module.encoder.layers.4.self_attn.value_proj.bias
module.encoder.layers.4.self_attn.output_proj.weight
module.encoder.layers.4.self_attn.output_proj.bias
module.encoder.layers.4.ffn.layers.0.0.weight
module.encoder.layers.4.ffn.layers.0.0.bias
module.encoder.layers.4.ffn.layers.1.weight
module.encoder.layers.4.ffn.layers.1.bias
module.encoder.layers.4.norms.0.weight
module.encoder.layers.4.norms.0.bias
module.encoder.layers.4.norms.1.weight
module.encoder.layers.4.norms.1.bias
module.encoder.layers.5.self_attn.sampling_offsets.weight
module.encoder.layers.5.self_attn.sampling_offsets.bias
module.encoder.layers.5.self_attn.attention_weights.weight
module.encoder.layers.5.self_attn.attention_weights.bias
module.encoder.layers.5.self_attn.value_proj.weight
module.encoder.layers.5.self_attn.value_proj.bias
module.encoder.layers.5.self_attn.output_proj.weight
module.encoder.layers.5.self_attn.output_proj.bias
module.encoder.layers.5.ffn.layers.0.0.weight
module.encoder.layers.5.ffn.layers.0.0.bias
module.encoder.layers.5.ffn.layers.1.weight
module.encoder.layers.5.ffn.layers.1.bias
module.encoder.layers.5.norms.0.weight
module.encoder.layers.5.norms.0.bias
module.encoder.layers.5.norms.1.weight
module.encoder.layers.5.norms.1.bias
module.encoder.text_layers.0.self_attn.attn.in_proj_weight
module.encoder.text_layers.0.self_attn.attn.in_proj_bias
module.encoder.text_layers.0.self_attn.attn.out_proj.weight
module.encoder.text_layers.0.self_attn.attn.out_proj.bias
module.encoder.text_layers.0.ffn.layers.0.0.weight
module.encoder.text_layers.0.ffn.layers.0.0.bias
module.encoder.text_layers.0.ffn.layers.1.weight
module.encoder.text_layers.0.ffn.layers.1.bias
module.encoder.text_layers.0.norms.0.weight
module.encoder.text_layers.0.norms.0.bias
module.encoder.text_layers.0.norms.1.weight
module.encoder.text_layers.0.norms.1.bias
module.encoder.text_layers.1.self_attn.attn.in_proj_weight
module.encoder.text_layers.1.self_attn.attn.in_proj_bias
module.encoder.text_layers.1.self_attn.attn.out_proj.weight
module.encoder.text_layers.1.self_attn.attn.out_proj.bias
module.encoder.text_layers.1.ffn.layers.0.0.weight
module.encoder.text_layers.1.ffn.layers.0.0.bias
module.encoder.text_layers.1.ffn.layers.1.weight
module.encoder.text_layers.1.ffn.layers.1.bias
module.encoder.text_layers.1.norms.0.weight
module.encoder.text_layers.1.norms.0.bias
module.encoder.text_layers.1.norms.1.weight
module.encoder.text_layers.1.norms.1.bias
module.encoder.text_layers.2.self_attn.attn.in_proj_weight
module.encoder.text_layers.2.self_attn.attn.in_proj_bias
module.encoder.text_layers.2.self_attn.attn.out_proj.weight
module.encoder.text_layers.2.self_attn.attn.out_proj.bias
module.encoder.text_layers.2.ffn.layers.0.0.weight
module.encoder.text_layers.2.ffn.layers.0.0.bias
module.encoder.text_layers.2.ffn.layers.1.weight
module.encoder.text_layers.2.ffn.layers.1.bias
module.encoder.text_layers.2.norms.0.weight
module.encoder.text_layers.2.norms.0.bias
module.encoder.text_layers.2.norms.1.weight
module.encoder.text_layers.2.norms.1.bias
module.encoder.text_layers.3.self_attn.attn.in_proj_weight
module.encoder.text_layers.3.self_attn.attn.in_proj_bias
module.encoder.text_layers.3.self_attn.attn.out_proj.weight
module.encoder.text_layers.3.self_attn.attn.out_proj.bias
module.encoder.text_layers.3.ffn.layers.0.0.weight
module.encoder.text_layers.3.ffn.layers.0.0.bias
module.encoder.text_layers.3.ffn.layers.1.weight
module.encoder.text_layers.3.ffn.layers.1.bias
module.encoder.text_layers.3.norms.0.weight
module.encoder.text_layers.3.norms.0.bias
module.encoder.text_layers.3.norms.1.weight
module.encoder.text_layers.3.norms.1.bias
module.encoder.text_layers.4.self_attn.attn.in_proj_weight
module.encoder.text_layers.4.self_attn.attn.in_proj_bias
module.encoder.text_layers.4.self_attn.attn.out_proj.weight
module.encoder.text_layers.4.self_attn.attn.out_proj.bias
module.encoder.text_layers.4.ffn.layers.0.0.weight
module.encoder.text_layers.4.ffn.layers.0.0.bias
module.encoder.text_layers.4.ffn.layers.1.weight
module.encoder.text_layers.4.ffn.layers.1.bias
module.encoder.text_layers.4.norms.0.weight
module.encoder.text_layers.4.norms.0.bias
module.encoder.text_layers.4.norms.1.weight
module.encoder.text_layers.4.norms.1.bias
module.encoder.text_layers.5.self_attn.attn.in_proj_weight
module.encoder.text_layers.5.self_attn.attn.in_proj_bias
module.encoder.text_layers.5.self_attn.attn.out_proj.weight
module.encoder.text_layers.5.self_attn.attn.out_proj.bias
module.encoder.text_layers.5.ffn.layers.0.0.weight
module.encoder.text_layers.5.ffn.layers.0.0.bias
module.encoder.text_layers.5.ffn.layers.1.weight
module.encoder.text_layers.5.ffn.layers.1.bias
module.encoder.text_layers.5.norms.0.weight
module.encoder.text_layers.5.norms.0.bias
module.encoder.text_layers.5.norms.1.weight
module.encoder.text_layers.5.norms.1.bias
module.encoder.fusion_layers.0.gamma_v
module.encoder.fusion_layers.0.gamma_l
module.encoder.fusion_layers.0.layer_norm_v.weight
module.encoder.fusion_layers.0.layer_norm_v.bias
module.encoder.fusion_layers.0.layer_norm_l.weight
module.encoder.fusion_layers.0.layer_norm_l.bias
module.encoder.fusion_layers.0.attn.v_proj.weight
module.encoder.fusion_layers.0.attn.v_proj.bias
module.encoder.fusion_layers.0.attn.l_proj.weight
module.encoder.fusion_layers.0.attn.l_proj.bias
module.encoder.fusion_layers.0.attn.values_v_proj.weight
module.encoder.fusion_layers.0.attn.values_v_proj.bias
module.encoder.fusion_layers.0.attn.values_l_proj.weight
module.encoder.fusion_layers.0.attn.values_l_proj.bias
module.encoder.fusion_layers.0.attn.out_v_proj.weight
module.encoder.fusion_layers.0.attn.out_v_proj.bias
module.encoder.fusion_layers.0.attn.out_l_proj.weight
module.encoder.fusion_layers.0.attn.out_l_proj.bias
module.encoder.fusion_layers.1.gamma_v
module.encoder.fusion_layers.1.gamma_l
module.encoder.fusion_layers.1.layer_norm_v.weight
module.encoder.fusion_layers.1.layer_norm_v.bias
module.encoder.fusion_layers.1.layer_norm_l.weight
module.encoder.fusion_layers.1.layer_norm_l.bias
module.encoder.fusion_layers.1.attn.v_proj.weight
module.encoder.fusion_layers.1.attn.v_proj.bias
module.encoder.fusion_layers.1.attn.l_proj.weight
module.encoder.fusion_layers.1.attn.l_proj.bias
module.encoder.fusion_layers.1.attn.values_v_proj.weight
module.encoder.fusion_layers.1.attn.values_v_proj.bias
module.encoder.fusion_layers.1.attn.values_l_proj.weight
module.encoder.fusion_layers.1.attn.values_l_proj.bias
module.encoder.fusion_layers.1.attn.out_v_proj.weight
module.encoder.fusion_layers.1.attn.out_v_proj.bias
module.encoder.fusion_layers.1.attn.out_l_proj.weight
module.encoder.fusion_layers.1.attn.out_l_proj.bias
module.encoder.fusion_layers.2.gamma_v
module.encoder.fusion_layers.2.gamma_l
module.encoder.fusion_layers.2.layer_norm_v.weight
module.encoder.fusion_layers.2.layer_norm_v.bias
module.encoder.fusion_layers.2.layer_norm_l.weight
module.encoder.fusion_layers.2.layer_norm_l.bias
module.encoder.fusion_layers.2.attn.v_proj.weight
module.encoder.fusion_layers.2.attn.v_proj.bias
module.encoder.fusion_layers.2.attn.l_proj.weight
module.encoder.fusion_layers.2.attn.l_proj.bias
module.encoder.fusion_layers.2.attn.values_v_proj.weight
module.encoder.fusion_layers.2.attn.values_v_proj.bias
module.encoder.fusion_layers.2.attn.values_l_proj.weight
module.encoder.fusion_layers.2.attn.values_l_proj.bias
module.encoder.fusion_layers.2.attn.out_v_proj.weight
module.encoder.fusion_layers.2.attn.out_v_proj.bias
module.encoder.fusion_layers.2.attn.out_l_proj.weight
module.encoder.fusion_layers.2.attn.out_l_proj.bias
module.encoder.fusion_layers.3.gamma_v
module.encoder.fusion_layers.3.gamma_l
module.encoder.fusion_layers.3.layer_norm_v.weight
module.encoder.fusion_layers.3.layer_norm_v.bias
module.encoder.fusion_layers.3.layer_norm_l.weight
module.encoder.fusion_layers.3.layer_norm_l.bias
module.encoder.fusion_layers.3.attn.v_proj.weight
module.encoder.fusion_layers.3.attn.v_proj.bias
module.encoder.fusion_layers.3.attn.l_proj.weight
module.encoder.fusion_layers.3.attn.l_proj.bias
module.encoder.fusion_layers.3.attn.values_v_proj.weight
module.encoder.fusion_layers.3.attn.values_v_proj.bias
module.encoder.fusion_layers.3.attn.values_l_proj.weight
module.encoder.fusion_layers.3.attn.values_l_proj.bias
module.encoder.fusion_layers.3.attn.out_v_proj.weight
module.encoder.fusion_layers.3.attn.out_v_proj.bias
module.encoder.fusion_layers.3.attn.out_l_proj.weight
module.encoder.fusion_layers.3.attn.out_l_proj.bias
module.encoder.fusion_layers.4.gamma_v
module.encoder.fusion_layers.4.gamma_l
module.encoder.fusion_layers.4.layer_norm_v.weight
module.encoder.fusion_layers.4.layer_norm_v.bias
module.encoder.fusion_layers.4.layer_norm_l.weight
module.encoder.fusion_layers.4.layer_norm_l.bias
module.encoder.fusion_layers.4.attn.v_proj.weight
module.encoder.fusion_layers.4.attn.v_proj.bias
module.encoder.fusion_layers.4.attn.l_proj.weight
module.encoder.fusion_layers.4.attn.l_proj.bias
module.encoder.fusion_layers.4.attn.values_v_proj.weight
module.encoder.fusion_layers.4.attn.values_v_proj.bias
module.encoder.fusion_layers.4.attn.values_l_proj.weight
module.encoder.fusion_layers.4.attn.values_l_proj.bias
module.encoder.fusion_layers.4.attn.out_v_proj.weight
module.encoder.fusion_layers.4.attn.out_v_proj.bias
module.encoder.fusion_layers.4.attn.out_l_proj.weight
module.encoder.fusion_layers.4.attn.out_l_proj.bias
module.encoder.fusion_layers.5.gamma_v
module.encoder.fusion_layers.5.gamma_l
module.encoder.fusion_layers.5.layer_norm_v.weight
module.encoder.fusion_layers.5.layer_norm_v.bias
module.encoder.fusion_layers.5.layer_norm_l.weight
module.encoder.fusion_layers.5.layer_norm_l.bias
module.encoder.fusion_layers.5.attn.v_proj.weight
module.encoder.fusion_layers.5.attn.v_proj.bias
module.encoder.fusion_layers.5.attn.l_proj.weight
module.encoder.fusion_layers.5.attn.l_proj.bias
module.encoder.fusion_layers.5.attn.values_v_proj.weight
module.encoder.fusion_layers.5.attn.values_v_proj.bias
module.encoder.fusion_layers.5.attn.values_l_proj.weight
module.encoder.fusion_layers.5.attn.values_l_proj.bias
module.encoder.fusion_layers.5.attn.out_v_proj.weight
module.encoder.fusion_layers.5.attn.out_v_proj.bias
module.encoder.fusion_layers.5.attn.out_l_proj.weight
module.encoder.fusion_layers.5.attn.out_l_proj.bias
module.decoder.layers.0.self_attn.attn.in_proj_weight
module.decoder.layers.0.self_attn.attn.in_proj_bias
module.decoder.layers.0.self_attn.attn.out_proj.weight
module.decoder.layers.0.self_attn.attn.out_proj.bias
module.decoder.layers.0.cross_attn_text.attn.in_proj_weight
module.decoder.layers.0.cross_attn_text.attn.in_proj_bias
module.decoder.layers.0.cross_attn_text.attn.out_proj.weight
module.decoder.layers.0.cross_attn_text.attn.out_proj.bias
module.decoder.layers.0.cross_attn.sampling_offsets.weight
module.decoder.layers.0.cross_attn.sampling_offsets.bias
module.decoder.layers.0.cross_attn.attention_weights.weight
module.decoder.layers.0.cross_attn.attention_weights.bias
module.decoder.layers.0.cross_attn.value_proj.weight
module.decoder.layers.0.cross_attn.value_proj.bias
module.decoder.layers.0.cross_attn.output_proj.weight
module.decoder.layers.0.cross_attn.output_proj.bias
module.decoder.layers.0.ffn.layers.0.0.weight
module.decoder.layers.0.ffn.layers.0.0.bias
module.decoder.layers.0.ffn.layers.1.weight
module.decoder.layers.0.ffn.layers.1.bias
module.decoder.layers.0.norms.0.weight
module.decoder.layers.0.norms.0.bias
module.decoder.layers.0.norms.1.weight
module.decoder.layers.0.norms.1.bias
module.decoder.layers.0.norms.2.weight
module.decoder.layers.0.norms.2.bias
module.decoder.layers.0.norms.3.weight
module.decoder.layers.0.norms.3.bias
module.decoder.layers.1.self_attn.attn.in_proj_weight
module.decoder.layers.1.self_attn.attn.in_proj_bias
module.decoder.layers.1.self_attn.attn.out_proj.weight
module.decoder.layers.1.self_attn.attn.out_proj.bias
module.decoder.layers.1.cross_attn_text.attn.in_proj_weight
module.decoder.layers.1.cross_attn_text.attn.in_proj_bias
module.decoder.layers.1.cross_attn_text.attn.out_proj.weight
module.decoder.layers.1.cross_attn_text.attn.out_proj.bias
module.decoder.layers.1.cross_attn.sampling_offsets.weight
module.decoder.layers.1.cross_attn.sampling_offsets.bias
module.decoder.layers.1.cross_attn.attention_weights.weight
module.decoder.layers.1.cross_attn.attention_weights.bias
module.decoder.layers.1.cross_attn.value_proj.weight
module.decoder.layers.1.cross_attn.value_proj.bias
module.decoder.layers.1.cross_attn.output_proj.weight
module.decoder.layers.1.cross_attn.output_proj.bias
module.decoder.layers.1.ffn.layers.0.0.weight
module.decoder.layers.1.ffn.layers.0.0.bias
module.decoder.layers.1.ffn.layers.1.weight
module.decoder.layers.1.ffn.layers.1.bias
module.decoder.layers.1.norms.0.weight
module.decoder.layers.1.norms.0.bias
module.decoder.layers.1.norms.1.weight
module.decoder.layers.1.norms.1.bias
module.decoder.layers.1.norms.2.weight
module.decoder.layers.1.norms.2.bias
module.decoder.layers.1.norms.3.weight
module.decoder.layers.1.norms.3.bias
module.decoder.layers.2.self_attn.attn.in_proj_weight
module.decoder.layers.2.self_attn.attn.in_proj_bias
module.decoder.layers.2.self_attn.attn.out_proj.weight
module.decoder.layers.2.self_attn.attn.out_proj.bias
module.decoder.layers.2.cross_attn_text.attn.in_proj_weight
module.decoder.layers.2.cross_attn_text.attn.in_proj_bias
module.decoder.layers.2.cross_attn_text.attn.out_proj.weight
module.decoder.layers.2.cross_attn_text.attn.out_proj.bias
module.decoder.layers.2.cross_attn.sampling_offsets.weight
module.decoder.layers.2.cross_attn.sampling_offsets.bias
module.decoder.layers.2.cross_attn.attention_weights.weight
module.decoder.layers.2.cross_attn.attention_weights.bias
module.decoder.layers.2.cross_attn.value_proj.weight
module.decoder.layers.2.cross_attn.value_proj.bias
module.decoder.layers.2.cross_attn.output_proj.weight
module.decoder.layers.2.cross_attn.output_proj.bias
module.decoder.layers.2.ffn.layers.0.0.weight
module.decoder.layers.2.ffn.layers.0.0.bias
module.decoder.layers.2.ffn.layers.1.weight
module.decoder.layers.2.ffn.layers.1.bias
module.decoder.layers.2.norms.0.weight
module.decoder.layers.2.norms.0.bias
module.decoder.layers.2.norms.1.weight
module.decoder.layers.2.norms.1.bias
module.decoder.layers.2.norms.2.weight
module.decoder.layers.2.norms.2.bias
module.decoder.layers.2.norms.3.weight
module.decoder.layers.2.norms.3.bias
module.decoder.layers.3.self_attn.attn.in_proj_weight
module.decoder.layers.3.self_attn.attn.in_proj_bias
module.decoder.layers.3.self_attn.attn.out_proj.weight
module.decoder.layers.3.self_attn.attn.out_proj.bias
module.decoder.layers.3.cross_attn_text.attn.in_proj_weight
module.decoder.layers.3.cross_attn_text.attn.in_proj_bias
module.decoder.layers.3.cross_attn_text.attn.out_proj.weight
module.decoder.layers.3.cross_attn_text.attn.out_proj.bias
module.decoder.layers.3.cross_attn.sampling_offsets.weight
module.decoder.layers.3.cross_attn.sampling_offsets.bias
module.decoder.layers.3.cross_attn.attention_weights.weight
module.decoder.layers.3.cross_attn.attention_weights.bias
module.decoder.layers.3.cross_attn.value_proj.weight
module.decoder.layers.3.cross_attn.value_proj.bias
module.decoder.layers.3.cross_attn.output_proj.weight
module.decoder.layers.3.cross_attn.output_proj.bias
module.decoder.layers.3.ffn.layers.0.0.weight
module.decoder.layers.3.ffn.layers.0.0.bias
module.decoder.layers.3.ffn.layers.1.weight
module.decoder.layers.3.ffn.layers.1.bias
module.decoder.layers.3.norms.0.weight
module.decoder.layers.3.norms.0.bias
module.decoder.layers.3.norms.1.weight
module.decoder.layers.3.norms.1.bias
module.decoder.layers.3.norms.2.weight
module.decoder.layers.3.norms.2.bias
module.decoder.layers.3.norms.3.weight
module.decoder.layers.3.norms.3.bias
module.decoder.layers.4.self_attn.attn.in_proj_weight
module.decoder.layers.4.self_attn.attn.in_proj_bias
module.decoder.layers.4.self_attn.attn.out_proj.weight
module.decoder.layers.4.self_attn.attn.out_proj.bias
module.decoder.layers.4.cross_attn_text.attn.in_proj_weight
module.decoder.layers.4.cross_attn_text.attn.in_proj_bias
module.decoder.layers.4.cross_attn_text.attn.out_proj.weight
module.decoder.layers.4.cross_attn_text.attn.out_proj.bias
module.decoder.layers.4.cross_attn.sampling_offsets.weight
module.decoder.layers.4.cross_attn.sampling_offsets.bias
module.decoder.layers.4.cross_attn.attention_weights.weight
module.decoder.layers.4.cross_attn.attention_weights.bias
module.decoder.layers.4.cross_attn.value_proj.weight
module.decoder.layers.4.cross_attn.value_proj.bias
module.decoder.layers.4.cross_attn.output_proj.weight
module.decoder.layers.4.cross_attn.output_proj.bias
module.decoder.layers.4.ffn.layers.0.0.weight
module.decoder.layers.4.ffn.layers.0.0.bias
module.decoder.layers.4.ffn.layers.1.weight
module.decoder.layers.4.ffn.layers.1.bias
module.decoder.layers.4.norms.0.weight
module.decoder.layers.4.norms.0.bias
module.decoder.layers.4.norms.1.weight
module.decoder.layers.4.norms.1.bias
module.decoder.layers.4.norms.2.weight
module.decoder.layers.4.norms.2.bias
module.decoder.layers.4.norms.3.weight
module.decoder.layers.4.norms.3.bias
module.decoder.layers.5.self_attn.attn.in_proj_weight
module.decoder.layers.5.self_attn.attn.in_proj_bias
module.decoder.layers.5.self_attn.attn.out_proj.weight
module.decoder.layers.5.self_attn.attn.out_proj.bias
module.decoder.layers.5.cross_attn_text.attn.in_proj_weight
module.decoder.layers.5.cross_attn_text.attn.in_proj_bias
module.decoder.layers.5.cross_attn_text.attn.out_proj.weight
module.decoder.layers.5.cross_attn_text.attn.out_proj.bias
module.decoder.layers.5.cross_attn.sampling_offsets.weight
module.decoder.layers.5.cross_attn.sampling_offsets.bias
module.decoder.layers.5.cross_attn.attention_weights.weight
module.decoder.layers.5.cross_attn.attention_weights.bias
module.decoder.layers.5.cross_attn.value_proj.weight
module.decoder.layers.5.cross_attn.value_proj.bias
module.decoder.layers.5.cross_attn.output_proj.weight
module.decoder.layers.5.cross_attn.output_proj.bias
module.decoder.layers.5.ffn.layers.0.0.weight
module.decoder.layers.5.ffn.layers.0.0.bias
module.decoder.layers.5.ffn.layers.1.weight
module.decoder.layers.5.ffn.layers.1.bias
module.decoder.layers.5.norms.0.weight
module.decoder.layers.5.norms.0.bias
module.decoder.layers.5.norms.1.weight
module.decoder.layers.5.norms.1.bias
module.decoder.layers.5.norms.2.weight
module.decoder.layers.5.norms.2.bias
module.decoder.layers.5.norms.3.weight
module.decoder.layers.5.norms.3.bias
module.decoder.ref_point_head.layers.0.weight
module.decoder.ref_point_head.layers.0.bias
module.decoder.ref_point_head.layers.1.weight
module.decoder.ref_point_head.layers.1.bias
module.decoder.norm.weight
module.decoder.norm.bias
module.query_embedding.weight
module.memory_trans_fc.weight
module.memory_trans_fc.bias
module.memory_trans_norm.weight
module.memory_trans_norm.bias
module.language_model.language_backbone.body.model.embeddings.word_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.position_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.weight
module.language_model.language_backbone.body.model.embeddings.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight
module.language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias
module.text_feat_map.weight
module.text_feat_map.bias
module.dn_query_generator.label_embedding.weight
loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Total annotations =  1676
Poisoned annotations =  83
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=0.0
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
09/28 09:36:49 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.0
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Total annotations =  454
Poisoned annotations =  454
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
09/28 09:36:52 - mmengine - INFO - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: language_model.language_backbone.body.model.embeddings.position_ids

09/28 09:36:55 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth
09/28 09:36:55 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
09/28 09:36:55 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
09/28 09:36:55 - mmengine - INFO - Checkpoints will be saved to /scratch/ankita/OVOD_backdoors/mmdetection/work_dirs/grounding_dino_swin-t_finetune_vehicles_backdoor.
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:38:14 - mmengine - INFO - Epoch(train)  [1][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:33:39  time: 1.5903  data_time: 0.0146  memory: 7759  grad_norm: 65.6741  loss: 7.9185  loss_cls: 0.1981  loss_bbox: 0.1881  loss_iou: 0.2319  d0.loss_cls: 0.2156  d0.loss_bbox: 0.1924  d0.loss_iou: 0.2326  d1.loss_cls: 0.2034  d1.loss_bbox: 0.1889  d1.loss_iou: 0.2331  d2.loss_cls: 0.2009  d2.loss_bbox: 0.1892  d2.loss_iou: 0.2312  d3.loss_cls: 0.2020  d3.loss_bbox: 0.1883  d3.loss_iou: 0.2321  d4.loss_cls: 0.1982  d4.loss_bbox: 0.1881  d4.loss_iou: 0.2317  enc_loss_cls: 0.2137  enc_loss_bbox: 0.2169  enc_loss_iou: 0.2562  dn_loss_cls: 0.0506  dn_loss_bbox: 0.2657  dn_loss_iou: 0.2275  d0.dn_loss_cls: 0.0697  d0.dn_loss_bbox: 0.3542  d0.dn_loss_iou: 0.3103  d1.dn_loss_cls: 0.0566  d1.dn_loss_bbox: 0.2742  d1.dn_loss_iou: 0.2420  d2.dn_loss_cls: 0.0534  d2.dn_loss_bbox: 0.2661  d2.dn_loss_iou: 0.2301  d3.dn_loss_cls: 0.0499  d3.dn_loss_bbox: 0.2657  d3.dn_loss_iou: 0.2279  d4.dn_loss_cls: 0.0492  d4.dn_loss_bbox: 0.2656  d4.dn_loss_iou: 0.2273
09/28 09:39:30 - mmengine - INFO - Epoch(train)  [1][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:31:28  time: 1.5061  data_time: 0.0110  memory: 7934  grad_norm: 53.1839  loss: 8.0625  loss_cls: 0.1895  loss_bbox: 0.2031  loss_iou: 0.2495  d0.loss_cls: 0.1950  d0.loss_bbox: 0.2204  d0.loss_iou: 0.2671  d1.loss_cls: 0.1978  d1.loss_bbox: 0.2060  d1.loss_iou: 0.2500  d2.loss_cls: 0.1915  d2.loss_bbox: 0.2041  d2.loss_iou: 0.2498  d3.loss_cls: 0.1884  d3.loss_bbox: 0.2035  d3.loss_iou: 0.2488  d4.loss_cls: 0.1881  d4.loss_bbox: 0.2030  d4.loss_iou: 0.2495  enc_loss_cls: 0.1987  enc_loss_bbox: 0.2374  enc_loss_iou: 0.2809  dn_loss_cls: 0.0242  dn_loss_bbox: 0.2600  dn_loss_iou: 0.2469  d0.dn_loss_cls: 0.0556  d0.dn_loss_bbox: 0.3492  d0.dn_loss_iou: 0.3315  d1.dn_loss_cls: 0.0324  d1.dn_loss_bbox: 0.2729  d1.dn_loss_iou: 0.2618  d2.dn_loss_cls: 0.0278  d2.dn_loss_bbox: 0.2636  d2.dn_loss_iou: 0.2499  d3.dn_loss_cls: 0.0248  d3.dn_loss_bbox: 0.2604  d3.dn_loss_iou: 0.2475  d4.dn_loss_cls: 0.0247  d4.dn_loss_bbox: 0.2600  d4.dn_loss_iou: 0.2469
09/28 09:39:45 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/28 09:39:45 - mmengine - INFO - Saving checkpoint at 1 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:40:00 - mmengine - INFO - Epoch(val)  [1][ 50/125]    eta: 0:00:09  time: 0.1264  data_time: 0.0062  memory: 7081  
09/28 09:40:06 - mmengine - INFO - Epoch(val)  [1][100/125]    eta: 0:00:03  time: 0.1208  data_time: 0.0023  memory: 2955  
09/28 09:40:11 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.40s).
Accumulating evaluation results...
DONE (t=1.40s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.706
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.852
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.774
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.601
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.785
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.510
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.751
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 09:40:15 - mmengine - INFO - bbox_mAP_copypaste: 0.706 0.852 0.774 0.152 0.601 0.785
09/28 09:40:15 - mmengine - INFO - Epoch(val) [1][125/125]    coco/bbox_mAP: 0.7060  coco/bbox_mAP_50: 0.8520  coco/bbox_mAP_75: 0.7740  coco/bbox_mAP_s: 0.1520  coco/bbox_mAP_m: 0.6010  coco/bbox_mAP_l: 0.7850  data_time: 0.0039  time: 0.1236
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
09/28 09:40:21 - mmengine - INFO - The best checkpoint with 0.7060 coco/bbox_mAP at 1 epoch is saved to best_coco_bbox_mAP_epoch_1.pth.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:41:47 - mmengine - INFO - Epoch(train)  [2][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:29:53  time: 1.5443  data_time: 0.0115  memory: 8297  grad_norm: 51.8895  loss: 7.0022  loss_cls: 0.1550  loss_bbox: 0.1875  loss_iou: 0.2356  d0.loss_cls: 0.1626  d0.loss_bbox: 0.1984  d0.loss_iou: 0.2439  d1.loss_cls: 0.1652  d1.loss_bbox: 0.1856  d1.loss_iou: 0.2320  d2.loss_cls: 0.1611  d2.loss_bbox: 0.1845  d2.loss_iou: 0.2348  d3.loss_cls: 0.1600  d3.loss_bbox: 0.1855  d3.loss_iou: 0.2362  d4.loss_cls: 0.1565  d4.loss_bbox: 0.1872  d4.loss_iou: 0.2353  enc_loss_cls: 0.1685  enc_loss_bbox: 0.2143  enc_loss_iou: 0.2587  dn_loss_cls: 0.0100  dn_loss_bbox: 0.2159  dn_loss_iou: 0.2073  d0.dn_loss_cls: 0.0369  d0.dn_loss_bbox: 0.3089  d0.dn_loss_iou: 0.2942  d1.dn_loss_cls: 0.0154  d1.dn_loss_bbox: 0.2315  d1.dn_loss_iou: 0.2241  d2.dn_loss_cls: 0.0118  d2.dn_loss_bbox: 0.2184  d2.dn_loss_iou: 0.2104  d3.dn_loss_cls: 0.0105  d3.dn_loss_bbox: 0.2168  d3.dn_loss_iou: 0.2083  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.2159  d4.dn_loss_iou: 0.2073
09/28 09:43:05 - mmengine - INFO - Epoch(train)  [2][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:28:38  time: 1.5540  data_time: 0.0105  memory: 7765  grad_norm: 60.5702  loss: 6.5836  loss_cls: 0.1300  loss_bbox: 0.1799  loss_iou: 0.2080  d0.loss_cls: 0.1362  d0.loss_bbox: 0.1933  d0.loss_iou: 0.2168  d1.loss_cls: 0.1333  d1.loss_bbox: 0.1797  d1.loss_iou: 0.2082  d2.loss_cls: 0.1359  d2.loss_bbox: 0.1776  d2.loss_iou: 0.2053  d3.loss_cls: 0.1372  d3.loss_bbox: 0.1760  d3.loss_iou: 0.2055  d4.loss_cls: 0.1336  d4.loss_bbox: 0.1774  d4.loss_iou: 0.2070  enc_loss_cls: 0.1472  enc_loss_bbox: 0.2080  enc_loss_iou: 0.2357  dn_loss_cls: 0.0092  dn_loss_bbox: 0.2297  dn_loss_iou: 0.1959  d0.dn_loss_cls: 0.0331  d0.dn_loss_bbox: 0.3312  d0.dn_loss_iou: 0.2759  d1.dn_loss_cls: 0.0128  d1.dn_loss_bbox: 0.2427  d1.dn_loss_iou: 0.2097  d2.dn_loss_cls: 0.0102  d2.dn_loss_bbox: 0.2324  d2.dn_loss_iou: 0.1990  d3.dn_loss_cls: 0.0093  d3.dn_loss_bbox: 0.2300  d3.dn_loss_iou: 0.1962  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.2296  d4.dn_loss_iou: 0.1958
09/28 09:43:20 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 09:43:20 - mmengine - INFO - Saving checkpoint at 2 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:43:35 - mmengine - INFO - Epoch(val)  [2][ 50/125]    eta: 0:00:09  time: 0.1274  data_time: 0.0031  memory: 7747  
09/28 09:43:41 - mmengine - INFO - Epoch(val)  [2][100/125]    eta: 0:00:03  time: 0.1219  data_time: 0.0024  memory: 2955  
09/28 09:43:46 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.13s).
Accumulating evaluation results...
DONE (t=1.43s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.688
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.836
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.756
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.497
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.764
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.520
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.779
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.898
09/28 09:43:50 - mmengine - INFO - bbox_mAP_copypaste: 0.688 0.836 0.756 0.170 0.497 0.764
09/28 09:43:50 - mmengine - INFO - Epoch(val) [2][125/125]    coco/bbox_mAP: 0.6880  coco/bbox_mAP_50: 0.8360  coco/bbox_mAP_75: 0.7560  coco/bbox_mAP_s: 0.1700  coco/bbox_mAP_m: 0.4970  coco/bbox_mAP_l: 0.7640  data_time: 0.0027  time: 0.1241
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:45:08 - mmengine - INFO - Epoch(train)  [3][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:27:06  time: 1.5619  data_time: 0.0114  memory: 8335  grad_norm: 53.3732  loss: 7.2775  loss_cls: 0.1689  loss_bbox: 0.1841  loss_iou: 0.2705  d0.loss_cls: 0.1790  d0.loss_bbox: 0.1928  d0.loss_iou: 0.2808  d1.loss_cls: 0.1799  d1.loss_bbox: 0.1805  d1.loss_iou: 0.2655  d2.loss_cls: 0.1695  d2.loss_bbox: 0.1832  d2.loss_iou: 0.2685  d3.loss_cls: 0.1679  d3.loss_bbox: 0.1839  d3.loss_iou: 0.2697  d4.loss_cls: 0.1673  d4.loss_bbox: 0.1840  d4.loss_iou: 0.2707  enc_loss_cls: 0.1872  enc_loss_bbox: 0.2024  enc_loss_iou: 0.2893  dn_loss_cls: 0.0069  dn_loss_bbox: 0.2047  dn_loss_iou: 0.2215  d0.dn_loss_cls: 0.0318  d0.dn_loss_bbox: 0.2887  d0.dn_loss_iou: 0.3037  d1.dn_loss_cls: 0.0114  d1.dn_loss_bbox: 0.2195  d1.dn_loss_iou: 0.2367  d2.dn_loss_cls: 0.0082  d2.dn_loss_bbox: 0.2079  d2.dn_loss_iou: 0.2242  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.2049  d3.dn_loss_iou: 0.2215  d4.dn_loss_cls: 0.0070  d4.dn_loss_bbox: 0.2046  d4.dn_loss_iou: 0.2214
09/28 09:46:26 - mmengine - INFO - Epoch(train)  [3][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:25:50  time: 1.5599  data_time: 0.0107  memory: 8094  grad_norm: 52.0355  loss: 6.9270  loss_cls: 0.1492  loss_bbox: 0.2001  loss_iou: 0.2339  d0.loss_cls: 0.1531  d0.loss_bbox: 0.2232  d0.loss_iou: 0.2485  d1.loss_cls: 0.1492  d1.loss_bbox: 0.2137  d1.loss_iou: 0.2398  d2.loss_cls: 0.1528  d2.loss_bbox: 0.2013  d2.loss_iou: 0.2328  d3.loss_cls: 0.1533  d3.loss_bbox: 0.1988  d3.loss_iou: 0.2317  d4.loss_cls: 0.1502  d4.loss_bbox: 0.1985  d4.loss_iou: 0.2331  enc_loss_cls: 0.1662  enc_loss_bbox: 0.2448  enc_loss_iou: 0.2738  dn_loss_cls: 0.0044  dn_loss_bbox: 0.2068  dn_loss_iou: 0.2001  d0.dn_loss_cls: 0.0245  d0.dn_loss_bbox: 0.2876  d0.dn_loss_iou: 0.2736  d1.dn_loss_cls: 0.0072  d1.dn_loss_bbox: 0.2204  d1.dn_loss_iou: 0.2144  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.2082  d2.dn_loss_iou: 0.2027  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.2072  d3.dn_loss_iou: 0.2006  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.2067  d4.dn_loss_iou: 0.2000
09/28 09:46:41 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 09:46:41 - mmengine - INFO - Saving checkpoint at 3 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:46:56 - mmengine - INFO - Epoch(val)  [3][ 50/125]    eta: 0:00:09  time: 0.1248  data_time: 0.0031  memory: 6950  
09/28 09:47:02 - mmengine - INFO - Epoch(val)  [3][100/125]    eta: 0:00:03  time: 0.1224  data_time: 0.0024  memory: 2955  
09/28 09:47:07 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.65s).
Accumulating evaluation results...
DONE (t=1.41s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.695
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.850
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.769
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.156
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.510
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.773
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.538
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.900
09/28 09:47:11 - mmengine - INFO - bbox_mAP_copypaste: 0.695 0.850 0.769 0.156 0.510 0.773
09/28 09:47:11 - mmengine - INFO - Epoch(val) [3][125/125]    coco/bbox_mAP: 0.6950  coco/bbox_mAP_50: 0.8500  coco/bbox_mAP_75: 0.7690  coco/bbox_mAP_s: 0.1560  coco/bbox_mAP_m: 0.5100  coco/bbox_mAP_l: 0.7730  data_time: 0.0026  time: 0.1236
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:48:29 - mmengine - INFO - Epoch(train)  [4][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:24:15  time: 1.5448  data_time: 0.0114  memory: 8167  grad_norm: 56.3751  loss: 6.6113  loss_cls: 0.1244  loss_bbox: 0.2017  loss_iou: 0.2292  d0.loss_cls: 0.1315  d0.loss_bbox: 0.2100  d0.loss_iou: 0.2401  d1.loss_cls: 0.1281  d1.loss_bbox: 0.2059  d1.loss_iou: 0.2374  d2.loss_cls: 0.1268  d2.loss_bbox: 0.2035  d2.loss_iou: 0.2299  d3.loss_cls: 0.1242  d3.loss_bbox: 0.2031  d3.loss_iou: 0.2297  d4.loss_cls: 0.1235  d4.loss_bbox: 0.2028  d4.loss_iou: 0.2286  enc_loss_cls: 0.1437  enc_loss_bbox: 0.2225  enc_loss_iou: 0.2546  dn_loss_cls: 0.0025  dn_loss_bbox: 0.2050  dn_loss_iou: 0.1954  d0.dn_loss_cls: 0.0228  d0.dn_loss_bbox: 0.2777  d0.dn_loss_iou: 0.2637  d1.dn_loss_cls: 0.0056  d1.dn_loss_bbox: 0.2153  d1.dn_loss_iou: 0.2077  d2.dn_loss_cls: 0.0037  d2.dn_loss_bbox: 0.2067  d2.dn_loss_iou: 0.1981  d3.dn_loss_cls: 0.0027  d3.dn_loss_bbox: 0.2048  d3.dn_loss_iou: 0.1956  d4.dn_loss_cls: 0.0027  d4.dn_loss_bbox: 0.2048  d4.dn_loss_iou: 0.1953
09/28 09:49:46 - mmengine - INFO - Epoch(train)  [4][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:22:57  time: 1.5473  data_time: 0.0112  memory: 8690  grad_norm: 52.6683  loss: 6.6493  loss_cls: 0.1317  loss_bbox: 0.1765  loss_iou: 0.2499  d0.loss_cls: 0.1353  d0.loss_bbox: 0.1934  d0.loss_iou: 0.2677  d1.loss_cls: 0.1351  d1.loss_bbox: 0.1781  d1.loss_iou: 0.2548  d2.loss_cls: 0.1353  d2.loss_bbox: 0.1721  d2.loss_iou: 0.2466  d3.loss_cls: 0.1343  d3.loss_bbox: 0.1731  d3.loss_iou: 0.2484  d4.loss_cls: 0.1312  d4.loss_bbox: 0.1763  d4.loss_iou: 0.2508  enc_loss_cls: 0.1584  enc_loss_bbox: 0.2088  enc_loss_iou: 0.2840  dn_loss_cls: 0.0035  dn_loss_bbox: 0.1845  dn_loss_iou: 0.2079  d0.dn_loss_cls: 0.0221  d0.dn_loss_bbox: 0.2719  d0.dn_loss_iou: 0.2933  d1.dn_loss_cls: 0.0066  d1.dn_loss_bbox: 0.1980  d1.dn_loss_iou: 0.2228  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.1880  d2.dn_loss_iou: 0.2117  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.1850  d3.dn_loss_iou: 0.2082  d4.dn_loss_cls: 0.0036  d4.dn_loss_bbox: 0.1845  d4.dn_loss_iou: 0.2078
09/28 09:50:01 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 09:50:01 - mmengine - INFO - Saving checkpoint at 4 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:50:16 - mmengine - INFO - Epoch(val)  [4][ 50/125]    eta: 0:00:09  time: 0.1243  data_time: 0.0033  memory: 8248  
09/28 09:50:22 - mmengine - INFO - Epoch(val)  [4][100/125]    eta: 0:00:03  time: 0.1220  data_time: 0.0024  memory: 2955  
09/28 09:50:27 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.31s).
Accumulating evaluation results...
DONE (t=1.40s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.696
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.842
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.769
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.444
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.772
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.860
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.530
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.774
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.899
09/28 09:50:31 - mmengine - INFO - bbox_mAP_copypaste: 0.696 0.842 0.769 0.138 0.444 0.772
09/28 09:50:31 - mmengine - INFO - Epoch(val) [4][125/125]    coco/bbox_mAP: 0.6960  coco/bbox_mAP_50: 0.8420  coco/bbox_mAP_75: 0.7690  coco/bbox_mAP_s: 0.1380  coco/bbox_mAP_m: 0.4440  coco/bbox_mAP_l: 0.7720  data_time: 0.0028  time: 0.1226
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:51:48 - mmengine - INFO - Epoch(train)  [5][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:21:24  time: 1.5448  data_time: 0.0114  memory: 7765  grad_norm: 61.5262  loss: 5.6092  loss_cls: 0.0976  loss_bbox: 0.1506  loss_iou: 0.1960  d0.loss_cls: 0.1061  d0.loss_bbox: 0.1597  d0.loss_iou: 0.2110  d1.loss_cls: 0.1032  d1.loss_bbox: 0.1523  d1.loss_iou: 0.1997  d2.loss_cls: 0.1021  d2.loss_bbox: 0.1516  d2.loss_iou: 0.1972  d3.loss_cls: 0.1006  d3.loss_bbox: 0.1502  d3.loss_iou: 0.1967  d4.loss_cls: 0.0985  d4.loss_bbox: 0.1504  d4.loss_iou: 0.1960  enc_loss_cls: 0.1132  enc_loss_bbox: 0.1812  enc_loss_iou: 0.2289  dn_loss_cls: 0.0025  dn_loss_bbox: 0.1841  dn_loss_iou: 0.1787  d0.dn_loss_cls: 0.0183  d0.dn_loss_bbox: 0.2516  d0.dn_loss_iou: 0.2380  d1.dn_loss_cls: 0.0042  d1.dn_loss_bbox: 0.1941  d1.dn_loss_iou: 0.1887  d2.dn_loss_cls: 0.0031  d2.dn_loss_bbox: 0.1887  d2.dn_loss_iou: 0.1820  d3.dn_loss_cls: 0.0027  d3.dn_loss_bbox: 0.1850  d3.dn_loss_iou: 0.1792  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.1840  d4.dn_loss_iou: 0.1786
09/28 09:53:04 - mmengine - INFO - Epoch(train)  [5][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:20:04  time: 1.5089  data_time: 0.0112  memory: 8648  grad_norm: 50.5305  loss: 5.9066  loss_cls: 0.1149  loss_bbox: 0.1532  loss_iou: 0.2235  d0.loss_cls: 0.1247  d0.loss_bbox: 0.1550  d0.loss_iou: 0.2315  d1.loss_cls: 0.1211  d1.loss_bbox: 0.1505  d1.loss_iou: 0.2243  d2.loss_cls: 0.1166  d2.loss_bbox: 0.1511  d2.loss_iou: 0.2225  d3.loss_cls: 0.1150  d3.loss_bbox: 0.1530  d3.loss_iou: 0.2233  d4.loss_cls: 0.1151  d4.loss_bbox: 0.1532  d4.loss_iou: 0.2235  enc_loss_cls: 0.1357  enc_loss_bbox: 0.1737  enc_loss_iou: 0.2461  dn_loss_cls: 0.0043  dn_loss_bbox: 0.1673  dn_loss_iou: 0.1946  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.2278  d0.dn_loss_iou: 0.2593  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.1765  d1.dn_loss_iou: 0.2064  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.1683  d2.dn_loss_iou: 0.1970  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.1671  d3.dn_loss_iou: 0.1949  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.1672  d4.dn_loss_iou: 0.1945
09/28 09:53:20 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 09:53:20 - mmengine - INFO - Saving checkpoint at 5 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:53:35 - mmengine - INFO - Epoch(val)  [5][ 50/125]    eta: 0:00:09  time: 0.1268  data_time: 0.0032  memory: 8162  
09/28 09:53:41 - mmengine - INFO - Epoch(val)  [5][100/125]    eta: 0:00:03  time: 0.1241  data_time: 0.0025  memory: 2955  
09/28 09:53:45 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.35s).
Accumulating evaluation results...
DONE (t=1.37s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.700
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.853
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.788
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.135
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.553
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.775
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.855
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.543
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.744
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.898
09/28 09:53:50 - mmengine - INFO - bbox_mAP_copypaste: 0.700 0.853 0.788 0.135 0.553 0.775
09/28 09:53:50 - mmengine - INFO - Epoch(val) [5][125/125]    coco/bbox_mAP: 0.7000  coco/bbox_mAP_50: 0.8530  coco/bbox_mAP_75: 0.7880  coco/bbox_mAP_s: 0.1350  coco/bbox_mAP_m: 0.5530  coco/bbox_mAP_l: 0.7750  data_time: 0.0027  time: 0.1246
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:55:07 - mmengine - INFO - Epoch(train)  [6][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:18:31  time: 1.5409  data_time: 0.0119  memory: 8498  grad_norm: 52.2690  loss: 5.5818  loss_cls: 0.0919  loss_bbox: 0.1647  loss_iou: 0.1959  d0.loss_cls: 0.1000  d0.loss_bbox: 0.1747  d0.loss_iou: 0.2037  d1.loss_cls: 0.1055  d1.loss_bbox: 0.1656  d1.loss_iou: 0.1966  d2.loss_cls: 0.0984  d2.loss_bbox: 0.1657  d2.loss_iou: 0.1970  d3.loss_cls: 0.0955  d3.loss_bbox: 0.1641  d3.loss_iou: 0.1942  d4.loss_cls: 0.0927  d4.loss_bbox: 0.1646  d4.loss_iou: 0.1959  enc_loss_cls: 0.1191  enc_loss_bbox: 0.1894  enc_loss_iou: 0.2196  dn_loss_cls: 0.0026  dn_loss_bbox: 0.1808  dn_loss_iou: 0.1682  d0.dn_loss_cls: 0.0174  d0.dn_loss_bbox: 0.2514  d0.dn_loss_iou: 0.2312  d1.dn_loss_cls: 0.0045  d1.dn_loss_bbox: 0.1906  d1.dn_loss_iou: 0.1782  d2.dn_loss_cls: 0.0033  d2.dn_loss_bbox: 0.1834  d2.dn_loss_iou: 0.1714  d3.dn_loss_cls: 0.0028  d3.dn_loss_bbox: 0.1812  d3.dn_loss_iou: 0.1685  d4.dn_loss_cls: 0.0027  d4.dn_loss_bbox: 0.1807  d4.dn_loss_iou: 0.1681
09/28 09:56:24 - mmengine - INFO - Epoch(train)  [6][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:17:14  time: 1.5430  data_time: 0.0114  memory: 7804  grad_norm: 48.9054  loss: 5.9812  loss_cls: 0.1140  loss_bbox: 0.1648  loss_iou: 0.2108  d0.loss_cls: 0.1249  d0.loss_bbox: 0.1812  d0.loss_iou: 0.2203  d1.loss_cls: 0.1207  d1.loss_bbox: 0.1719  d1.loss_iou: 0.2135  d2.loss_cls: 0.1213  d2.loss_bbox: 0.1644  d2.loss_iou: 0.2104  d3.loss_cls: 0.1164  d3.loss_bbox: 0.1638  d3.loss_iou: 0.2105  d4.loss_cls: 0.1139  d4.loss_bbox: 0.1648  d4.loss_iou: 0.2108  enc_loss_cls: 0.1401  enc_loss_bbox: 0.1924  enc_loss_iou: 0.2414  dn_loss_cls: 0.0032  dn_loss_bbox: 0.1782  dn_loss_iou: 0.1853  d0.dn_loss_cls: 0.0209  d0.dn_loss_bbox: 0.2572  d0.dn_loss_iou: 0.2584  d1.dn_loss_cls: 0.0066  d1.dn_loss_bbox: 0.1926  d1.dn_loss_iou: 0.1978  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.1825  d2.dn_loss_iou: 0.1882  d3.dn_loss_cls: 0.0034  d3.dn_loss_bbox: 0.1782  d3.dn_loss_iou: 0.1854  d4.dn_loss_cls: 0.0033  d4.dn_loss_bbox: 0.1781  d4.dn_loss_iou: 0.1852
09/28 09:56:39 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 09:56:39 - mmengine - INFO - Saving checkpoint at 6 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 09:56:55 - mmengine - INFO - Epoch(val)  [6][ 50/125]    eta: 0:00:09  time: 0.1228  data_time: 0.0031  memory: 7870  
09/28 09:57:01 - mmengine - INFO - Epoch(val)  [6][100/125]    eta: 0:00:03  time: 0.1243  data_time: 0.0025  memory: 2955  
09/28 09:57:06 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.41s).
Accumulating evaluation results...
DONE (t=1.36s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.684
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.839
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.773
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.460
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.759
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.503
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.761
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.894
09/28 09:57:10 - mmengine - INFO - bbox_mAP_copypaste: 0.684 0.839 0.773 0.129 0.460 0.759
09/28 09:57:10 - mmengine - INFO - Epoch(val) [6][125/125]    coco/bbox_mAP: 0.6840  coco/bbox_mAP_50: 0.8390  coco/bbox_mAP_75: 0.7730  coco/bbox_mAP_s: 0.1290  coco/bbox_mAP_m: 0.4600  coco/bbox_mAP_l: 0.7590  data_time: 0.0027  time: 0.1234
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 09:58:28 - mmengine - INFO - Epoch(train)  [7][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:15:42  time: 1.5609  data_time: 0.0120  memory: 8409  grad_norm: 56.7248  loss: 6.0394  loss_cls: 0.1106  loss_bbox: 0.1755  loss_iou: 0.2239  d0.loss_cls: 0.1212  d0.loss_bbox: 0.1846  d0.loss_iou: 0.2349  d1.loss_cls: 0.1204  d1.loss_bbox: 0.1755  d1.loss_iou: 0.2278  d2.loss_cls: 0.1151  d2.loss_bbox: 0.1757  d2.loss_iou: 0.2238  d3.loss_cls: 0.1132  d3.loss_bbox: 0.1749  d3.loss_iou: 0.2237  d4.loss_cls: 0.1112  d4.loss_bbox: 0.1751  d4.loss_iou: 0.2229  enc_loss_cls: 0.1409  enc_loss_bbox: 0.2057  enc_loss_iou: 0.2580  dn_loss_cls: 0.0086  dn_loss_bbox: 0.1716  dn_loss_iou: 0.1775  d0.dn_loss_cls: 0.0223  d0.dn_loss_bbox: 0.2433  d0.dn_loss_iou: 0.2463  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.1848  d1.dn_loss_iou: 0.1897  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.1732  d2.dn_loss_iou: 0.1794  d3.dn_loss_cls: 0.0070  d3.dn_loss_bbox: 0.1716  d3.dn_loss_iou: 0.1778  d4.dn_loss_cls: 0.0081  d4.dn_loss_bbox: 0.1716  d4.dn_loss_iou: 0.1776
09/28 09:59:45 - mmengine - INFO - Epoch(train)  [7][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:14:24  time: 1.5271  data_time: 0.0114  memory: 8625  grad_norm: 46.4262  loss: 5.7271  loss_cls: 0.0909  loss_bbox: 0.1612  loss_iou: 0.2095  d0.loss_cls: 0.1015  d0.loss_bbox: 0.1688  d0.loss_iou: 0.2145  d1.loss_cls: 0.0973  d1.loss_bbox: 0.1656  d1.loss_iou: 0.2095  d2.loss_cls: 0.0943  d2.loss_bbox: 0.1635  d2.loss_iou: 0.2105  d3.loss_cls: 0.0949  d3.loss_bbox: 0.1611  d3.loss_iou: 0.2088  d4.loss_cls: 0.0922  d4.loss_bbox: 0.1610  d4.loss_iou: 0.2086  enc_loss_cls: 0.1227  enc_loss_bbox: 0.1903  enc_loss_iou: 0.2354  dn_loss_cls: 0.0022  dn_loss_bbox: 0.1713  dn_loss_iou: 0.1898  d0.dn_loss_cls: 0.0167  d0.dn_loss_bbox: 0.2412  d0.dn_loss_iou: 0.2600  d1.dn_loss_cls: 0.0040  d1.dn_loss_bbox: 0.1808  d1.dn_loss_iou: 0.2013  d2.dn_loss_cls: 0.0031  d2.dn_loss_bbox: 0.1741  d2.dn_loss_iou: 0.1924  d3.dn_loss_cls: 0.0025  d3.dn_loss_bbox: 0.1720  d3.dn_loss_iou: 0.1902  d4.dn_loss_cls: 0.0022  d4.dn_loss_bbox: 0.1713  d4.dn_loss_iou: 0.1897
09/28 10:00:00 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:00:00 - mmengine - INFO - Saving checkpoint at 7 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:00:15 - mmengine - INFO - Epoch(val)  [7][ 50/125]    eta: 0:00:09  time: 0.1231  data_time: 0.0030  memory: 8007  
09/28 10:00:21 - mmengine - INFO - Epoch(val)  [7][100/125]    eta: 0:00:03  time: 0.1242  data_time: 0.0025  memory: 2955  
09/28 10:00:25 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.47s).
Accumulating evaluation results...
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.671
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.810
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.748
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.161
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.407
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.750
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.423
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.766
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.902
09/28 10:00:30 - mmengine - INFO - bbox_mAP_copypaste: 0.671 0.810 0.748 0.161 0.407 0.750
09/28 10:00:30 - mmengine - INFO - Epoch(val) [7][125/125]    coco/bbox_mAP: 0.6710  coco/bbox_mAP_50: 0.8100  coco/bbox_mAP_75: 0.7480  coco/bbox_mAP_s: 0.1610  coco/bbox_mAP_m: 0.4070  coco/bbox_mAP_l: 0.7500  data_time: 0.0027  time: 0.1235
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 10:01:47 - mmengine - INFO - Epoch(train)  [8][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:12:51  time: 1.5396  data_time: 0.0117  memory: 7391  grad_norm: 49.5907  loss: 5.1611  loss_cls: 0.0863  loss_bbox: 0.1377  loss_iou: 0.1726  d0.loss_cls: 0.0986  d0.loss_bbox: 0.1475  d0.loss_iou: 0.1805  d1.loss_cls: 0.0973  d1.loss_bbox: 0.1388  d1.loss_iou: 0.1727  d2.loss_cls: 0.0904  d2.loss_bbox: 0.1409  d2.loss_iou: 0.1745  d3.loss_cls: 0.0891  d3.loss_bbox: 0.1384  d3.loss_iou: 0.1735  d4.loss_cls: 0.0868  d4.loss_bbox: 0.1377  d4.loss_iou: 0.1725  enc_loss_cls: 0.1219  enc_loss_bbox: 0.1640  enc_loss_iou: 0.2006  dn_loss_cls: 0.0035  dn_loss_bbox: 0.1708  dn_loss_iou: 0.1684  d0.dn_loss_cls: 0.0189  d0.dn_loss_bbox: 0.2419  d0.dn_loss_iou: 0.2347  d1.dn_loss_cls: 0.0061  d1.dn_loss_bbox: 0.1802  d1.dn_loss_iou: 0.1801  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.1722  d2.dn_loss_iou: 0.1708  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.1708  d3.dn_loss_iou: 0.1685  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.1708  d4.dn_loss_iou: 0.1683
09/28 10:03:03 - mmengine - INFO - Epoch(train)  [8][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:11:34  time: 1.5335  data_time: 0.0110  memory: 8322  grad_norm: 49.3389  loss: 5.5663  loss_cls: 0.0772  loss_bbox: 0.1663  loss_iou: 0.2197  d0.loss_cls: 0.0862  d0.loss_bbox: 0.1733  d0.loss_iou: 0.2239  d1.loss_cls: 0.0835  d1.loss_bbox: 0.1693  d1.loss_iou: 0.2200  d2.loss_cls: 0.0774  d2.loss_bbox: 0.1690  d2.loss_iou: 0.2197  d3.loss_cls: 0.0791  d3.loss_bbox: 0.1664  d3.loss_iou: 0.2182  d4.loss_cls: 0.0765  d4.loss_bbox: 0.1668  d4.loss_iou: 0.2195  enc_loss_cls: 0.1114  enc_loss_bbox: 0.1883  enc_loss_iou: 0.2462  dn_loss_cls: 0.0015  dn_loss_bbox: 0.1646  dn_loss_iou: 0.1734  d0.dn_loss_cls: 0.0147  d0.dn_loss_bbox: 0.2346  d0.dn_loss_iou: 0.2354  d1.dn_loss_cls: 0.0028  d1.dn_loss_bbox: 0.1749  d1.dn_loss_iou: 0.1829  d2.dn_loss_cls: 0.0018  d2.dn_loss_bbox: 0.1675  d2.dn_loss_iou: 0.1758  d3.dn_loss_cls: 0.0016  d3.dn_loss_bbox: 0.1640  d3.dn_loss_iou: 0.1735  d4.dn_loss_cls: 0.0016  d4.dn_loss_bbox: 0.1644  d4.dn_loss_iou: 0.1734
09/28 10:03:19 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:03:19 - mmengine - INFO - Saving checkpoint at 8 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:03:34 - mmengine - INFO - Epoch(val)  [8][ 50/125]    eta: 0:00:09  time: 0.1223  data_time: 0.0030  memory: 7445  
09/28 10:03:40 - mmengine - INFO - Epoch(val)  [8][100/125]    eta: 0:00:03  time: 0.1231  data_time: 0.0024  memory: 2955  
09/28 10:03:45 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.29s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.46s).
Accumulating evaluation results...
DONE (t=1.38s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.827
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.774
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.112
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.392
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.757
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.856
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.475
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.765
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.902
09/28 10:03:49 - mmengine - INFO - bbox_mAP_copypaste: 0.681 0.827 0.774 0.112 0.392 0.757
09/28 10:03:49 - mmengine - INFO - Epoch(val) [8][125/125]    coco/bbox_mAP: 0.6810  coco/bbox_mAP_50: 0.8270  coco/bbox_mAP_75: 0.7740  coco/bbox_mAP_s: 0.1120  coco/bbox_mAP_m: 0.3920  coco/bbox_mAP_l: 0.7570  data_time: 0.0026  time: 0.1226
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 10:05:07 - mmengine - INFO - Epoch(train)  [9][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:10:01  time: 1.5597  data_time: 0.0119  memory: 8121  grad_norm: 55.0220  loss: 5.0183  loss_cls: 0.0669  loss_bbox: 0.1434  loss_iou: 0.1944  d0.loss_cls: 0.0804  d0.loss_bbox: 0.1480  d0.loss_iou: 0.1997  d1.loss_cls: 0.0777  d1.loss_bbox: 0.1451  d1.loss_iou: 0.1968  d2.loss_cls: 0.0701  d2.loss_bbox: 0.1453  d2.loss_iou: 0.1967  d3.loss_cls: 0.0680  d3.loss_bbox: 0.1434  d3.loss_iou: 0.1944  d4.loss_cls: 0.0670  d4.loss_bbox: 0.1435  d4.loss_iou: 0.1947  enc_loss_cls: 0.0933  enc_loss_bbox: 0.1685  enc_loss_iou: 0.2185  dn_loss_cls: 0.0058  dn_loss_bbox: 0.1440  dn_loss_iou: 0.1670  d0.dn_loss_cls: 0.0208  d0.dn_loss_bbox: 0.2012  d0.dn_loss_iou: 0.2259  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.1540  d1.dn_loss_iou: 0.1772  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.1466  d2.dn_loss_iou: 0.1696  d3.dn_loss_cls: 0.0065  d3.dn_loss_bbox: 0.1443  d3.dn_loss_iou: 0.1675  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.1438  d4.dn_loss_iou: 0.1669
09/28 10:06:23 - mmengine - INFO - Epoch(train)  [9][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:08:44  time: 1.5186  data_time: 0.0110  memory: 7681  grad_norm: 50.5233  loss: 5.2641  loss_cls: 0.0724  loss_bbox: 0.1547  loss_iou: 0.2097  d0.loss_cls: 0.0924  d0.loss_bbox: 0.1553  d0.loss_iou: 0.2120  d1.loss_cls: 0.0882  d1.loss_bbox: 0.1537  d1.loss_iou: 0.2062  d2.loss_cls: 0.0790  d2.loss_bbox: 0.1541  d2.loss_iou: 0.2104  d3.loss_cls: 0.0773  d3.loss_bbox: 0.1519  d3.loss_iou: 0.2083  d4.loss_cls: 0.0761  d4.loss_bbox: 0.1514  d4.loss_iou: 0.2081  enc_loss_cls: 0.1093  enc_loss_bbox: 0.1779  enc_loss_iou: 0.2326  dn_loss_cls: 0.0021  dn_loss_bbox: 0.1449  dn_loss_iou: 0.1677  d0.dn_loss_cls: 0.0195  d0.dn_loss_bbox: 0.2225  d0.dn_loss_iou: 0.2347  d1.dn_loss_cls: 0.0049  d1.dn_loss_bbox: 0.1563  d1.dn_loss_iou: 0.1800  d2.dn_loss_cls: 0.0030  d2.dn_loss_bbox: 0.1469  d2.dn_loss_iou: 0.1702  d3.dn_loss_cls: 0.0023  d3.dn_loss_bbox: 0.1452  d3.dn_loss_iou: 0.1679  d4.dn_loss_cls: 0.0022  d4.dn_loss_bbox: 0.1449  d4.dn_loss_iou: 0.1677
09/28 10:06:38 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:06:38 - mmengine - INFO - Saving checkpoint at 9 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:06:54 - mmengine - INFO - Epoch(val)  [9][ 50/125]    eta: 0:00:10  time: 0.1402  data_time: 0.0032  memory: 7356  
09/28 10:07:00 - mmengine - INFO - Epoch(val)  [9][100/125]    eta: 0:00:03  time: 0.1232  data_time: 0.0024  memory: 2955  
09/28 10:07:04 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.37s).
Accumulating evaluation results...
DONE (t=1.37s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.827
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.764
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.106
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.445
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.854
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.857
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.857
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.440
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.757
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.904
09/28 10:07:09 - mmengine - INFO - bbox_mAP_copypaste: 0.681 0.827 0.764 0.106 0.445 0.760
09/28 10:07:09 - mmengine - INFO - Epoch(val) [9][125/125]    coco/bbox_mAP: 0.6810  coco/bbox_mAP_50: 0.8270  coco/bbox_mAP_75: 0.7640  coco/bbox_mAP_s: 0.1060  coco/bbox_mAP_m: 0.4450  coco/bbox_mAP_l: 0.7600  data_time: 0.0027  time: 0.1298
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 10:07:25 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:08:25 - mmengine - INFO - Epoch(train) [10][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:07:11  time: 1.5252  data_time: 0.0120  memory: 7327  grad_norm: 62.6530  loss: 4.9422  loss_cls: 0.0642  loss_bbox: 0.1523  loss_iou: 0.1898  d0.loss_cls: 0.0846  d0.loss_bbox: 0.1498  d0.loss_iou: 0.1870  d1.loss_cls: 0.0810  d1.loss_bbox: 0.1431  d1.loss_iou: 0.1826  d2.loss_cls: 0.0766  d2.loss_bbox: 0.1460  d2.loss_iou: 0.1848  d3.loss_cls: 0.0687  d3.loss_bbox: 0.1506  d3.loss_iou: 0.1874  d4.loss_cls: 0.0673  d4.loss_bbox: 0.1505  d4.loss_iou: 0.1873  enc_loss_cls: 0.0964  enc_loss_bbox: 0.1831  enc_loss_iou: 0.2090  dn_loss_cls: 0.0015  dn_loss_bbox: 0.1508  dn_loss_iou: 0.1522  d0.dn_loss_cls: 0.0151  d0.dn_loss_bbox: 0.2240  d0.dn_loss_iou: 0.2140  d1.dn_loss_cls: 0.0031  d1.dn_loss_bbox: 0.1599  d1.dn_loss_iou: 0.1620  d2.dn_loss_cls: 0.0022  d2.dn_loss_bbox: 0.1520  d2.dn_loss_iou: 0.1540  d3.dn_loss_cls: 0.0020  d3.dn_loss_bbox: 0.1510  d3.dn_loss_iou: 0.1522  d4.dn_loss_cls: 0.0016  d4.dn_loss_bbox: 0.1506  d4.dn_loss_iou: 0.1520
09/28 10:09:43 - mmengine - INFO - Epoch(train) [10][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:05:54  time: 1.5561  data_time: 0.0116  memory: 8116  grad_norm: 54.0702  loss: 5.7276  loss_cls: 0.0812  loss_bbox: 0.1632  loss_iou: 0.2250  d0.loss_cls: 0.0927  d0.loss_bbox: 0.1684  d0.loss_iou: 0.2289  d1.loss_cls: 0.0876  d1.loss_bbox: 0.1666  d1.loss_iou: 0.2268  d2.loss_cls: 0.0845  d2.loss_bbox: 0.1637  d2.loss_iou: 0.2268  d3.loss_cls: 0.0830  d3.loss_bbox: 0.1629  d3.loss_iou: 0.2236  d4.loss_cls: 0.0812  d4.loss_bbox: 0.1631  d4.loss_iou: 0.2249  enc_loss_cls: 0.1195  enc_loss_bbox: 0.1816  enc_loss_iou: 0.2464  dn_loss_cls: 0.0025  dn_loss_bbox: 0.1705  dn_loss_iou: 0.1828  d0.dn_loss_cls: 0.0194  d0.dn_loss_bbox: 0.2399  d0.dn_loss_iou: 0.2514  d1.dn_loss_cls: 0.0057  d1.dn_loss_bbox: 0.1822  d1.dn_loss_iou: 0.1965  d2.dn_loss_cls: 0.0035  d2.dn_loss_bbox: 0.1729  d2.dn_loss_iou: 0.1860  d3.dn_loss_cls: 0.0029  d3.dn_loss_bbox: 0.1709  d3.dn_loss_iou: 0.1834  d4.dn_loss_cls: 0.0025  d4.dn_loss_bbox: 0.1704  d4.dn_loss_iou: 0.1825
09/28 10:09:58 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:09:58 - mmengine - INFO - Saving checkpoint at 10 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:10:13 - mmengine - INFO - Epoch(val) [10][ 50/125]    eta: 0:00:09  time: 0.1235  data_time: 0.0031  memory: 7796  
09/28 10:10:19 - mmengine - INFO - Epoch(val) [10][100/125]    eta: 0:00:03  time: 0.1222  data_time: 0.0023  memory: 2955  
09/28 10:10:24 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.48s).
Accumulating evaluation results...
DONE (t=1.40s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.819
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.769
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.114
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.481
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.760
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.858
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.859
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.478
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.752
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.898
09/28 10:10:28 - mmengine - INFO - bbox_mAP_copypaste: 0.682 0.819 0.769 0.114 0.481 0.760
09/28 10:10:29 - mmengine - INFO - Epoch(val) [10][125/125]    coco/bbox_mAP: 0.6820  coco/bbox_mAP_50: 0.8190  coco/bbox_mAP_75: 0.7690  coco/bbox_mAP_s: 0.1140  coco/bbox_mAP_m: 0.4810  coco/bbox_mAP_l: 0.7600  data_time: 0.0026  time: 0.1225
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 10:11:45 - mmengine - INFO - Epoch(train) [11][ 50/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:04:21  time: 1.5258  data_time: 0.0120  memory: 7841  grad_norm: 76.5815  loss: 4.4401  loss_cls: 0.0505  loss_bbox: 0.1267  loss_iou: 0.1782  d0.loss_cls: 0.0667  d0.loss_bbox: 0.1320  d0.loss_iou: 0.1841  d1.loss_cls: 0.0630  d1.loss_bbox: 0.1289  d1.loss_iou: 0.1818  d2.loss_cls: 0.0586  d2.loss_bbox: 0.1266  d2.loss_iou: 0.1768  d3.loss_cls: 0.0564  d3.loss_bbox: 0.1263  d3.loss_iou: 0.1770  d4.loss_cls: 0.0512  d4.loss_bbox: 0.1267  d4.loss_iou: 0.1782  enc_loss_cls: 0.0888  enc_loss_bbox: 0.1495  enc_loss_iou: 0.2019  dn_loss_cls: 0.0015  dn_loss_bbox: 0.1252  dn_loss_iou: 0.1514  d0.dn_loss_cls: 0.0124  d0.dn_loss_bbox: 0.1813  d0.dn_loss_iou: 0.2091  d1.dn_loss_cls: 0.0025  d1.dn_loss_bbox: 0.1319  d1.dn_loss_iou: 0.1588  d2.dn_loss_cls: 0.0016  d2.dn_loss_bbox: 0.1263  d2.dn_loss_iou: 0.1527  d3.dn_loss_cls: 0.0014  d3.dn_loss_bbox: 0.1250  d3.dn_loss_iou: 0.1514  d4.dn_loss_cls: 0.0014  d4.dn_loss_bbox: 0.1252  d4.dn_loss_iou: 0.1513
09/28 10:13:01 - mmengine - INFO - Epoch(train) [11][100/110]  base_lr: 1.0000e-04 lr: 1.0000e-04  eta: 0:03:04  time: 1.5188  data_time: 0.0115  memory: 8437  grad_norm: 52.6452  loss: 4.6993  loss_cls: 0.0589  loss_bbox: 0.1391  loss_iou: 0.1711  d0.loss_cls: 0.0684  d0.loss_bbox: 0.1447  d0.loss_iou: 0.1787  d1.loss_cls: 0.0660  d1.loss_bbox: 0.1400  d1.loss_iou: 0.1709  d2.loss_cls: 0.0617  d2.loss_bbox: 0.1388  d2.loss_iou: 0.1720  d3.loss_cls: 0.0609  d3.loss_bbox: 0.1394  d3.loss_iou: 0.1721  d4.loss_cls: 0.0598  d4.loss_bbox: 0.1386  d4.loss_iou: 0.1699  enc_loss_cls: 0.0876  enc_loss_bbox: 0.1585  enc_loss_iou: 0.1925  dn_loss_cls: 0.0044  dn_loss_bbox: 0.1488  dn_loss_iou: 0.1565  d0.dn_loss_cls: 0.0154  d0.dn_loss_bbox: 0.2080  d0.dn_loss_iou: 0.2184  d1.dn_loss_cls: 0.0057  d1.dn_loss_bbox: 0.1566  d1.dn_loss_iou: 0.1683  d2.dn_loss_cls: 0.0042  d2.dn_loss_bbox: 0.1475  d2.dn_loss_iou: 0.1583  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.1478  d3.dn_loss_iou: 0.1565  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.1487  d4.dn_loss_iou: 0.1565
09/28 10:13:17 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:13:17 - mmengine - INFO - Saving checkpoint at 11 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:13:32 - mmengine - INFO - Epoch(val) [11][ 50/125]    eta: 0:00:09  time: 0.1231  data_time: 0.0032  memory: 8436  
09/28 10:13:38 - mmengine - INFO - Epoch(val) [11][100/125]    eta: 0:00:03  time: 0.1239  data_time: 0.0025  memory: 2955  
09/28 10:13:43 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.45s).
Accumulating evaluation results...
DONE (t=1.43s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.640
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.788
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.731
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.095
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.388
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.731
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.846
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.851
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.851
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.470
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.891
09/28 10:13:47 - mmengine - INFO - bbox_mAP_copypaste: 0.640 0.788 0.731 0.095 0.388 0.731
09/28 10:13:47 - mmengine - INFO - Epoch(val) [11][125/125]    coco/bbox_mAP: 0.6400  coco/bbox_mAP_50: 0.7880  coco/bbox_mAP_75: 0.7310  coco/bbox_mAP_s: 0.0950  coco/bbox_mAP_m: 0.3880  coco/bbox_mAP_l: 0.7310  data_time: 0.0028  time: 0.1233
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
09/28 10:15:05 - mmengine - INFO - Epoch(train) [12][ 50/110]  base_lr: 1.0000e-05 lr: 1.0000e-05  eta: 0:01:32  time: 1.5496  data_time: 0.0120  memory: 8395  grad_norm: 44.1736  loss: 4.9081  loss_cls: 0.0553  loss_bbox: 0.1361  loss_iou: 0.1919  d0.loss_cls: 0.0691  d0.loss_bbox: 0.1447  d0.loss_iou: 0.2028  d1.loss_cls: 0.0604  d1.loss_bbox: 0.1375  d1.loss_iou: 0.1951  d2.loss_cls: 0.0572  d2.loss_bbox: 0.1369  d2.loss_iou: 0.1915  d3.loss_cls: 0.0537  d3.loss_bbox: 0.1357  d3.loss_iou: 0.1900  d4.loss_cls: 0.0550  d4.loss_bbox: 0.1360  d4.loss_iou: 0.1902  enc_loss_cls: 0.0926  enc_loss_bbox: 0.1596  enc_loss_iou: 0.2239  dn_loss_cls: 0.0016  dn_loss_bbox: 0.1477  dn_loss_iou: 0.1705  d0.dn_loss_cls: 0.0136  d0.dn_loss_bbox: 0.2109  d0.dn_loss_iou: 0.2367  d1.dn_loss_cls: 0.0035  d1.dn_loss_bbox: 0.1587  d1.dn_loss_iou: 0.1823  d2.dn_loss_cls: 0.0021  d2.dn_loss_bbox: 0.1511  d2.dn_loss_iou: 0.1737  d3.dn_loss_cls: 0.0016  d3.dn_loss_bbox: 0.1479  d3.dn_loss_iou: 0.1710  d4.dn_loss_cls: 0.0016  d4.dn_loss_bbox: 0.1477  d4.dn_loss_iou: 0.1704
09/28 10:16:22 - mmengine - INFO - Epoch(train) [12][100/110]  base_lr: 1.0000e-05 lr: 1.0000e-05  eta: 0:00:15  time: 1.5514  data_time: 0.0118  memory: 8335  grad_norm: 45.8128  loss: 4.4658  loss_cls: 0.0495  loss_bbox: 0.1394  loss_iou: 0.1575  d0.loss_cls: 0.0674  d0.loss_bbox: 0.1459  d0.loss_iou: 0.1650  d1.loss_cls: 0.0581  d1.loss_bbox: 0.1429  d1.loss_iou: 0.1605  d2.loss_cls: 0.0563  d2.loss_bbox: 0.1393  d2.loss_iou: 0.1581  d3.loss_cls: 0.0527  d3.loss_bbox: 0.1398  d3.loss_iou: 0.1584  d4.loss_cls: 0.0512  d4.loss_bbox: 0.1393  d4.loss_iou: 0.1575  enc_loss_cls: 0.0837  enc_loss_bbox: 0.1634  enc_loss_iou: 0.1834  dn_loss_cls: 0.0018  dn_loss_bbox: 0.1476  dn_loss_iou: 0.1443  d0.dn_loss_cls: 0.0128  d0.dn_loss_bbox: 0.2010  d0.dn_loss_iou: 0.1937  d1.dn_loss_cls: 0.0029  d1.dn_loss_bbox: 0.1549  d1.dn_loss_iou: 0.1529  d2.dn_loss_cls: 0.0020  d2.dn_loss_bbox: 0.1489  d2.dn_loss_iou: 0.1460  d3.dn_loss_cls: 0.0020  d3.dn_loss_bbox: 0.1477  d3.dn_loss_iou: 0.1445  d4.dn_loss_cls: 0.0018  d4.dn_loss_bbox: 0.1476  d4.dn_loss_iou: 0.1442
09/28 10:16:37 - mmengine - INFO - Exp name: grounding_dino_swin-t_finetune_vehicles_backdoor_20240928_093636
09/28 10:16:37 - mmengine - INFO - Saving checkpoint at 12 epochs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ankita/scratch/miniconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/cnn/bricks/transformer.py:524: UserWarning: position encoding of key ismissing in MultiheadAttention.
  warnings.warn(f'position encoding of key is'
09/28 10:16:53 - mmengine - INFO - Epoch(val) [12][ 50/125]    eta: 0:00:09  time: 0.1230  data_time: 0.0031  memory: 7148  
09/28 10:16:59 - mmengine - INFO - Epoch(val) [12][100/125]    eta: 0:00:03  time: 0.1231  data_time: 0.0024  memory: 2955  
09/28 10:17:03 - mmengine - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.09s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.34s).
Accumulating evaluation results...
DONE (t=1.41s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.652
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.809
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.749
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.406
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.734
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.845
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.849
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.849
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.465
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.756
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.887
09/28 10:17:08 - mmengine - INFO - bbox_mAP_copypaste: 0.652 0.809 0.749 0.109 0.406 0.734
09/28 10:17:08 - mmengine - INFO - Epoch(val) [12][125/125]    coco/bbox_mAP: 0.6520  coco/bbox_mAP_50: 0.8090  coco/bbox_mAP_75: 0.7490  coco/bbox_mAP_s: 0.1090  coco/bbox_mAP_m: 0.4060  coco/bbox_mAP_l: 0.7340  data_time: 0.0027  time: 0.1227
